{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_news_lag1 = pd.read_csv(\n",
    "    \"../Data/bitcoin_news_sentiment_daily_lag1.csv\", index_col=0\n",
    ")\n",
    "BB_sentiment_lag1 = pd.read_csv(\"../Data/BB_sentiment_lag1.csv\", index_col=0)\n",
    "btc_direction = pd.read_csv(\"../Data/btc_direction.csv\", index_col=0)\n",
    "btc_magnitude = pd.read_csv(\"../Data/btc_magnitude.csv\", index_col=0)\n",
    "coinbase_news_lag1 = pd.read_csv(\n",
    "    \"../Data/coinbase_news_sentiment_daily_lag1.csv\", index_col=0\n",
    ")\n",
    "non_sentiment_lag1 = pd.read_csv(\"../Data/features_lag1.csv\", index_col=0)\n",
    "gg_trend = pd.read_csv(\"../Data/google_trends_signal_daily.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive_count_bitcoin</th>\n",
       "      <th>negative_count_bitcoin</th>\n",
       "      <th>neutral_count_bitcoin</th>\n",
       "      <th>positive_avg_score_bitcoin</th>\n",
       "      <th>negative_avg_score_bitcoin</th>\n",
       "      <th>neutral_avg_score_bitcoin</th>\n",
       "      <th>total_news_count_bitcoin</th>\n",
       "      <th>total_news_score_bitcoin</th>\n",
       "      <th>signal_bitcoin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-04-15</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.907243</td>\n",
       "      <td>0.959677</td>\n",
       "      <td>0.849137</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.153077</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-16</th>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.623001</td>\n",
       "      <td>0.895473</td>\n",
       "      <td>0.726584</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-5.216729</td>\n",
       "      <td>-0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.725935</td>\n",
       "      <td>0.804262</td>\n",
       "      <td>0.818327</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.882590</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-19</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.886130</td>\n",
       "      <td>0.962285</td>\n",
       "      <td>0.899825</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-4.001451</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-20</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.867508</td>\n",
       "      <td>0.785641</td>\n",
       "      <td>0.786232</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.325679</td>\n",
       "      <td>-0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-09</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.774210</td>\n",
       "      <td>0.975991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.346641</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-12</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.871973</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.187121</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-13</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.821627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.901766</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.464880</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684063</td>\n",
       "      <td>0.642996</td>\n",
       "      <td>0.939473</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041067</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-18</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.865381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.608785</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.730762</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            positive_count_bitcoin  negative_count_bitcoin  \\\n",
       "date_dt                                                      \n",
       "2021-04-15                    10.0                     2.0   \n",
       "2021-04-16                     6.0                    10.0   \n",
       "2021-04-17                     1.0                     2.0   \n",
       "2021-04-19                     2.0                     6.0   \n",
       "2021-04-20                     3.0                     5.0   \n",
       "...                            ...                     ...   \n",
       "2024-03-09                     3.0                     1.0   \n",
       "2024-03-12                     9.0                     0.0   \n",
       "2024-03-13                     3.0                     0.0   \n",
       "2024-03-14                     1.0                     1.0   \n",
       "2024-03-18                     2.0                     0.0   \n",
       "\n",
       "            neutral_count_bitcoin  positive_avg_score_bitcoin  \\\n",
       "date_dt                                                         \n",
       "2021-04-15                   10.0                    0.907243   \n",
       "2021-04-16                    7.0                    0.623001   \n",
       "2021-04-17                    3.0                    0.725935   \n",
       "2021-04-19                    1.0                    0.886130   \n",
       "2021-04-20                    2.0                    0.867508   \n",
       "...                           ...                         ...   \n",
       "2024-03-09                    0.0                    0.774210   \n",
       "2024-03-12                    1.0                    0.798569   \n",
       "2024-03-13                    1.0                    0.821627   \n",
       "2024-03-14                    1.0                    0.684063   \n",
       "2024-03-18                    1.0                    0.865381   \n",
       "\n",
       "            negative_avg_score_bitcoin  neutral_avg_score_bitcoin  \\\n",
       "date_dt                                                             \n",
       "2021-04-15                    0.959677                   0.849137   \n",
       "2021-04-16                    0.895473                   0.726584   \n",
       "2021-04-17                    0.804262                   0.818327   \n",
       "2021-04-19                    0.962285                   0.899825   \n",
       "2021-04-20                    0.785641                   0.786232   \n",
       "...                                ...                        ...   \n",
       "2024-03-09                    0.975991                   0.000000   \n",
       "2024-03-12                    0.000000                   0.871973   \n",
       "2024-03-13                    0.000000                   0.901766   \n",
       "2024-03-14                    0.642996                   0.939473   \n",
       "2024-03-18                    0.000000                   0.608785   \n",
       "\n",
       "            total_news_count_bitcoin  total_news_score_bitcoin  signal_bitcoin  \n",
       "date_dt                                                                         \n",
       "2021-04-15                      22.0                  7.153077        0.666667  \n",
       "2021-04-16                      23.0                 -5.216729       -0.250000  \n",
       "2021-04-17                       6.0                 -0.882590       -0.333333  \n",
       "2021-04-19                       9.0                 -4.001451       -0.500000  \n",
       "2021-04-20                      10.0                 -1.325679       -0.250000  \n",
       "...                              ...                       ...             ...  \n",
       "2024-03-09                       4.0                  1.346641        0.500000  \n",
       "2024-03-12                      10.0                  7.187121        1.000000  \n",
       "2024-03-13                       4.0                  2.464880        1.000000  \n",
       "2024-03-14                       3.0                  0.041067        0.000000  \n",
       "2024-03-18                       3.0                  1.730762        1.000000  \n",
       "\n",
       "[510 rows x 9 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin_news_lag1.columns = [col + \"_bitcoin\" for col in bitcoin_news_lag1.columns]\n",
    "bitcoin_news_lag1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TWITTER_SENTIMENT_DAILY_AVG</th>\n",
       "      <th>TWITTER_PUBLICATION_COUNT</th>\n",
       "      <th>TWITTER_NEG_SENTIMENT_COUNT</th>\n",
       "      <th>TWITTER_POS_SENTIMENT_COUNT</th>\n",
       "      <th>TWITTER_SENTIMENT_DAILY_MAX</th>\n",
       "      <th>TWITTER_NEUTRAL_SENTIMENT_CNT</th>\n",
       "      <th>TWITTER_SENTIMENT_DAILY_MIN</th>\n",
       "      <th>NEWS_SENTIMENT_DAILY_AVG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-04-02</th>\n",
       "      <td>0.0032</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-0.1068</td>\n",
       "      <td>-0.6321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-03</th>\n",
       "      <td>0.0019</td>\n",
       "      <td>174.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>169.0</td>\n",
       "      <td>-0.1068</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-04</th>\n",
       "      <td>0.0019</td>\n",
       "      <td>174.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>169.0</td>\n",
       "      <td>-0.1068</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-05</th>\n",
       "      <td>0.0019</td>\n",
       "      <td>174.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>169.0</td>\n",
       "      <td>-0.1068</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-06</th>\n",
       "      <td>0.0025</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-28</th>\n",
       "      <td>-0.0049</td>\n",
       "      <td>502.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.4438</td>\n",
       "      <td>145.0</td>\n",
       "      <td>-0.1830</td>\n",
       "      <td>0.0113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-29</th>\n",
       "      <td>-0.1393</td>\n",
       "      <td>955.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.4438</td>\n",
       "      <td>348.0</td>\n",
       "      <td>-0.1830</td>\n",
       "      <td>-0.5112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-30</th>\n",
       "      <td>-0.0236</td>\n",
       "      <td>470.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.4438</td>\n",
       "      <td>128.0</td>\n",
       "      <td>-0.1830</td>\n",
       "      <td>-0.0477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-31</th>\n",
       "      <td>-0.0236</td>\n",
       "      <td>470.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.4438</td>\n",
       "      <td>128.0</td>\n",
       "      <td>-0.1830</td>\n",
       "      <td>-0.0477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-01</th>\n",
       "      <td>-0.0236</td>\n",
       "      <td>470.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.4438</td>\n",
       "      <td>128.0</td>\n",
       "      <td>-0.1830</td>\n",
       "      <td>-0.0477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1096 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            TWITTER_SENTIMENT_DAILY_AVG  TWITTER_PUBLICATION_COUNT  \\\n",
       "date_dt                                                              \n",
       "2021-04-02                       0.0032                       10.0   \n",
       "2021-04-03                       0.0019                      174.0   \n",
       "2021-04-04                       0.0019                      174.0   \n",
       "2021-04-05                       0.0019                      174.0   \n",
       "2021-04-06                       0.0025                       41.0   \n",
       "...                                 ...                        ...   \n",
       "2024-03-28                      -0.0049                      502.0   \n",
       "2024-03-29                      -0.1393                      955.0   \n",
       "2024-03-30                      -0.0236                      470.0   \n",
       "2024-03-31                      -0.0236                      470.0   \n",
       "2024-04-01                      -0.0236                      470.0   \n",
       "\n",
       "            TWITTER_NEG_SENTIMENT_COUNT  TWITTER_POS_SENTIMENT_COUNT  \\\n",
       "date_dt                                                                \n",
       "2021-04-02                          1.0                          1.0   \n",
       "2021-04-03                          2.0                          8.0   \n",
       "2021-04-04                          2.0                          8.0   \n",
       "2021-04-05                          2.0                          8.0   \n",
       "2021-04-06                          1.0                          2.0   \n",
       "...                                 ...                          ...   \n",
       "2024-03-28                         12.0                          7.0   \n",
       "2024-03-29                        148.0                         22.0   \n",
       "2024-03-30                         22.0                          8.0   \n",
       "2024-03-31                         22.0                          8.0   \n",
       "2024-04-01                         22.0                          8.0   \n",
       "\n",
       "            TWITTER_SENTIMENT_DAILY_MAX  TWITTER_NEUTRAL_SENTIMENT_CNT  \\\n",
       "date_dt                                                                  \n",
       "2021-04-02                       0.1068                           14.0   \n",
       "2021-04-03                       0.1068                          169.0   \n",
       "2021-04-04                       0.1068                          169.0   \n",
       "2021-04-05                       0.1068                          169.0   \n",
       "2021-04-06                       0.1070                           34.0   \n",
       "...                                 ...                            ...   \n",
       "2024-03-28                       0.4438                          145.0   \n",
       "2024-03-29                       0.4438                          348.0   \n",
       "2024-03-30                       0.4438                          128.0   \n",
       "2024-03-31                       0.4438                          128.0   \n",
       "2024-04-01                       0.4438                          128.0   \n",
       "\n",
       "            TWITTER_SENTIMENT_DAILY_MIN  NEWS_SENTIMENT_DAILY_AVG  \n",
       "date_dt                                                            \n",
       "2021-04-02                      -0.1068                   -0.6321  \n",
       "2021-04-03                      -0.1068                    0.0005  \n",
       "2021-04-04                      -0.1068                    0.0005  \n",
       "2021-04-05                      -0.1068                    0.0005  \n",
       "2021-04-06                       0.0000                    0.0000  \n",
       "...                                 ...                       ...  \n",
       "2024-03-28                      -0.1830                    0.0113  \n",
       "2024-03-29                      -0.1830                   -0.5112  \n",
       "2024-03-30                      -0.1830                   -0.0477  \n",
       "2024-03-31                      -0.1830                   -0.0477  \n",
       "2024-04-01                      -0.1830                   -0.0477  \n",
       "\n",
       "[1096 rows x 8 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BB_sentiment_lag1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_dt</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-09-18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-20</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-22</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-28</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-30</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-31</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-02</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3484 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Direction\n",
       "date_dt              \n",
       "2014-09-18          0\n",
       "2014-09-19          0\n",
       "2014-09-20          1\n",
       "2014-09-21          0\n",
       "2014-09-22          1\n",
       "...               ...\n",
       "2024-03-28          1\n",
       "2024-03-29          0\n",
       "2024-03-30          0\n",
       "2024-03-31          1\n",
       "2024-04-02          0\n",
       "\n",
       "[3484 rows x 1 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>magnitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_dt</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-09-18</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-19</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-20</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-21</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-22</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-31</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-01</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-02</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-03</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-04</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3487 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            magnitude\n",
       "date_dt              \n",
       "2014-09-18          5\n",
       "2014-09-19          5\n",
       "2014-09-20          6\n",
       "2014-09-21          5\n",
       "2014-09-22          6\n",
       "...               ...\n",
       "2024-03-31         10\n",
       "2024-04-01          1\n",
       "2024-04-02          1\n",
       "2024-04-03          7\n",
       "2024-04-04         10\n",
       "\n",
       "[3487 rows x 1 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive_count_coinbase</th>\n",
       "      <th>negative_count_coinbase</th>\n",
       "      <th>neutral_count_coinbase</th>\n",
       "      <th>positive_avg_score_coinbase</th>\n",
       "      <th>negative_avg_score_coinbase</th>\n",
       "      <th>neutral_avg_score_coinbase</th>\n",
       "      <th>total_news_count_coinbase</th>\n",
       "      <th>total_news_score_coinbase</th>\n",
       "      <th>signal_coinbase</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-04-14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.757015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.798267</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.757015</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-15</th>\n",
       "      <td>44.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.786364</td>\n",
       "      <td>0.817363</td>\n",
       "      <td>0.884039</td>\n",
       "      <td>161.0</td>\n",
       "      <td>15.800687</td>\n",
       "      <td>0.313433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-16</th>\n",
       "      <td>51.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.823400</td>\n",
       "      <td>0.890134</td>\n",
       "      <td>0.858108</td>\n",
       "      <td>152.0</td>\n",
       "      <td>17.069669</td>\n",
       "      <td>0.291139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-17</th>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.794744</td>\n",
       "      <td>0.798550</td>\n",
       "      <td>0.832263</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.543818</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-18</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.662382</td>\n",
       "      <td>0.733936</td>\n",
       "      <td>0.782948</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.519273</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-13</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.814736</td>\n",
       "      <td>0.759975</td>\n",
       "      <td>0.912939</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.183200</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-14</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.818522</td>\n",
       "      <td>0.771731</td>\n",
       "      <td>0.851901</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-1.449878</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-15</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.833427</td>\n",
       "      <td>0.977488</td>\n",
       "      <td>0.903085</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.234667</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-18</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.844617</td>\n",
       "      <td>0.680807</td>\n",
       "      <td>0.908060</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.706089</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.821265</td>\n",
       "      <td>0.684396</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-2.463795</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>978 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            positive_count_coinbase  negative_count_coinbase  \\\n",
       "date_dt                                                        \n",
       "2021-04-14                      1.0                      0.0   \n",
       "2021-04-15                     44.0                     23.0   \n",
       "2021-04-16                     51.0                     28.0   \n",
       "2021-04-17                     14.0                     12.0   \n",
       "2021-04-18                      3.0                      2.0   \n",
       "...                             ...                      ...   \n",
       "2024-03-13                      7.0                      2.0   \n",
       "2024-03-14                      2.0                      4.0   \n",
       "2024-03-15                      5.0                      3.0   \n",
       "2024-03-18                      6.0                      2.0   \n",
       "2024-03-19                      0.0                      3.0   \n",
       "\n",
       "            neutral_count_coinbase  positive_avg_score_coinbase  \\\n",
       "date_dt                                                           \n",
       "2021-04-14                     2.0                     0.757015   \n",
       "2021-04-15                    94.0                     0.786364   \n",
       "2021-04-16                    73.0                     0.823400   \n",
       "2021-04-17                    44.0                     0.794744   \n",
       "2021-04-18                     9.0                     0.662382   \n",
       "...                            ...                          ...   \n",
       "2024-03-13                    18.0                     0.814736   \n",
       "2024-03-14                    18.0                     0.818522   \n",
       "2024-03-15                     5.0                     0.833427   \n",
       "2024-03-18                     9.0                     0.844617   \n",
       "2024-03-19                     6.0                     0.000000   \n",
       "\n",
       "            negative_avg_score_coinbase  neutral_avg_score_coinbase  \\\n",
       "date_dt                                                               \n",
       "2021-04-14                     0.000000                    0.798267   \n",
       "2021-04-15                     0.817363                    0.884039   \n",
       "2021-04-16                     0.890134                    0.858108   \n",
       "2021-04-17                     0.798550                    0.832263   \n",
       "2021-04-18                     0.733936                    0.782948   \n",
       "...                                 ...                         ...   \n",
       "2024-03-13                     0.759975                    0.912939   \n",
       "2024-03-14                     0.771731                    0.851901   \n",
       "2024-03-15                     0.977488                    0.903085   \n",
       "2024-03-18                     0.680807                    0.908060   \n",
       "2024-03-19                     0.821265                    0.684396   \n",
       "\n",
       "            total_news_count_coinbase  total_news_score_coinbase  \\\n",
       "date_dt                                                            \n",
       "2021-04-14                        3.0                   0.757015   \n",
       "2021-04-15                      161.0                  15.800687   \n",
       "2021-04-16                      152.0                  17.069669   \n",
       "2021-04-17                       70.0                   1.543818   \n",
       "2021-04-18                       14.0                   0.519273   \n",
       "...                               ...                        ...   \n",
       "2024-03-13                       27.0                   4.183200   \n",
       "2024-03-14                       24.0                  -1.449878   \n",
       "2024-03-15                       13.0                   1.234667   \n",
       "2024-03-18                       17.0                   3.706089   \n",
       "2024-03-19                        9.0                  -2.463795   \n",
       "\n",
       "            signal_coinbase  \n",
       "date_dt                      \n",
       "2021-04-14         1.000000  \n",
       "2021-04-15         0.313433  \n",
       "2021-04-16         0.291139  \n",
       "2021-04-17         0.076923  \n",
       "2021-04-18         0.200000  \n",
       "...                     ...  \n",
       "2024-03-13         0.555556  \n",
       "2024-03-14        -0.333333  \n",
       "2024-03-15         0.250000  \n",
       "2024-03-18         0.500000  \n",
       "2024-03-19        -1.000000  \n",
       "\n",
       "[978 rows x 9 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coinbase_news_lag1.columns = [col + \"_coinbase\" for col in coinbase_news_lag1.columns]\n",
    "coinbase_news_lag1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>close_open</th>\n",
       "      <th>Daily Return</th>\n",
       "      <th>cumulative_return</th>\n",
       "      <th>30D_Moving_STD</th>\n",
       "      <th>...</th>\n",
       "      <th>DTB3_lag1</th>\n",
       "      <th>DTB3_lag2</th>\n",
       "      <th>T5YIE_lag1</th>\n",
       "      <th>T5YIE_lag2</th>\n",
       "      <th>S&amp;P500_Close_lag1</th>\n",
       "      <th>S&amp;P500_Close_lag2</th>\n",
       "      <th>VIX_Close_lag1</th>\n",
       "      <th>VIX_Close_lag2</th>\n",
       "      <th>S&amp;P_Return_lag1</th>\n",
       "      <th>S&amp;P_Return_lag2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-04-15</th>\n",
       "      <td>63523.753906</td>\n",
       "      <td>64863.097656</td>\n",
       "      <td>61554.796875</td>\n",
       "      <td>63109.695312</td>\n",
       "      <td>63109.695312</td>\n",
       "      <td>7.745178e+10</td>\n",
       "      <td>-414.058594</td>\n",
       "      <td>0.003237</td>\n",
       "      <td>0.003237</td>\n",
       "      <td>3795.270489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.56</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>16.99</td>\n",
       "      <td>16.99</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.011094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-16</th>\n",
       "      <td>63075.195312</td>\n",
       "      <td>63821.671875</td>\n",
       "      <td>62208.964844</td>\n",
       "      <td>63314.011719</td>\n",
       "      <td>63314.011719</td>\n",
       "      <td>6.095438e+10</td>\n",
       "      <td>238.816406</td>\n",
       "      <td>0.003237</td>\n",
       "      <td>0.003237</td>\n",
       "      <td>3795.270489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.56</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>16.99</td>\n",
       "      <td>16.99</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.011094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-17</th>\n",
       "      <td>63258.503906</td>\n",
       "      <td>63594.722656</td>\n",
       "      <td>60222.531250</td>\n",
       "      <td>61572.789062</td>\n",
       "      <td>61572.789062</td>\n",
       "      <td>8.429301e+10</td>\n",
       "      <td>-1685.714844</td>\n",
       "      <td>-0.027501</td>\n",
       "      <td>-0.024353</td>\n",
       "      <td>3795.270489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.56</td>\n",
       "      <td>4170.419922</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>16.57</td>\n",
       "      <td>16.99</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.011094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-18</th>\n",
       "      <td>61529.921875</td>\n",
       "      <td>62572.175781</td>\n",
       "      <td>60361.351562</td>\n",
       "      <td>60683.820312</td>\n",
       "      <td>60683.820312</td>\n",
       "      <td>6.613876e+10</td>\n",
       "      <td>-846.101562</td>\n",
       "      <td>-0.014438</td>\n",
       "      <td>-0.038439</td>\n",
       "      <td>3795.270489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.55</td>\n",
       "      <td>4185.470215</td>\n",
       "      <td>4170.419922</td>\n",
       "      <td>16.25</td>\n",
       "      <td>16.57</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.011094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-19</th>\n",
       "      <td>60701.886719</td>\n",
       "      <td>61057.457031</td>\n",
       "      <td>52829.535156</td>\n",
       "      <td>56216.183594</td>\n",
       "      <td>56216.183594</td>\n",
       "      <td>9.746887e+10</td>\n",
       "      <td>-4485.703125</td>\n",
       "      <td>-0.073622</td>\n",
       "      <td>-0.109231</td>\n",
       "      <td>3795.270489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.57</td>\n",
       "      <td>4185.470215</td>\n",
       "      <td>4185.470215</td>\n",
       "      <td>16.25</td>\n",
       "      <td>16.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-30</th>\n",
       "      <td>70744.796875</td>\n",
       "      <td>70913.093750</td>\n",
       "      <td>69076.656250</td>\n",
       "      <td>69892.828125</td>\n",
       "      <td>69892.828125</td>\n",
       "      <td>2.523085e+10</td>\n",
       "      <td>-851.968750</td>\n",
       "      <td>-0.012045</td>\n",
       "      <td>0.107482</td>\n",
       "      <td>3319.982750</td>\n",
       "      <td>...</td>\n",
       "      <td>5.23</td>\n",
       "      <td>5.22</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.37</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>5248.490234</td>\n",
       "      <td>13.01</td>\n",
       "      <td>12.78</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.008631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-31</th>\n",
       "      <td>69893.445312</td>\n",
       "      <td>70355.492188</td>\n",
       "      <td>69601.062500</td>\n",
       "      <td>69645.304688</td>\n",
       "      <td>69645.304688</td>\n",
       "      <td>1.713024e+10</td>\n",
       "      <td>-248.140625</td>\n",
       "      <td>-0.003541</td>\n",
       "      <td>0.103560</td>\n",
       "      <td>3137.874526</td>\n",
       "      <td>...</td>\n",
       "      <td>5.23</td>\n",
       "      <td>5.23</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-01</th>\n",
       "      <td>69647.781250</td>\n",
       "      <td>71377.781250</td>\n",
       "      <td>69624.867188</td>\n",
       "      <td>71333.648438</td>\n",
       "      <td>71333.648438</td>\n",
       "      <td>2.005094e+10</td>\n",
       "      <td>1685.867188</td>\n",
       "      <td>0.024242</td>\n",
       "      <td>0.130312</td>\n",
       "      <td>3054.432327</td>\n",
       "      <td>...</td>\n",
       "      <td>5.23</td>\n",
       "      <td>5.23</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-02</th>\n",
       "      <td>71333.484375</td>\n",
       "      <td>71342.093750</td>\n",
       "      <td>68110.695312</td>\n",
       "      <td>69702.148438</td>\n",
       "      <td>69702.148438</td>\n",
       "      <td>3.487353e+10</td>\n",
       "      <td>-1631.335938</td>\n",
       "      <td>-0.022871</td>\n",
       "      <td>0.104460</td>\n",
       "      <td>2863.107821</td>\n",
       "      <td>...</td>\n",
       "      <td>5.23</td>\n",
       "      <td>5.23</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-03</th>\n",
       "      <td>69705.023438</td>\n",
       "      <td>69708.382812</td>\n",
       "      <td>64586.593750</td>\n",
       "      <td>65446.972656</td>\n",
       "      <td>65446.972656</td>\n",
       "      <td>5.070524e+10</td>\n",
       "      <td>-4258.050781</td>\n",
       "      <td>-0.061048</td>\n",
       "      <td>0.037035</td>\n",
       "      <td>2754.968124</td>\n",
       "      <td>...</td>\n",
       "      <td>5.23</td>\n",
       "      <td>5.23</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5243.770020</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>13.65</td>\n",
       "      <td>13.01</td>\n",
       "      <td>-0.002014</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1085 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Open          High           Low         Close  \\\n",
       "date_dt                                                              \n",
       "2021-04-15  63523.753906  64863.097656  61554.796875  63109.695312   \n",
       "2021-04-16  63075.195312  63821.671875  62208.964844  63314.011719   \n",
       "2021-04-17  63258.503906  63594.722656  60222.531250  61572.789062   \n",
       "2021-04-18  61529.921875  62572.175781  60361.351562  60683.820312   \n",
       "2021-04-19  60701.886719  61057.457031  52829.535156  56216.183594   \n",
       "...                  ...           ...           ...           ...   \n",
       "2024-03-30  70744.796875  70913.093750  69076.656250  69892.828125   \n",
       "2024-03-31  69893.445312  70355.492188  69601.062500  69645.304688   \n",
       "2024-04-01  69647.781250  71377.781250  69624.867188  71333.648438   \n",
       "2024-04-02  71333.484375  71342.093750  68110.695312  69702.148438   \n",
       "2024-04-03  69705.023438  69708.382812  64586.593750  65446.972656   \n",
       "\n",
       "               Adj Close        Volume   close_open  Daily Return  \\\n",
       "date_dt                                                             \n",
       "2021-04-15  63109.695312  7.745178e+10  -414.058594      0.003237   \n",
       "2021-04-16  63314.011719  6.095438e+10   238.816406      0.003237   \n",
       "2021-04-17  61572.789062  8.429301e+10 -1685.714844     -0.027501   \n",
       "2021-04-18  60683.820312  6.613876e+10  -846.101562     -0.014438   \n",
       "2021-04-19  56216.183594  9.746887e+10 -4485.703125     -0.073622   \n",
       "...                  ...           ...          ...           ...   \n",
       "2024-03-30  69892.828125  2.523085e+10  -851.968750     -0.012045   \n",
       "2024-03-31  69645.304688  1.713024e+10  -248.140625     -0.003541   \n",
       "2024-04-01  71333.648438  2.005094e+10  1685.867188      0.024242   \n",
       "2024-04-02  69702.148438  3.487353e+10 -1631.335938     -0.022871   \n",
       "2024-04-03  65446.972656  5.070524e+10 -4258.050781     -0.061048   \n",
       "\n",
       "            cumulative_return  30D_Moving_STD  ...  DTB3_lag1  DTB3_lag2  \\\n",
       "date_dt                                        ...                         \n",
       "2021-04-15           0.003237     3795.270489  ...       0.02       0.02   \n",
       "2021-04-16           0.003237     3795.270489  ...       0.02       0.02   \n",
       "2021-04-17          -0.024353     3795.270489  ...       0.02       0.02   \n",
       "2021-04-18          -0.038439     3795.270489  ...       0.02       0.02   \n",
       "2021-04-19          -0.109231     3795.270489  ...       0.02       0.02   \n",
       "...                       ...             ...  ...        ...        ...   \n",
       "2024-03-30           0.107482     3319.982750  ...       5.23       5.22   \n",
       "2024-03-31           0.103560     3137.874526  ...       5.23       5.23   \n",
       "2024-04-01           0.130312     3054.432327  ...       5.23       5.23   \n",
       "2024-04-02           0.104460     2863.107821  ...       5.23       5.23   \n",
       "2024-04-03           0.037035     2754.968124  ...       5.23       5.23   \n",
       "\n",
       "            T5YIE_lag1  T5YIE_lag2  S&P500_Close_lag1  S&P500_Close_lag2  \\\n",
       "date_dt                                                                    \n",
       "2021-04-15        2.56        2.56        4124.660156        4124.660156   \n",
       "2021-04-16        2.56        2.56        4124.660156        4124.660156   \n",
       "2021-04-17        2.55        2.56        4170.419922        4124.660156   \n",
       "2021-04-18        2.57        2.55        4185.470215        4170.419922   \n",
       "2021-04-19        2.57        2.57        4185.470215        4185.470215   \n",
       "...                ...         ...                ...                ...   \n",
       "2024-03-30        2.38        2.37        5254.350098        5248.490234   \n",
       "2024-03-31        2.38        2.38        5254.350098        5254.350098   \n",
       "2024-04-01        2.38        2.38        5254.350098        5254.350098   \n",
       "2024-04-02        2.38        2.38        5254.350098        5254.350098   \n",
       "2024-04-03        2.41        2.38        5243.770020        5254.350098   \n",
       "\n",
       "            VIX_Close_lag1  VIX_Close_lag2  S&P_Return_lag1  S&P_Return_lag2  \n",
       "date_dt                                                                       \n",
       "2021-04-15           16.99           16.99         0.011094         0.011094  \n",
       "2021-04-16           16.99           16.99         0.011094         0.011094  \n",
       "2021-04-17           16.57           16.99         0.011094         0.011094  \n",
       "2021-04-18           16.25           16.57         0.003609         0.011094  \n",
       "2021-04-19           16.25           16.25         0.000000         0.003609  \n",
       "...                    ...             ...              ...              ...  \n",
       "2024-03-30           13.01           12.78         0.001116         0.008631  \n",
       "2024-03-31           13.01           13.01         0.000000         0.001116  \n",
       "2024-04-01           13.01           13.01         0.000000         0.000000  \n",
       "2024-04-02           13.01           13.01         0.000000         0.000000  \n",
       "2024-04-03           13.65           13.01        -0.002014         0.000000  \n",
       "\n",
       "[1085 rows x 39 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_sentiment_lag1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gg_trend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_dt</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-04-11</th>\n",
       "      <td>0.137663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-12</th>\n",
       "      <td>0.137663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-13</th>\n",
       "      <td>0.137663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-14</th>\n",
       "      <td>0.137663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-15</th>\n",
       "      <td>0.137663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-20</th>\n",
       "      <td>0.237439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-21</th>\n",
       "      <td>0.237439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-22</th>\n",
       "      <td>0.237439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-23</th>\n",
       "      <td>0.237439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-24</th>\n",
       "      <td>0.142479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1079 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gg_trend\n",
       "date_dt             \n",
       "2021-04-11  0.137663\n",
       "2021-04-12  0.137663\n",
       "2021-04-13  0.137663\n",
       "2021-04-14  0.137663\n",
       "2021-04-15  0.137663\n",
       "...              ...\n",
       "2024-03-20  0.237439\n",
       "2024-03-21  0.237439\n",
       "2024-03-22  0.237439\n",
       "2024-03-23  0.237439\n",
       "2024-03-24  0.142479\n",
       "\n",
       "[1079 rows x 1 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg_trend.index.name = \"date_dt\"\n",
    "gg_trend.rename(columns={\"signal\": \"gg_trend\"}, inplace=True)\n",
    "gg_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Direction</th>\n",
       "      <th>magnitude</th>\n",
       "      <th>positive_count_bitcoin</th>\n",
       "      <th>negative_count_bitcoin</th>\n",
       "      <th>neutral_count_bitcoin</th>\n",
       "      <th>positive_avg_score_bitcoin</th>\n",
       "      <th>negative_avg_score_bitcoin</th>\n",
       "      <th>neutral_avg_score_bitcoin</th>\n",
       "      <th>total_news_count_bitcoin</th>\n",
       "      <th>total_news_score_bitcoin</th>\n",
       "      <th>...</th>\n",
       "      <th>DTB3_lag2</th>\n",
       "      <th>T5YIE_lag1</th>\n",
       "      <th>T5YIE_lag2</th>\n",
       "      <th>S&amp;P500_Close_lag1</th>\n",
       "      <th>S&amp;P500_Close_lag2</th>\n",
       "      <th>VIX_Close_lag1</th>\n",
       "      <th>VIX_Close_lag2</th>\n",
       "      <th>S&amp;P_Return_lag1</th>\n",
       "      <th>S&amp;P_Return_lag2</th>\n",
       "      <th>gg_trend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-04-15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.907243</td>\n",
       "      <td>0.959677</td>\n",
       "      <td>0.849137</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.153077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.56</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>16.99</td>\n",
       "      <td>16.99</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.137663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.623001</td>\n",
       "      <td>0.895473</td>\n",
       "      <td>0.726584</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-5.216729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.56</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>16.99</td>\n",
       "      <td>16.99</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.137663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.725935</td>\n",
       "      <td>0.804262</td>\n",
       "      <td>0.818327</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.882590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.56</td>\n",
       "      <td>4170.419922</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>16.57</td>\n",
       "      <td>16.99</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.137663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.55</td>\n",
       "      <td>4185.470215</td>\n",
       "      <td>4170.419922</td>\n",
       "      <td>16.25</td>\n",
       "      <td>16.57</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.018928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.886130</td>\n",
       "      <td>0.962285</td>\n",
       "      <td>0.899825</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-4.001451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.57</td>\n",
       "      <td>4185.470215</td>\n",
       "      <td>4185.470215</td>\n",
       "      <td>16.25</td>\n",
       "      <td>16.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.018928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.23</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-02</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.23</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.23</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.23</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5243.770020</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>13.65</td>\n",
       "      <td>13.01</td>\n",
       "      <td>-0.002014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1086 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Direction  magnitude  positive_count_bitcoin  \\\n",
       "date_dt                                                    \n",
       "2021-04-15        1.0          6                    10.0   \n",
       "2021-04-16        0.0          1                     6.0   \n",
       "2021-04-17        0.0          3                     1.0   \n",
       "2021-04-18        0.0          1                     NaN   \n",
       "2021-04-19        0.0          4                     2.0   \n",
       "...               ...        ...                     ...   \n",
       "2024-03-31        1.0         10                     NaN   \n",
       "2024-04-02        0.0          1                     NaN   \n",
       "2024-04-01        NaN          1                     NaN   \n",
       "2024-04-03        NaN          7                     NaN   \n",
       "2024-04-04        NaN         10                     NaN   \n",
       "\n",
       "            negative_count_bitcoin  neutral_count_bitcoin  \\\n",
       "date_dt                                                     \n",
       "2021-04-15                     2.0                   10.0   \n",
       "2021-04-16                    10.0                    7.0   \n",
       "2021-04-17                     2.0                    3.0   \n",
       "2021-04-18                     NaN                    NaN   \n",
       "2021-04-19                     6.0                    1.0   \n",
       "...                            ...                    ...   \n",
       "2024-03-31                     NaN                    NaN   \n",
       "2024-04-02                     NaN                    NaN   \n",
       "2024-04-01                     NaN                    NaN   \n",
       "2024-04-03                     NaN                    NaN   \n",
       "2024-04-04                     NaN                    NaN   \n",
       "\n",
       "            positive_avg_score_bitcoin  negative_avg_score_bitcoin  \\\n",
       "date_dt                                                              \n",
       "2021-04-15                    0.907243                    0.959677   \n",
       "2021-04-16                    0.623001                    0.895473   \n",
       "2021-04-17                    0.725935                    0.804262   \n",
       "2021-04-18                         NaN                         NaN   \n",
       "2021-04-19                    0.886130                    0.962285   \n",
       "...                                ...                         ...   \n",
       "2024-03-31                         NaN                         NaN   \n",
       "2024-04-02                         NaN                         NaN   \n",
       "2024-04-01                         NaN                         NaN   \n",
       "2024-04-03                         NaN                         NaN   \n",
       "2024-04-04                         NaN                         NaN   \n",
       "\n",
       "            neutral_avg_score_bitcoin  total_news_count_bitcoin  \\\n",
       "date_dt                                                           \n",
       "2021-04-15                   0.849137                      22.0   \n",
       "2021-04-16                   0.726584                      23.0   \n",
       "2021-04-17                   0.818327                       6.0   \n",
       "2021-04-18                        NaN                       NaN   \n",
       "2021-04-19                   0.899825                       9.0   \n",
       "...                               ...                       ...   \n",
       "2024-03-31                        NaN                       NaN   \n",
       "2024-04-02                        NaN                       NaN   \n",
       "2024-04-01                        NaN                       NaN   \n",
       "2024-04-03                        NaN                       NaN   \n",
       "2024-04-04                        NaN                       NaN   \n",
       "\n",
       "            total_news_score_bitcoin  ...  DTB3_lag2  T5YIE_lag1  T5YIE_lag2  \\\n",
       "date_dt                               ...                                      \n",
       "2021-04-15                  7.153077  ...       0.02        2.56        2.56   \n",
       "2021-04-16                 -5.216729  ...       0.02        2.56        2.56   \n",
       "2021-04-17                 -0.882590  ...       0.02        2.55        2.56   \n",
       "2021-04-18                       NaN  ...       0.02        2.57        2.55   \n",
       "2021-04-19                 -4.001451  ...       0.02        2.57        2.57   \n",
       "...                              ...  ...        ...         ...         ...   \n",
       "2024-03-31                       NaN  ...       5.23        2.38        2.38   \n",
       "2024-04-02                       NaN  ...       5.23        2.38        2.38   \n",
       "2024-04-01                       NaN  ...       5.23        2.38        2.38   \n",
       "2024-04-03                       NaN  ...       5.23        2.41        2.38   \n",
       "2024-04-04                       NaN  ...        NaN         NaN         NaN   \n",
       "\n",
       "            S&P500_Close_lag1  S&P500_Close_lag2  VIX_Close_lag1  \\\n",
       "date_dt                                                            \n",
       "2021-04-15        4124.660156        4124.660156           16.99   \n",
       "2021-04-16        4124.660156        4124.660156           16.99   \n",
       "2021-04-17        4170.419922        4124.660156           16.57   \n",
       "2021-04-18        4185.470215        4170.419922           16.25   \n",
       "2021-04-19        4185.470215        4185.470215           16.25   \n",
       "...                       ...                ...             ...   \n",
       "2024-03-31        5254.350098        5254.350098           13.01   \n",
       "2024-04-02        5254.350098        5254.350098           13.01   \n",
       "2024-04-01        5254.350098        5254.350098           13.01   \n",
       "2024-04-03        5243.770020        5254.350098           13.65   \n",
       "2024-04-04                NaN                NaN             NaN   \n",
       "\n",
       "            VIX_Close_lag2  S&P_Return_lag1  S&P_Return_lag2  gg_trend  \n",
       "date_dt                                                                 \n",
       "2021-04-15           16.99         0.011094         0.011094  0.137663  \n",
       "2021-04-16           16.99         0.011094         0.011094  0.137663  \n",
       "2021-04-17           16.99         0.011094         0.011094  0.137663  \n",
       "2021-04-18           16.57         0.003609         0.011094  0.018928  \n",
       "2021-04-19           16.25         0.000000         0.003609  0.018928  \n",
       "...                    ...              ...              ...       ...  \n",
       "2024-03-31           13.01         0.000000         0.001116       NaN  \n",
       "2024-04-02           13.01         0.000000         0.000000       NaN  \n",
       "2024-04-01           13.01         0.000000         0.000000       NaN  \n",
       "2024-04-03           13.01        -0.002014         0.000000       NaN  \n",
       "2024-04-04             NaN              NaN              NaN       NaN  \n",
       "\n",
       "[1086 rows x 68 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.concat(\n",
    "    [\n",
    "        btc_direction,\n",
    "        btc_magnitude,\n",
    "        bitcoin_news_lag1,\n",
    "        BB_sentiment_lag1,\n",
    "        coinbase_news_lag1,\n",
    "        non_sentiment_lag1,\n",
    "        gg_trend,\n",
    "    ],\n",
    "    axis=1,\n",
    "    join=\"outer\",\n",
    ")\n",
    "combined_df.index = pd.to_datetime(combined_df.index)\n",
    "combined_df = combined_df[combined_df.index >= pd.Timestamp(\"2021-04-15\")]\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.indexes.datetimes.DatetimeIndex"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(combined_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_missing = combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(\"../Data/combined_Lin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Direction</th>\n",
       "      <th>magnitude</th>\n",
       "      <th>positive_count_bitcoin</th>\n",
       "      <th>negative_count_bitcoin</th>\n",
       "      <th>neutral_count_bitcoin</th>\n",
       "      <th>positive_avg_score_bitcoin</th>\n",
       "      <th>negative_avg_score_bitcoin</th>\n",
       "      <th>neutral_avg_score_bitcoin</th>\n",
       "      <th>total_news_count_bitcoin</th>\n",
       "      <th>total_news_score_bitcoin</th>\n",
       "      <th>...</th>\n",
       "      <th>DTB3_lag2</th>\n",
       "      <th>T5YIE_lag1</th>\n",
       "      <th>T5YIE_lag2</th>\n",
       "      <th>S&amp;P500_Close_lag1</th>\n",
       "      <th>S&amp;P500_Close_lag2</th>\n",
       "      <th>VIX_Close_lag1</th>\n",
       "      <th>VIX_Close_lag2</th>\n",
       "      <th>S&amp;P_Return_lag1</th>\n",
       "      <th>S&amp;P_Return_lag2</th>\n",
       "      <th>gg_trend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-04-15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.907243</td>\n",
       "      <td>0.959677</td>\n",
       "      <td>0.849137</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.153077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.56</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>16.99</td>\n",
       "      <td>16.99</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.137663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.623001</td>\n",
       "      <td>0.895473</td>\n",
       "      <td>0.726584</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-5.216729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.56</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>16.99</td>\n",
       "      <td>16.99</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.137663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.725935</td>\n",
       "      <td>0.804262</td>\n",
       "      <td>0.818327</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.882590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.56</td>\n",
       "      <td>4170.419922</td>\n",
       "      <td>4124.660156</td>\n",
       "      <td>16.57</td>\n",
       "      <td>16.99</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.137663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.55</td>\n",
       "      <td>4185.470215</td>\n",
       "      <td>4170.419922</td>\n",
       "      <td>16.25</td>\n",
       "      <td>16.57</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.018928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.886130</td>\n",
       "      <td>0.962285</td>\n",
       "      <td>0.899825</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-4.001451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.57</td>\n",
       "      <td>4185.470215</td>\n",
       "      <td>4185.470215</td>\n",
       "      <td>16.25</td>\n",
       "      <td>16.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.018928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.23</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-02</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.23</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.23</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>13.01</td>\n",
       "      <td>13.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.23</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5243.770020</td>\n",
       "      <td>5254.350098</td>\n",
       "      <td>13.65</td>\n",
       "      <td>13.01</td>\n",
       "      <td>-0.002014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1086 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Direction  magnitude  positive_count_bitcoin  \\\n",
       "date_dt                                                    \n",
       "2021-04-15        1.0          6                    10.0   \n",
       "2021-04-16        0.0          1                     6.0   \n",
       "2021-04-17        0.0          3                     1.0   \n",
       "2021-04-18        0.0          1                     NaN   \n",
       "2021-04-19        0.0          4                     2.0   \n",
       "...               ...        ...                     ...   \n",
       "2024-03-31        1.0         10                     NaN   \n",
       "2024-04-02        0.0          1                     NaN   \n",
       "2024-04-01        NaN          1                     NaN   \n",
       "2024-04-03        NaN          7                     NaN   \n",
       "2024-04-04        NaN         10                     NaN   \n",
       "\n",
       "            negative_count_bitcoin  neutral_count_bitcoin  \\\n",
       "date_dt                                                     \n",
       "2021-04-15                     2.0                   10.0   \n",
       "2021-04-16                    10.0                    7.0   \n",
       "2021-04-17                     2.0                    3.0   \n",
       "2021-04-18                     NaN                    NaN   \n",
       "2021-04-19                     6.0                    1.0   \n",
       "...                            ...                    ...   \n",
       "2024-03-31                     NaN                    NaN   \n",
       "2024-04-02                     NaN                    NaN   \n",
       "2024-04-01                     NaN                    NaN   \n",
       "2024-04-03                     NaN                    NaN   \n",
       "2024-04-04                     NaN                    NaN   \n",
       "\n",
       "            positive_avg_score_bitcoin  negative_avg_score_bitcoin  \\\n",
       "date_dt                                                              \n",
       "2021-04-15                    0.907243                    0.959677   \n",
       "2021-04-16                    0.623001                    0.895473   \n",
       "2021-04-17                    0.725935                    0.804262   \n",
       "2021-04-18                         NaN                         NaN   \n",
       "2021-04-19                    0.886130                    0.962285   \n",
       "...                                ...                         ...   \n",
       "2024-03-31                         NaN                         NaN   \n",
       "2024-04-02                         NaN                         NaN   \n",
       "2024-04-01                         NaN                         NaN   \n",
       "2024-04-03                         NaN                         NaN   \n",
       "2024-04-04                         NaN                         NaN   \n",
       "\n",
       "            neutral_avg_score_bitcoin  total_news_count_bitcoin  \\\n",
       "date_dt                                                           \n",
       "2021-04-15                   0.849137                      22.0   \n",
       "2021-04-16                   0.726584                      23.0   \n",
       "2021-04-17                   0.818327                       6.0   \n",
       "2021-04-18                        NaN                       NaN   \n",
       "2021-04-19                   0.899825                       9.0   \n",
       "...                               ...                       ...   \n",
       "2024-03-31                        NaN                       NaN   \n",
       "2024-04-02                        NaN                       NaN   \n",
       "2024-04-01                        NaN                       NaN   \n",
       "2024-04-03                        NaN                       NaN   \n",
       "2024-04-04                        NaN                       NaN   \n",
       "\n",
       "            total_news_score_bitcoin  ...  DTB3_lag2  T5YIE_lag1  T5YIE_lag2  \\\n",
       "date_dt                               ...                                      \n",
       "2021-04-15                  7.153077  ...       0.02        2.56        2.56   \n",
       "2021-04-16                 -5.216729  ...       0.02        2.56        2.56   \n",
       "2021-04-17                 -0.882590  ...       0.02        2.55        2.56   \n",
       "2021-04-18                       NaN  ...       0.02        2.57        2.55   \n",
       "2021-04-19                 -4.001451  ...       0.02        2.57        2.57   \n",
       "...                              ...  ...        ...         ...         ...   \n",
       "2024-03-31                       NaN  ...       5.23        2.38        2.38   \n",
       "2024-04-02                       NaN  ...       5.23        2.38        2.38   \n",
       "2024-04-01                       NaN  ...       5.23        2.38        2.38   \n",
       "2024-04-03                       NaN  ...       5.23        2.41        2.38   \n",
       "2024-04-04                       NaN  ...        NaN         NaN         NaN   \n",
       "\n",
       "            S&P500_Close_lag1  S&P500_Close_lag2  VIX_Close_lag1  \\\n",
       "date_dt                                                            \n",
       "2021-04-15        4124.660156        4124.660156           16.99   \n",
       "2021-04-16        4124.660156        4124.660156           16.99   \n",
       "2021-04-17        4170.419922        4124.660156           16.57   \n",
       "2021-04-18        4185.470215        4170.419922           16.25   \n",
       "2021-04-19        4185.470215        4185.470215           16.25   \n",
       "...                       ...                ...             ...   \n",
       "2024-03-31        5254.350098        5254.350098           13.01   \n",
       "2024-04-02        5254.350098        5254.350098           13.01   \n",
       "2024-04-01        5254.350098        5254.350098           13.01   \n",
       "2024-04-03        5243.770020        5254.350098           13.65   \n",
       "2024-04-04                NaN                NaN             NaN   \n",
       "\n",
       "            VIX_Close_lag2  S&P_Return_lag1  S&P_Return_lag2  gg_trend  \n",
       "date_dt                                                                 \n",
       "2021-04-15           16.99         0.011094         0.011094  0.137663  \n",
       "2021-04-16           16.99         0.011094         0.011094  0.137663  \n",
       "2021-04-17           16.99         0.011094         0.011094  0.137663  \n",
       "2021-04-18           16.57         0.003609         0.011094  0.018928  \n",
       "2021-04-19           16.25         0.000000         0.003609  0.018928  \n",
       "...                    ...              ...              ...       ...  \n",
       "2024-03-31           13.01         0.000000         0.001116       NaN  \n",
       "2024-04-02           13.01         0.000000         0.000000       NaN  \n",
       "2024-04-01           13.01         0.000000         0.000000       NaN  \n",
       "2024-04-03           13.01        -0.002014         0.000000       NaN  \n",
       "2024-04-04             NaN              NaN              NaN       NaN  \n",
       "\n",
       "[1086 rows x 68 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns with more than 100 missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1086, 50)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_per_column = combined_df.isna().sum()\n",
    "filtered_columns = missing_values_per_column[missing_values_per_column < 100].index\n",
    "combined_df_filtered = combined_df[filtered_columns]\n",
    "combined_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2j/k_c_x4ps7n352tzdqvckkvb80000gn/T/ipykernel_42034/1165557484.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_df_filtered.dropna(inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1075, 50)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df_filtered.dropna(inplace=True)\n",
    "# combined_df_filtered.to_csv(\"../../combined_Lin_filtered.csv\")\n",
    "combined_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direction class distribution:\n",
      "Direction\n",
      "0.0    0.506977\n",
      "1.0    0.493023\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Direction class distribution:\")\n",
    "print(combined_df_filtered[\"Direction\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnitude class distribution:\n",
      "magnitude\n",
      "5     0.222326\n",
      "6     0.186977\n",
      "7     0.104186\n",
      "4     0.099535\n",
      "1     0.095814\n",
      "10    0.095814\n",
      "8     0.063256\n",
      "3     0.053953\n",
      "9     0.042791\n",
      "2     0.035349\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Magnitude class distribution:\")\n",
    "print(combined_df_filtered[\"magnitude\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, val, test split (80%), earlier as train and val, later as test w.r.t data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = len(combined_df_filtered)\n",
    "train_val_split_index = int(total_rows * 0.8)\n",
    "\n",
    "train_val_df = combined_df_filtered[:train_val_split_index]\n",
    "test_df = combined_df_filtered[train_val_split_index:]\n",
    "\n",
    "train_split_index = int(len(train_val_df) * 0.8)\n",
    "\n",
    "train_df = train_val_df[:train_split_index]\n",
    "val_df = train_val_df[train_split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop([\"Direction\", \"magnitude\"], axis=1)\n",
    "X_val = val_df.drop([\"Direction\", \"magnitude\"], axis=1)\n",
    "X_test = test_df.drop([\"Direction\", \"magnitude\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction Data (balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_d = train_df[\"Direction\"]\n",
    "y_val_d = val_df[\"Direction\"]\n",
    "y_test_d = test_df[\"Direction\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost for Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 51.16%\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "xgb_model.fit(X_train, y_train_d)\n",
    "y_val_pred = xgb_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val_d, y_val_pred)\n",
    "print(f\"Validation Accuracy: {100* val_accuracy:.4}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2700 candidates, totalling 13500 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric=&#x27;logloss&#x27;, feature_types=None,\n",
       "                                     gamma=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=...\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.8, 0.9, 1.0],\n",
       "                         &#x27;lambda&#x27;: [1, 1.5, 2],\n",
       "                         &#x27;learning_rate&#x27;: [0.01, 0.05, 0.1, 0.15, 0.2],\n",
       "                         &#x27;max_depth&#x27;: [3, 4, 5, 6],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 200, 300, 400],\n",
       "                         &#x27;subsample&#x27;: [0.8, 0.9, 1.0]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric=&#x27;logloss&#x27;, feature_types=None,\n",
       "                                     gamma=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=...\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.8, 0.9, 1.0],\n",
       "                         &#x27;lambda&#x27;: [1, 1.5, 2],\n",
       "                         &#x27;learning_rate&#x27;: [0.01, 0.05, 0.1, 0.15, 0.2],\n",
       "                         &#x27;max_depth&#x27;: [3, 4, 5, 6],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 200, 300, 400],\n",
       "                         &#x27;subsample&#x27;: [0.8, 0.9, 1.0]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric='logloss', feature_types=None,\n",
       "                                     gamma=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=...\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.8, 0.9, 1.0],\n",
       "                         'lambda': [1, 1.5, 2],\n",
       "                         'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
       "                         'max_depth': [3, 4, 5, 6],\n",
       "                         'n_estimators': [50, 100, 200, 300, 400],\n",
       "                         'subsample': [0.8, 0.9, 1.0]},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb_grid = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200, 300, 400],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    \"max_depth\": [3, 4, 5, 6],\n",
    "    \"subsample\": [0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 0.9, 1.0],\n",
    "    \"alpha\": [0, 0.5, 1, 1.5, 2],  # L1\n",
    "    \"lambda\": [1, 1.5, 2],  # L2\n",
    "}\n",
    "\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    estimator=xgb_grid,\n",
    "    param_grid=xgb_param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "xgb_grid_search.fit(X_train, y_train_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'colsample_bytree': 0.9, 'lambda': 2, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Best accuracy found: 55.53%\n",
      "Validation Accuracy with best parameters: 51.16%\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters found: \", xgb_grid_search.best_params_)\n",
    "print(f\"Best accuracy found: {100 * xgb_grid_search.best_score_:.4}%\")\n",
    "y_val_pred_best = xgb_grid_search.predict(X_val)\n",
    "val_accuracy_best = accuracy_score(y_val_d, y_val_pred_best)\n",
    "print(f\"Validation Accuracy with best parameters: {100* val_accuracy_best:.4}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude Data (imbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_m = train_df[\"magnitude\"].astype(\"category\").cat.codes\n",
    "y_val_m = val_df[\"magnitude\"].astype(\"category\").cat.codes\n",
    "y_test_m = test_df[\"magnitude\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.20348837209302326\n"
     ]
    }
   ],
   "source": [
    "xgb_model_m = xgb.XGBClassifier(\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=10,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"mlogloss\",\n",
    ")\n",
    "\n",
    "xgb_model_m.fit(X_train_scaled, y_train_m)\n",
    "y_pred_m = xgb_model_m.predict(X_val_scaled)\n",
    "\n",
    "accuracy_m = accuracy_score(y_val_m, y_pred_m)\n",
    "print(f\"Accuracy: {accuracy_m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maginitude Precision: 0.24957167733962476\n",
      "Maginitude Recall: 0.20348837209302326\n",
      "Maginitude F1 Score: 0.21581336232541865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lin/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "precision_m = precision_score(y_val_m, y_pred_m, average=\"weighted\")\n",
    "recall_m = recall_score(y_val_m, y_pred_m, average=\"weighted\")\n",
    "f1_m = f1_score(y_val_m, y_pred_m, average=\"weighted\")\n",
    "\n",
    "print(f\"Maginitude Precision: {precision_m}\")\n",
    "print(f\"Maginitude Recall: {recall_m}\")\n",
    "print(f\"Maginitude F1 Score: {f1_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 11664 candidates, totalling 34992 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric=&#x27;logloss&#x27;, feature_types=None,\n",
       "                                     gamma=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=...\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.8, 0.9, 1.0],\n",
       "                         &#x27;gamma&#x27;: [0, 0.1, 0.2], &#x27;lambda&#x27;: [1, 1.5, 2],\n",
       "                         &#x27;learning_rate&#x27;: [0.01, 0.05, 0.1, 0.15],\n",
       "                         &#x27;max_depth&#x27;: [4, 5, 6], &#x27;min_child_weight&#x27;: [1, 3, 5],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300, 400],\n",
       "                         &#x27;subsample&#x27;: [0.8, 0.9, 1.0]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric=&#x27;logloss&#x27;, feature_types=None,\n",
       "                                     gamma=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=...\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.8, 0.9, 1.0],\n",
       "                         &#x27;gamma&#x27;: [0, 0.1, 0.2], &#x27;lambda&#x27;: [1, 1.5, 2],\n",
       "                         &#x27;learning_rate&#x27;: [0.01, 0.05, 0.1, 0.15],\n",
       "                         &#x27;max_depth&#x27;: [4, 5, 6], &#x27;min_child_weight&#x27;: [1, 3, 5],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300, 400],\n",
       "                         &#x27;subsample&#x27;: [0.8, 0.9, 1.0]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric='logloss', feature_types=None,\n",
       "                                     gamma=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=...\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.8, 0.9, 1.0],\n",
       "                         'gamma': [0, 0.1, 0.2], 'lambda': [1, 1.5, 2],\n",
       "                         'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
       "                         'max_depth': [4, 5, 6], 'min_child_weight': [1, 3, 5],\n",
       "                         'n_estimators': [100, 200, 300, 400],\n",
       "                         'subsample': [0.8, 0.9, 1.0]},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_grid_m = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "xgb_param_grid_m = {\n",
    "    \"n_estimators\": [100, 200, 300, 400],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.15],\n",
    "    \"max_depth\": [4, 5, 6],\n",
    "    \"subsample\": [0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 0.9, 1.0],\n",
    "    \"lambda\": [1, 1.5, 2],  # L2\n",
    "    \"gamma\": [0, 0.1, 0.2],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "}\n",
    "\n",
    "xgb_grid_search_m = GridSearchCV(\n",
    "    estimator=xgb_grid_m,\n",
    "    param_grid=xgb_param_grid_m,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "xgb_grid_search_m.fit(X_train_scaled, y_train_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'colsample_bytree': 0.8, 'gamma': 0, 'lambda': 1, 'learning_rate': 0.15, 'max_depth': 5, 'min_child_weight': 5, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Best accuracy found:  0.136630593000443\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters found: \", xgb_grid_search_m.best_params_)\n",
    "print(\"Best accuracy found: \", xgb_grid_search_m.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision GS (Weighted): 0.21657784161698088\n",
      "Recall GS (Weighted): 0.21511627906976744\n",
      "F1 Score GS (Weighted): 0.19221034209848686\n",
      "Accuracy GS: 0.21511627906976744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lin/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred_val = xgb_grid_search_m.best_estimator_.predict(X_val_scaled)\n",
    "\n",
    "precision = precision_score(y_val_m, y_pred_val, average=\"weighted\")\n",
    "recall = recall_score(y_val_m, y_pred_val, average=\"weighted\")\n",
    "f1 = f1_score(y_val_m, y_pred_val, average=\"weighted\")\n",
    "accuracy = accuracy_score(y_val_m, y_pred_val)\n",
    "\n",
    "print(f\"Precision GS (Weighted): {precision}\")\n",
    "print(f\"Recall GS (Weighted): {recall}\")\n",
    "print(f\"F1 Score GS (Weighted): {f1}\")\n",
    "print(f\"Accuracy GS: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_m_ohe = to_categorical(y_train_m, num_classes=10)\n",
    "y_val_m_ohe = to_categorical(y_val_m, num_classes=10)\n",
    "X_train_scaled_cnn = X_train_scaled.reshape(\n",
    "    (X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    ")\n",
    "X_val_scaled_cnn = X_val_scaled.reshape(\n",
    "    (X_val_scaled.shape[0], X_val_scaled.shape[1], 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 2.1689 - accuracy: 0.2195 - val_loss: 1.9297 - val_accuracy: 0.3256\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 2.0460 - accuracy: 0.2456 - val_loss: 1.9547 - val_accuracy: 0.3256\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 2.0313 - accuracy: 0.2558 - val_loss: 1.9277 - val_accuracy: 0.3256\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2.0007 - accuracy: 0.2863 - val_loss: 1.9453 - val_accuracy: 0.3256\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.9835 - accuracy: 0.2718 - val_loss: 1.9374 - val_accuracy: 0.3198\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.9606 - accuracy: 0.2980 - val_loss: 1.9526 - val_accuracy: 0.3430\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.9459 - accuracy: 0.3110 - val_loss: 1.9958 - val_accuracy: 0.3198\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.9362 - accuracy: 0.3067 - val_loss: 1.9679 - val_accuracy: 0.3314\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.9172 - accuracy: 0.3067 - val_loss: 1.9609 - val_accuracy: 0.3256\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1.8979 - accuracy: 0.3140 - val_loss: 2.0044 - val_accuracy: 0.3198\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "Accuracy (CNN): 0.31976744186046513\n",
      "F1 Score (CNN): 0.2041478426973106\n"
     ]
    }
   ],
   "source": [
    "model_cnn = Sequential(\n",
    "    [\n",
    "        Conv1D(\n",
    "            filters=64,\n",
    "            kernel_size=2,\n",
    "            activation=\"relu\",\n",
    "            input_shape=(X_train_scaled_cnn.shape[1], 1),\n",
    "        ),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(50, activation=\"relu\"),\n",
    "        Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_cnn.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history_cnn = model_cnn.fit(\n",
    "    X_train_scaled_cnn,\n",
    "    y_train_m_ohe,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_scaled_cnn, y_val_m_ohe),\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "predictions_cnn = model_cnn.predict(X_val_scaled_cnn)\n",
    "accuracy_cnn = accuracy_score(\n",
    "    y_val_m_ohe.argmax(axis=1), predictions_cnn.argmax(axis=1)\n",
    ")\n",
    "f1_cnn = f1_score(\n",
    "    y_val_m_ohe.argmax(axis=1), predictions_cnn.argmax(axis=1), average=\"weighted\"\n",
    ")\n",
    "\n",
    "print(f\"Accuracy (CNN): {accuracy_cnn}\")\n",
    "print(f\"F1 Score (CNN): {f1_cnn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import RandomSearch\n",
    "\n",
    "\n",
    "# def build_cnn(hp):\n",
    "#     model = Sequential()\n",
    "#     model.add(\n",
    "#         Conv1D(\n",
    "#             filters=hp.Int(\"filters\", min_value=32, max_value=128, step=32),\n",
    "#             kernel_size=hp.Choice(\"kernel_size\", values=[2, 3, 4]),\n",
    "#             activation=\"relu\",\n",
    "#             input_shape=(X_train_scaled_cnn.shape[1], 1),\n",
    "#         )\n",
    "#     )\n",
    "#     # model.add(MaxPooling1D(pool_size=2))\n",
    "#     # model.add(Flatten())\n",
    "#     # model.add(\n",
    "#     #     Dense(\n",
    "#     #         units=hp.Int(\"dense_units\", min_value=50, max_value=100, step=10),\n",
    "#     #         activation=\"relu\",\n",
    "#     #     )\n",
    "#     # )\n",
    "#     model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=Adam(\n",
    "#             hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"LOG\")\n",
    "#         ),\n",
    "#         loss=\"categorical_crossentropy\",\n",
    "#         metrics=[\"accuracy\"],\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "\n",
    "def build_cnn(hp):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Conv1D(\n",
    "            filters=hp.Int(\"filters_1\", min_value=32, max_value=128, step=32),\n",
    "            kernel_size=hp.Choice(\"kernel_size_1\", values=[2, 3, 4]),\n",
    "            activation=\"relu\",\n",
    "            input_shape=(X_train_scaled_cnn.shape[1], 1),\n",
    "        )\n",
    "    )\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(\n",
    "        Conv1D(\n",
    "            filters=hp.Int(\"filters_2\", min_value=32, max_value=128, step=32),\n",
    "            kernel_size=hp.Choice(\"kernel_size_2\", values=[2, 3, 4]),\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "    )\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units=hp.Int(\"dense_units\", min_value=50, max_value=100, step=10),\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "    )\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"LOG\")\n",
    "        ),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 00m 34s]\n",
      "val_accuracy: 0.3255814015865326\n",
      "\n",
      "Best val_accuracy So Far: 0.3720930218696594\n",
      "Total elapsed time: 00h 34m 44s\n",
      "Best hyperparameters found:\n",
      "- Filters 1: 128\n",
      "- Kernel size 1: 2\n",
      "- Filters 2: 32\n",
      "- Kernel size 2: 4\n",
      "- Learning rate: 0.0001335476560946422\n"
     ]
    }
   ],
   "source": [
    "tuner_cnn = RandomSearch(\n",
    "    build_cnn,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=50,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"Lin_dir\",\n",
    "    project_name=\"keras_tuner_bitcoin\",\n",
    ")\n",
    "\n",
    "tuner_cnn.search(\n",
    "    X_train_scaled_cnn,\n",
    "    y_train_m_ohe,\n",
    "    epochs=200,\n",
    "    validation_data=(X_val_scaled_cnn, y_val_m_ohe),\n",
    ")\n",
    "\n",
    "best_hps_cnn = tuner_cnn.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(f\"- Filters 1: {best_hps_cnn.get('filters_1')}\")\n",
    "print(f\"- Kernel size 1: {best_hps_cnn.get('kernel_size_1')}\")\n",
    "print(f\"- Filters 2: {best_hps_cnn.get('filters_2')}\")\n",
    "print(f\"- Kernel size 2: {best_hps_cnn.get('kernel_size_2')}\")\n",
    "print(f\"- Learning rate: {best_hps_cnn.get('learning_rate')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step\n",
      "Validation Accuracy: 0.3720930218696594\n",
      "Validation F1 Score: 0.27371795533177884\n"
     ]
    }
   ],
   "source": [
    "best_cnn = tuner_cnn.get_best_models(num_models=1)[0]\n",
    "val_loss, val_accuracy = best_cnn.evaluate(X_val_scaled_cnn, y_val_m_ohe, verbose=0)\n",
    "\n",
    "val_predictions = best_cnn.predict(X_val_scaled_cnn)\n",
    "val_pred_classes = np.argmax(val_predictions, axis=1)\n",
    "val_true_classes = np.argmax(y_val_m_ohe, axis=1)\n",
    "\n",
    "val_f1 = f1_score(val_true_classes, val_pred_classes, average=\"weighted\")\n",
    "\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation F1 Score: {val_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.9449 - accuracy: 0.7195 - val_loss: 2.6574 - val_accuracy: 0.2674\n",
      "Epoch 2/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.9449 - accuracy: 0.7195 - val_loss: 2.6670 - val_accuracy: 0.2733\n",
      "Epoch 3/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.9411 - accuracy: 0.7238 - val_loss: 2.6527 - val_accuracy: 0.2907\n",
      "Epoch 4/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.9389 - accuracy: 0.7238 - val_loss: 2.6869 - val_accuracy: 0.2674\n",
      "Epoch 5/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.9337 - accuracy: 0.7224 - val_loss: 2.6858 - val_accuracy: 0.2849\n",
      "Epoch 6/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.9338 - accuracy: 0.7180 - val_loss: 2.7037 - val_accuracy: 0.2674\n",
      "Epoch 7/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.9247 - accuracy: 0.7180 - val_loss: 2.6707 - val_accuracy: 0.2733\n",
      "Epoch 8/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.9212 - accuracy: 0.7253 - val_loss: 2.7354 - val_accuracy: 0.2500\n",
      "Epoch 9/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.9191 - accuracy: 0.7282 - val_loss: 2.7071 - val_accuracy: 0.2674\n",
      "Epoch 10/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.9137 - accuracy: 0.7326 - val_loss: 2.7181 - val_accuracy: 0.2965\n",
      "Epoch 11/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.9113 - accuracy: 0.7355 - val_loss: 2.7059 - val_accuracy: 0.2616\n",
      "Epoch 12/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.9052 - accuracy: 0.7355 - val_loss: 2.6819 - val_accuracy: 0.2907\n",
      "Epoch 13/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.9085 - accuracy: 0.7384 - val_loss: 2.7624 - val_accuracy: 0.2616\n",
      "Epoch 14/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.9055 - accuracy: 0.7166 - val_loss: 2.7215 - val_accuracy: 0.2733\n",
      "Epoch 15/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.9106 - accuracy: 0.7224 - val_loss: 2.7215 - val_accuracy: 0.2849\n",
      "Epoch 16/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.9028 - accuracy: 0.7267 - val_loss: 2.7681 - val_accuracy: 0.2500\n",
      "Epoch 17/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.8937 - accuracy: 0.7413 - val_loss: 2.7502 - val_accuracy: 0.3081\n",
      "Epoch 18/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.8954 - accuracy: 0.7238 - val_loss: 2.7535 - val_accuracy: 0.2500\n",
      "Epoch 19/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.8979 - accuracy: 0.7384 - val_loss: 2.7703 - val_accuracy: 0.2674\n",
      "Epoch 20/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.8795 - accuracy: 0.7471 - val_loss: 2.7816 - val_accuracy: 0.2558\n",
      "Epoch 21/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.8738 - accuracy: 0.7471 - val_loss: 2.7477 - val_accuracy: 0.2849\n",
      "Epoch 22/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.8703 - accuracy: 0.7456 - val_loss: 2.7908 - val_accuracy: 0.2674\n",
      "Epoch 23/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.8707 - accuracy: 0.7558 - val_loss: 2.7702 - val_accuracy: 0.2907\n",
      "Epoch 24/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.8673 - accuracy: 0.7471 - val_loss: 2.7933 - val_accuracy: 0.2616\n",
      "Epoch 25/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.8685 - accuracy: 0.7471 - val_loss: 2.7743 - val_accuracy: 0.2733\n",
      "Epoch 26/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.8627 - accuracy: 0.7515 - val_loss: 2.7881 - val_accuracy: 0.2733\n",
      "Epoch 27/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.8558 - accuracy: 0.7573 - val_loss: 2.7855 - val_accuracy: 0.2791\n",
      "Epoch 28/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.8508 - accuracy: 0.7645 - val_loss: 2.7947 - val_accuracy: 0.2733\n",
      "Epoch 29/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.8539 - accuracy: 0.7485 - val_loss: 2.7909 - val_accuracy: 0.2907\n",
      "Epoch 30/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.8511 - accuracy: 0.7544 - val_loss: 2.8260 - val_accuracy: 0.2733\n",
      "Epoch 31/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.8485 - accuracy: 0.7616 - val_loss: 2.7964 - val_accuracy: 0.2674\n",
      "Epoch 32/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.8399 - accuracy: 0.7573 - val_loss: 2.8542 - val_accuracy: 0.2849\n",
      "Epoch 33/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.8375 - accuracy: 0.7791 - val_loss: 2.7867 - val_accuracy: 0.2733\n",
      "Epoch 34/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.8327 - accuracy: 0.7645 - val_loss: 2.8724 - val_accuracy: 0.2674\n",
      "Epoch 35/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.8275 - accuracy: 0.7674 - val_loss: 2.8065 - val_accuracy: 0.2907\n",
      "Epoch 36/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.8292 - accuracy: 0.7645 - val_loss: 2.8593 - val_accuracy: 0.2733\n",
      "Epoch 37/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.8134 - accuracy: 0.7776 - val_loss: 2.8540 - val_accuracy: 0.2849\n",
      "Epoch 38/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.8165 - accuracy: 0.7733 - val_loss: 2.8592 - val_accuracy: 0.2674\n",
      "Epoch 39/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.8121 - accuracy: 0.7776 - val_loss: 2.8522 - val_accuracy: 0.2616\n",
      "Epoch 40/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.8131 - accuracy: 0.7631 - val_loss: 2.8514 - val_accuracy: 0.2907\n",
      "Epoch 41/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.8045 - accuracy: 0.7747 - val_loss: 2.8697 - val_accuracy: 0.2733\n",
      "Epoch 42/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.8057 - accuracy: 0.7747 - val_loss: 2.8786 - val_accuracy: 0.2733\n",
      "Epoch 43/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.7983 - accuracy: 0.7863 - val_loss: 2.8520 - val_accuracy: 0.2674\n",
      "Epoch 44/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.7996 - accuracy: 0.7776 - val_loss: 2.9015 - val_accuracy: 0.2733\n",
      "Epoch 45/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.7976 - accuracy: 0.7747 - val_loss: 2.8883 - val_accuracy: 0.2849\n",
      "Epoch 46/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.8029 - accuracy: 0.7587 - val_loss: 2.9624 - val_accuracy: 0.2267\n",
      "Epoch 47/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.7910 - accuracy: 0.7660 - val_loss: 2.8793 - val_accuracy: 0.2849\n",
      "Epoch 48/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.7878 - accuracy: 0.7878 - val_loss: 2.9797 - val_accuracy: 0.2267\n",
      "Epoch 49/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.7903 - accuracy: 0.7747 - val_loss: 2.8965 - val_accuracy: 0.2907\n",
      "Epoch 50/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.7716 - accuracy: 0.7892 - val_loss: 2.9181 - val_accuracy: 0.2616\n",
      "Epoch 51/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.7760 - accuracy: 0.7965 - val_loss: 2.9117 - val_accuracy: 0.2674\n",
      "Epoch 52/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.7870 - accuracy: 0.7747 - val_loss: 2.9156 - val_accuracy: 0.2965\n",
      "Epoch 53/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.7794 - accuracy: 0.7863 - val_loss: 2.9166 - val_accuracy: 0.2616\n",
      "Epoch 54/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.7709 - accuracy: 0.7762 - val_loss: 2.9183 - val_accuracy: 0.2733\n",
      "Epoch 55/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.7700 - accuracy: 0.7776 - val_loss: 2.9306 - val_accuracy: 0.2965\n",
      "Epoch 56/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.7705 - accuracy: 0.7878 - val_loss: 2.9622 - val_accuracy: 0.2500\n",
      "Epoch 57/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.7564 - accuracy: 0.7892 - val_loss: 2.9715 - val_accuracy: 0.2849\n",
      "Epoch 58/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.7561 - accuracy: 0.8038 - val_loss: 2.9288 - val_accuracy: 0.2733\n",
      "Epoch 59/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.7497 - accuracy: 0.7878 - val_loss: 2.9625 - val_accuracy: 0.2907\n",
      "Epoch 60/1000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7532 - accuracy: 0.7892 - val_loss: 2.9272 - val_accuracy: 0.2791\n",
      "Epoch 61/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.7426 - accuracy: 0.8009 - val_loss: 2.9614 - val_accuracy: 0.2965\n",
      "Epoch 62/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.7405 - accuracy: 0.8052 - val_loss: 2.9725 - val_accuracy: 0.2616\n",
      "Epoch 63/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.7362 - accuracy: 0.8081 - val_loss: 2.9724 - val_accuracy: 0.2733\n",
      "Epoch 64/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.7288 - accuracy: 0.8154 - val_loss: 2.9736 - val_accuracy: 0.2674\n",
      "Epoch 65/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.7346 - accuracy: 0.7965 - val_loss: 2.9725 - val_accuracy: 0.2791\n",
      "Epoch 66/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.7259 - accuracy: 0.8023 - val_loss: 3.0107 - val_accuracy: 0.2849\n",
      "Epoch 67/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.7264 - accuracy: 0.8081 - val_loss: 2.9829 - val_accuracy: 0.2616\n",
      "Epoch 68/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.7191 - accuracy: 0.8038 - val_loss: 2.9896 - val_accuracy: 0.2733\n",
      "Epoch 69/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.7199 - accuracy: 0.8067 - val_loss: 2.9970 - val_accuracy: 0.2674\n",
      "Epoch 70/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.7219 - accuracy: 0.7980 - val_loss: 2.9886 - val_accuracy: 0.2791\n",
      "Epoch 71/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.7127 - accuracy: 0.8227 - val_loss: 2.9948 - val_accuracy: 0.2849\n",
      "Epoch 72/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.7095 - accuracy: 0.8125 - val_loss: 3.0302 - val_accuracy: 0.2674\n",
      "Epoch 73/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.7085 - accuracy: 0.8125 - val_loss: 2.9956 - val_accuracy: 0.2733\n",
      "Epoch 74/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.7089 - accuracy: 0.7951 - val_loss: 3.0455 - val_accuracy: 0.2849\n",
      "Epoch 75/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.6987 - accuracy: 0.8154 - val_loss: 3.0389 - val_accuracy: 0.2616\n",
      "Epoch 76/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.7107 - accuracy: 0.7994 - val_loss: 3.0467 - val_accuracy: 0.2733\n",
      "Epoch 77/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.7063 - accuracy: 0.8125 - val_loss: 3.0212 - val_accuracy: 0.2791\n",
      "Epoch 78/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.7046 - accuracy: 0.8009 - val_loss: 3.1084 - val_accuracy: 0.2791\n",
      "Epoch 79/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.7001 - accuracy: 0.8067 - val_loss: 3.0457 - val_accuracy: 0.2907\n",
      "Epoch 80/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6945 - accuracy: 0.8154 - val_loss: 3.1193 - val_accuracy: 0.2733\n",
      "Epoch 81/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6795 - accuracy: 0.8285 - val_loss: 3.0314 - val_accuracy: 0.2733\n",
      "Epoch 82/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6765 - accuracy: 0.8212 - val_loss: 3.0680 - val_accuracy: 0.2674\n",
      "Epoch 83/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6788 - accuracy: 0.8227 - val_loss: 3.0682 - val_accuracy: 0.3023\n",
      "Epoch 84/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6790 - accuracy: 0.8299 - val_loss: 3.1421 - val_accuracy: 0.2558\n",
      "Epoch 85/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6941 - accuracy: 0.8183 - val_loss: 3.1367 - val_accuracy: 0.2733\n",
      "Epoch 86/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6701 - accuracy: 0.8328 - val_loss: 3.0641 - val_accuracy: 0.2791\n",
      "Epoch 87/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6717 - accuracy: 0.8169 - val_loss: 3.1251 - val_accuracy: 0.2849\n",
      "Epoch 88/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6651 - accuracy: 0.8198 - val_loss: 3.1164 - val_accuracy: 0.2733\n",
      "Epoch 89/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6577 - accuracy: 0.8372 - val_loss: 3.1216 - val_accuracy: 0.2791\n",
      "Epoch 90/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6520 - accuracy: 0.8299 - val_loss: 3.1020 - val_accuracy: 0.2791\n",
      "Epoch 91/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6527 - accuracy: 0.8328 - val_loss: 3.1487 - val_accuracy: 0.2791\n",
      "Epoch 92/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6532 - accuracy: 0.8314 - val_loss: 3.1495 - val_accuracy: 0.2558\n",
      "Epoch 93/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6588 - accuracy: 0.8256 - val_loss: 3.1434 - val_accuracy: 0.2674\n",
      "Epoch 94/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6606 - accuracy: 0.8081 - val_loss: 3.1591 - val_accuracy: 0.2733\n",
      "Epoch 95/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6466 - accuracy: 0.8387 - val_loss: 3.1468 - val_accuracy: 0.2907\n",
      "Epoch 96/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.6417 - accuracy: 0.8416 - val_loss: 3.1511 - val_accuracy: 0.2674\n",
      "Epoch 97/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6423 - accuracy: 0.8314 - val_loss: 3.1999 - val_accuracy: 0.2616\n",
      "Epoch 98/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6485 - accuracy: 0.8256 - val_loss: 3.1313 - val_accuracy: 0.2849\n",
      "Epoch 99/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6374 - accuracy: 0.8343 - val_loss: 3.1593 - val_accuracy: 0.2674\n",
      "Epoch 100/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.6384 - accuracy: 0.8270 - val_loss: 3.1963 - val_accuracy: 0.2849\n",
      "Epoch 101/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6372 - accuracy: 0.8445 - val_loss: 3.1338 - val_accuracy: 0.2791\n",
      "Epoch 102/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.6399 - accuracy: 0.8256 - val_loss: 3.2213 - val_accuracy: 0.2616\n",
      "Epoch 103/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.6272 - accuracy: 0.8358 - val_loss: 3.1795 - val_accuracy: 0.2907\n",
      "Epoch 104/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.6145 - accuracy: 0.8488 - val_loss: 3.2001 - val_accuracy: 0.2733\n",
      "Epoch 105/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6117 - accuracy: 0.8387 - val_loss: 3.2023 - val_accuracy: 0.2791\n",
      "Epoch 106/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6144 - accuracy: 0.8459 - val_loss: 3.2474 - val_accuracy: 0.2674\n",
      "Epoch 107/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6165 - accuracy: 0.8445 - val_loss: 3.1776 - val_accuracy: 0.2965\n",
      "Epoch 108/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6062 - accuracy: 0.8503 - val_loss: 3.2308 - val_accuracy: 0.2558\n",
      "Epoch 109/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6043 - accuracy: 0.8517 - val_loss: 3.2337 - val_accuracy: 0.2849\n",
      "Epoch 110/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6015 - accuracy: 0.8445 - val_loss: 3.2126 - val_accuracy: 0.2791\n",
      "Epoch 111/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5954 - accuracy: 0.8561 - val_loss: 3.2201 - val_accuracy: 0.2849\n",
      "Epoch 112/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5906 - accuracy: 0.8532 - val_loss: 3.2348 - val_accuracy: 0.2791\n",
      "Epoch 113/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5916 - accuracy: 0.8459 - val_loss: 3.2203 - val_accuracy: 0.2733\n",
      "Epoch 114/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5906 - accuracy: 0.8576 - val_loss: 3.2575 - val_accuracy: 0.2733\n",
      "Epoch 115/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5879 - accuracy: 0.8488 - val_loss: 3.2368 - val_accuracy: 0.3023\n",
      "Epoch 116/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5823 - accuracy: 0.8576 - val_loss: 3.2834 - val_accuracy: 0.2674\n",
      "Epoch 117/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.5877 - accuracy: 0.8590 - val_loss: 3.2697 - val_accuracy: 0.3023\n",
      "Epoch 118/1000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.5857 - accuracy: 0.8503 - val_loss: 3.2977 - val_accuracy: 0.2674\n",
      "Epoch 119/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5767 - accuracy: 0.8561 - val_loss: 3.2733 - val_accuracy: 0.2849\n",
      "Epoch 120/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5776 - accuracy: 0.8634 - val_loss: 3.2885 - val_accuracy: 0.2907\n",
      "Epoch 121/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5718 - accuracy: 0.8561 - val_loss: 3.2708 - val_accuracy: 0.2791\n",
      "Epoch 122/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5767 - accuracy: 0.8561 - val_loss: 3.2865 - val_accuracy: 0.2907\n",
      "Epoch 123/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5693 - accuracy: 0.8561 - val_loss: 3.3117 - val_accuracy: 0.2849\n",
      "Epoch 124/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5675 - accuracy: 0.8706 - val_loss: 3.3048 - val_accuracy: 0.2965\n",
      "Epoch 125/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5668 - accuracy: 0.8503 - val_loss: 3.3150 - val_accuracy: 0.2965\n",
      "Epoch 126/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5610 - accuracy: 0.8721 - val_loss: 3.3390 - val_accuracy: 0.2733\n",
      "Epoch 127/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5652 - accuracy: 0.8547 - val_loss: 3.3166 - val_accuracy: 0.2849\n",
      "Epoch 128/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5521 - accuracy: 0.8648 - val_loss: 3.3086 - val_accuracy: 0.2791\n",
      "Epoch 129/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5551 - accuracy: 0.8706 - val_loss: 3.3770 - val_accuracy: 0.2674\n",
      "Epoch 130/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5525 - accuracy: 0.8750 - val_loss: 3.3291 - val_accuracy: 0.2849\n",
      "Epoch 131/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5532 - accuracy: 0.8634 - val_loss: 3.3624 - val_accuracy: 0.2733\n",
      "Epoch 132/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5421 - accuracy: 0.8619 - val_loss: 3.3382 - val_accuracy: 0.2965\n",
      "Epoch 133/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5409 - accuracy: 0.8721 - val_loss: 3.3864 - val_accuracy: 0.2733\n",
      "Epoch 134/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5390 - accuracy: 0.8735 - val_loss: 3.3719 - val_accuracy: 0.2674\n",
      "Epoch 135/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5395 - accuracy: 0.8648 - val_loss: 3.3422 - val_accuracy: 0.2907\n",
      "Epoch 136/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.5352 - accuracy: 0.8692 - val_loss: 3.4044 - val_accuracy: 0.2791\n",
      "Epoch 137/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5311 - accuracy: 0.8721 - val_loss: 3.3872 - val_accuracy: 0.2849\n",
      "Epoch 138/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5283 - accuracy: 0.8750 - val_loss: 3.3385 - val_accuracy: 0.2849\n",
      "Epoch 139/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5283 - accuracy: 0.8765 - val_loss: 3.4469 - val_accuracy: 0.2791\n",
      "Epoch 140/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5229 - accuracy: 0.8605 - val_loss: 3.4150 - val_accuracy: 0.2849\n",
      "Epoch 141/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5240 - accuracy: 0.8735 - val_loss: 3.3813 - val_accuracy: 0.2791\n",
      "Epoch 142/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5178 - accuracy: 0.8823 - val_loss: 3.4150 - val_accuracy: 0.2907\n",
      "Epoch 143/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5156 - accuracy: 0.8808 - val_loss: 3.4274 - val_accuracy: 0.2849\n",
      "Epoch 144/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.5113 - accuracy: 0.8750 - val_loss: 3.4392 - val_accuracy: 0.2849\n",
      "Epoch 145/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.5228 - accuracy: 0.8663 - val_loss: 3.4787 - val_accuracy: 0.2384\n",
      "Epoch 146/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.5241 - accuracy: 0.8721 - val_loss: 3.4176 - val_accuracy: 0.2791\n",
      "Epoch 147/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.5094 - accuracy: 0.8721 - val_loss: 3.4357 - val_accuracy: 0.2791\n",
      "Epoch 148/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5156 - accuracy: 0.8779 - val_loss: 3.5585 - val_accuracy: 0.2791\n",
      "Epoch 149/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.5075 - accuracy: 0.8924 - val_loss: 3.4111 - val_accuracy: 0.2849\n",
      "Epoch 150/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5034 - accuracy: 0.8837 - val_loss: 3.4750 - val_accuracy: 0.2791\n",
      "Epoch 151/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4953 - accuracy: 0.8866 - val_loss: 3.4744 - val_accuracy: 0.2849\n",
      "Epoch 152/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.5007 - accuracy: 0.8881 - val_loss: 3.5352 - val_accuracy: 0.2791\n",
      "Epoch 153/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4980 - accuracy: 0.8866 - val_loss: 3.4732 - val_accuracy: 0.2791\n",
      "Epoch 154/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4894 - accuracy: 0.8939 - val_loss: 3.4916 - val_accuracy: 0.2733\n",
      "Epoch 155/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4961 - accuracy: 0.8808 - val_loss: 3.4872 - val_accuracy: 0.2733\n",
      "Epoch 156/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4890 - accuracy: 0.8895 - val_loss: 3.5690 - val_accuracy: 0.2733\n",
      "Epoch 157/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4907 - accuracy: 0.8852 - val_loss: 3.4792 - val_accuracy: 0.2849\n",
      "Epoch 158/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4914 - accuracy: 0.8881 - val_loss: 3.5641 - val_accuracy: 0.2733\n",
      "Epoch 159/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4895 - accuracy: 0.8794 - val_loss: 3.4485 - val_accuracy: 0.2849\n",
      "Epoch 160/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.4766 - accuracy: 0.8983 - val_loss: 3.6003 - val_accuracy: 0.2616\n",
      "Epoch 161/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.4813 - accuracy: 0.8881 - val_loss: 3.5180 - val_accuracy: 0.2849\n",
      "Epoch 162/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.4737 - accuracy: 0.8910 - val_loss: 3.5358 - val_accuracy: 0.2791\n",
      "Epoch 163/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.4709 - accuracy: 0.8924 - val_loss: 3.5591 - val_accuracy: 0.2616\n",
      "Epoch 164/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.4772 - accuracy: 0.8924 - val_loss: 3.5399 - val_accuracy: 0.2849\n",
      "Epoch 165/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.4653 - accuracy: 0.9026 - val_loss: 3.5363 - val_accuracy: 0.2907\n",
      "Epoch 166/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4763 - accuracy: 0.8837 - val_loss: 3.6061 - val_accuracy: 0.2500\n",
      "Epoch 167/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4696 - accuracy: 0.8779 - val_loss: 3.5397 - val_accuracy: 0.2965\n",
      "Epoch 168/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.4743 - accuracy: 0.8881 - val_loss: 3.5530 - val_accuracy: 0.2674\n",
      "Epoch 169/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4641 - accuracy: 0.8881 - val_loss: 3.6207 - val_accuracy: 0.2733\n",
      "Epoch 170/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.4571 - accuracy: 0.8997 - val_loss: 3.5729 - val_accuracy: 0.2733\n",
      "Epoch 171/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4562 - accuracy: 0.8968 - val_loss: 3.6044 - val_accuracy: 0.2907\n",
      "Epoch 172/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.4672 - accuracy: 0.8895 - val_loss: 3.6367 - val_accuracy: 0.2791\n",
      "Epoch 173/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4558 - accuracy: 0.8953 - val_loss: 3.5635 - val_accuracy: 0.2907\n",
      "Epoch 174/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.4505 - accuracy: 0.8968 - val_loss: 3.5706 - val_accuracy: 0.2849\n",
      "Epoch 175/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4555 - accuracy: 0.8910 - val_loss: 3.6904 - val_accuracy: 0.2558\n",
      "Epoch 176/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4562 - accuracy: 0.8983 - val_loss: 3.6056 - val_accuracy: 0.2791\n",
      "Epoch 177/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4391 - accuracy: 0.8968 - val_loss: 3.6547 - val_accuracy: 0.2849\n",
      "Epoch 178/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.4368 - accuracy: 0.9026 - val_loss: 3.6291 - val_accuracy: 0.2733\n",
      "Epoch 179/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4394 - accuracy: 0.9084 - val_loss: 3.6512 - val_accuracy: 0.2965\n",
      "Epoch 180/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.4369 - accuracy: 0.9026 - val_loss: 3.6339 - val_accuracy: 0.2733\n",
      "Epoch 181/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4503 - accuracy: 0.9055 - val_loss: 3.6756 - val_accuracy: 0.2733\n",
      "Epoch 182/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4389 - accuracy: 0.8997 - val_loss: 3.6537 - val_accuracy: 0.2674\n",
      "Epoch 183/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4408 - accuracy: 0.8968 - val_loss: 3.7008 - val_accuracy: 0.2674\n",
      "Epoch 184/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4428 - accuracy: 0.8983 - val_loss: 3.7049 - val_accuracy: 0.2558\n",
      "Epoch 185/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.4366 - accuracy: 0.8895 - val_loss: 3.6865 - val_accuracy: 0.2791\n",
      "Epoch 186/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.4334 - accuracy: 0.8997 - val_loss: 3.7105 - val_accuracy: 0.2733\n",
      "Epoch 187/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.4265 - accuracy: 0.9070 - val_loss: 3.6912 - val_accuracy: 0.2733\n",
      "Epoch 188/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4246 - accuracy: 0.9128 - val_loss: 3.7193 - val_accuracy: 0.2849\n",
      "Epoch 189/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4186 - accuracy: 0.9157 - val_loss: 3.7149 - val_accuracy: 0.2733\n",
      "Epoch 190/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.4232 - accuracy: 0.9201 - val_loss: 3.7127 - val_accuracy: 0.2907\n",
      "Epoch 191/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4208 - accuracy: 0.9026 - val_loss: 3.7265 - val_accuracy: 0.2674\n",
      "Epoch 192/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4099 - accuracy: 0.9113 - val_loss: 3.7366 - val_accuracy: 0.2849\n",
      "Epoch 193/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.4056 - accuracy: 0.9201 - val_loss: 3.7622 - val_accuracy: 0.2791\n",
      "Epoch 194/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4098 - accuracy: 0.9157 - val_loss: 3.7429 - val_accuracy: 0.2733\n",
      "Epoch 195/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4098 - accuracy: 0.9186 - val_loss: 3.7652 - val_accuracy: 0.2791\n",
      "Epoch 196/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4169 - accuracy: 0.9012 - val_loss: 3.8266 - val_accuracy: 0.2500\n",
      "Epoch 197/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.4075 - accuracy: 0.9157 - val_loss: 3.7597 - val_accuracy: 0.2907\n",
      "Epoch 198/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3998 - accuracy: 0.9201 - val_loss: 3.7699 - val_accuracy: 0.2733\n",
      "Epoch 199/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.4028 - accuracy: 0.9244 - val_loss: 3.7966 - val_accuracy: 0.2791\n",
      "Epoch 200/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3958 - accuracy: 0.9070 - val_loss: 3.8372 - val_accuracy: 0.2442\n",
      "Epoch 201/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3995 - accuracy: 0.9142 - val_loss: 3.7818 - val_accuracy: 0.2849\n",
      "Epoch 202/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3957 - accuracy: 0.9172 - val_loss: 3.7871 - val_accuracy: 0.2616\n",
      "Epoch 203/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.4030 - accuracy: 0.9041 - val_loss: 3.8320 - val_accuracy: 0.2791\n",
      "Epoch 204/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3998 - accuracy: 0.9157 - val_loss: 3.7909 - val_accuracy: 0.2674\n",
      "Epoch 205/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3903 - accuracy: 0.9244 - val_loss: 3.8121 - val_accuracy: 0.2849\n",
      "Epoch 206/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4016 - accuracy: 0.9084 - val_loss: 3.7571 - val_accuracy: 0.2733\n",
      "Epoch 207/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3895 - accuracy: 0.9201 - val_loss: 3.8933 - val_accuracy: 0.2674\n",
      "Epoch 208/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.4006 - accuracy: 0.9026 - val_loss: 3.8008 - val_accuracy: 0.2733\n",
      "Epoch 209/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3914 - accuracy: 0.9288 - val_loss: 3.8653 - val_accuracy: 0.2791\n",
      "Epoch 210/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3806 - accuracy: 0.9215 - val_loss: 3.8329 - val_accuracy: 0.2733\n",
      "Epoch 211/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3818 - accuracy: 0.9186 - val_loss: 3.9309 - val_accuracy: 0.2674\n",
      "Epoch 212/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.4009 - accuracy: 0.9084 - val_loss: 3.9142 - val_accuracy: 0.2384\n",
      "Epoch 213/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3925 - accuracy: 0.9070 - val_loss: 3.9189 - val_accuracy: 0.2616\n",
      "Epoch 214/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3750 - accuracy: 0.9186 - val_loss: 3.8675 - val_accuracy: 0.2849\n",
      "Epoch 215/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3688 - accuracy: 0.9317 - val_loss: 3.8867 - val_accuracy: 0.2733\n",
      "Epoch 216/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3693 - accuracy: 0.9302 - val_loss: 3.9243 - val_accuracy: 0.2733\n",
      "Epoch 217/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3763 - accuracy: 0.9230 - val_loss: 3.9468 - val_accuracy: 0.2558\n",
      "Epoch 218/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3824 - accuracy: 0.9273 - val_loss: 3.9563 - val_accuracy: 0.2558\n",
      "Epoch 219/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3736 - accuracy: 0.9201 - val_loss: 3.8851 - val_accuracy: 0.2849\n",
      "Epoch 220/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3588 - accuracy: 0.9346 - val_loss: 3.9610 - val_accuracy: 0.2616\n",
      "Epoch 221/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3557 - accuracy: 0.9375 - val_loss: 3.8650 - val_accuracy: 0.2907\n",
      "Epoch 222/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.3596 - accuracy: 0.9259 - val_loss: 3.9351 - val_accuracy: 0.2674\n",
      "Epoch 223/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.3566 - accuracy: 0.9259 - val_loss: 3.9544 - val_accuracy: 0.2733\n",
      "Epoch 224/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3620 - accuracy: 0.9346 - val_loss: 3.9556 - val_accuracy: 0.2791\n",
      "Epoch 225/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3707 - accuracy: 0.9157 - val_loss: 3.9700 - val_accuracy: 0.2849\n",
      "Epoch 226/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.3548 - accuracy: 0.9317 - val_loss: 3.9445 - val_accuracy: 0.2558\n",
      "Epoch 227/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3533 - accuracy: 0.9331 - val_loss: 3.9844 - val_accuracy: 0.2791\n",
      "Epoch 228/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3535 - accuracy: 0.9302 - val_loss: 3.9405 - val_accuracy: 0.2791\n",
      "Epoch 229/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3475 - accuracy: 0.9259 - val_loss: 4.0341 - val_accuracy: 0.2733\n",
      "Epoch 230/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3677 - accuracy: 0.9273 - val_loss: 3.9768 - val_accuracy: 0.2616\n",
      "Epoch 231/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3453 - accuracy: 0.9317 - val_loss: 4.0338 - val_accuracy: 0.2733\n",
      "Epoch 232/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3514 - accuracy: 0.9273 - val_loss: 4.0018 - val_accuracy: 0.2616\n",
      "Epoch 233/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3506 - accuracy: 0.9302 - val_loss: 3.9943 - val_accuracy: 0.2616\n",
      "Epoch 234/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3469 - accuracy: 0.9273 - val_loss: 4.0530 - val_accuracy: 0.2674\n",
      "Epoch 235/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3441 - accuracy: 0.9346 - val_loss: 3.9866 - val_accuracy: 0.2907\n",
      "Epoch 236/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3338 - accuracy: 0.9404 - val_loss: 4.0380 - val_accuracy: 0.2849\n",
      "Epoch 237/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3319 - accuracy: 0.9375 - val_loss: 4.0368 - val_accuracy: 0.2849\n",
      "Epoch 238/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3333 - accuracy: 0.9375 - val_loss: 4.0441 - val_accuracy: 0.2791\n",
      "Epoch 239/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3271 - accuracy: 0.9375 - val_loss: 4.1128 - val_accuracy: 0.2500\n",
      "Epoch 240/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3284 - accuracy: 0.9346 - val_loss: 3.9999 - val_accuracy: 0.2907\n",
      "Epoch 241/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3286 - accuracy: 0.9535 - val_loss: 4.0896 - val_accuracy: 0.2558\n",
      "Epoch 242/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3270 - accuracy: 0.9390 - val_loss: 4.0470 - val_accuracy: 0.2733\n",
      "Epoch 243/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3245 - accuracy: 0.9404 - val_loss: 4.0562 - val_accuracy: 0.2791\n",
      "Epoch 244/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.3221 - accuracy: 0.9346 - val_loss: 4.1353 - val_accuracy: 0.2733\n",
      "Epoch 245/1000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.3251 - accuracy: 0.9346 - val_loss: 4.1051 - val_accuracy: 0.2500\n",
      "Epoch 246/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3181 - accuracy: 0.9448 - val_loss: 4.0797 - val_accuracy: 0.2849\n",
      "Epoch 247/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.3156 - accuracy: 0.9404 - val_loss: 4.0984 - val_accuracy: 0.2558\n",
      "Epoch 248/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3132 - accuracy: 0.9506 - val_loss: 4.0971 - val_accuracy: 0.2849\n",
      "Epoch 249/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3106 - accuracy: 0.9448 - val_loss: 4.1155 - val_accuracy: 0.2616\n",
      "Epoch 250/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3089 - accuracy: 0.9491 - val_loss: 4.1778 - val_accuracy: 0.2616\n",
      "Epoch 251/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3221 - accuracy: 0.9331 - val_loss: 4.1029 - val_accuracy: 0.2558\n",
      "Epoch 252/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3165 - accuracy: 0.9390 - val_loss: 4.1511 - val_accuracy: 0.2733\n",
      "Epoch 253/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3138 - accuracy: 0.9506 - val_loss: 4.1191 - val_accuracy: 0.2849\n",
      "Epoch 254/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3112 - accuracy: 0.9390 - val_loss: 4.2138 - val_accuracy: 0.2558\n",
      "Epoch 255/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.3072 - accuracy: 0.9390 - val_loss: 4.1581 - val_accuracy: 0.2558\n",
      "Epoch 256/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3047 - accuracy: 0.9433 - val_loss: 4.1922 - val_accuracy: 0.2849\n",
      "Epoch 257/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2977 - accuracy: 0.9462 - val_loss: 4.1845 - val_accuracy: 0.2674\n",
      "Epoch 258/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.3005 - accuracy: 0.9448 - val_loss: 4.1619 - val_accuracy: 0.2791\n",
      "Epoch 259/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2965 - accuracy: 0.9462 - val_loss: 4.1900 - val_accuracy: 0.2791\n",
      "Epoch 260/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2975 - accuracy: 0.9448 - val_loss: 4.1950 - val_accuracy: 0.2791\n",
      "Epoch 261/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2906 - accuracy: 0.9462 - val_loss: 4.2169 - val_accuracy: 0.2674\n",
      "Epoch 262/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.2895 - accuracy: 0.9477 - val_loss: 4.2269 - val_accuracy: 0.2500\n",
      "Epoch 263/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.2888 - accuracy: 0.9549 - val_loss: 4.2009 - val_accuracy: 0.2907\n",
      "Epoch 264/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2876 - accuracy: 0.9448 - val_loss: 4.2827 - val_accuracy: 0.2616\n",
      "Epoch 265/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2895 - accuracy: 0.9477 - val_loss: 4.2152 - val_accuracy: 0.2849\n",
      "Epoch 266/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2889 - accuracy: 0.9520 - val_loss: 4.2910 - val_accuracy: 0.2616\n",
      "Epoch 267/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2859 - accuracy: 0.9462 - val_loss: 4.2732 - val_accuracy: 0.2791\n",
      "Epoch 268/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2916 - accuracy: 0.9433 - val_loss: 4.2913 - val_accuracy: 0.2616\n",
      "Epoch 269/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2893 - accuracy: 0.9433 - val_loss: 4.2492 - val_accuracy: 0.2791\n",
      "Epoch 270/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2890 - accuracy: 0.9506 - val_loss: 4.3124 - val_accuracy: 0.2442\n",
      "Epoch 271/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2846 - accuracy: 0.9578 - val_loss: 4.2604 - val_accuracy: 0.2849\n",
      "Epoch 272/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2825 - accuracy: 0.9419 - val_loss: 4.3243 - val_accuracy: 0.2558\n",
      "Epoch 273/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2814 - accuracy: 0.9549 - val_loss: 4.3285 - val_accuracy: 0.2674\n",
      "Epoch 274/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2863 - accuracy: 0.9433 - val_loss: 4.3404 - val_accuracy: 0.2616\n",
      "Epoch 275/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2811 - accuracy: 0.9549 - val_loss: 4.3189 - val_accuracy: 0.2674\n",
      "Epoch 276/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2762 - accuracy: 0.9506 - val_loss: 4.2698 - val_accuracy: 0.2849\n",
      "Epoch 277/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2753 - accuracy: 0.9491 - val_loss: 4.3551 - val_accuracy: 0.2500\n",
      "Epoch 278/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2750 - accuracy: 0.9477 - val_loss: 4.3446 - val_accuracy: 0.2616\n",
      "Epoch 279/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2707 - accuracy: 0.9535 - val_loss: 4.3269 - val_accuracy: 0.2791\n",
      "Epoch 280/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2685 - accuracy: 0.9564 - val_loss: 4.4039 - val_accuracy: 0.2384\n",
      "Epoch 281/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2670 - accuracy: 0.9520 - val_loss: 4.3217 - val_accuracy: 0.2733\n",
      "Epoch 282/1000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.2628 - accuracy: 0.9535 - val_loss: 4.3953 - val_accuracy: 0.2558\n",
      "Epoch 283/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2637 - accuracy: 0.9491 - val_loss: 4.3772 - val_accuracy: 0.2500\n",
      "Epoch 284/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2680 - accuracy: 0.9506 - val_loss: 4.3827 - val_accuracy: 0.2616\n",
      "Epoch 285/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2652 - accuracy: 0.9549 - val_loss: 4.4123 - val_accuracy: 0.2616\n",
      "Epoch 286/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2671 - accuracy: 0.9564 - val_loss: 4.4847 - val_accuracy: 0.2151\n",
      "Epoch 287/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2712 - accuracy: 0.9491 - val_loss: 4.4135 - val_accuracy: 0.2442\n",
      "Epoch 288/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2591 - accuracy: 0.9564 - val_loss: 4.3939 - val_accuracy: 0.2907\n",
      "Epoch 289/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2609 - accuracy: 0.9549 - val_loss: 4.4683 - val_accuracy: 0.2500\n",
      "Epoch 290/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2585 - accuracy: 0.9549 - val_loss: 4.4662 - val_accuracy: 0.2616\n",
      "Epoch 291/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2578 - accuracy: 0.9535 - val_loss: 4.4327 - val_accuracy: 0.2384\n",
      "Epoch 292/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2541 - accuracy: 0.9593 - val_loss: 4.4882 - val_accuracy: 0.2442\n",
      "Epoch 293/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2526 - accuracy: 0.9491 - val_loss: 4.4373 - val_accuracy: 0.2267\n",
      "Epoch 294/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2526 - accuracy: 0.9593 - val_loss: 4.5057 - val_accuracy: 0.2500\n",
      "Epoch 295/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2727 - accuracy: 0.9375 - val_loss: 4.5042 - val_accuracy: 0.2209\n",
      "Epoch 296/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2628 - accuracy: 0.9462 - val_loss: 4.4343 - val_accuracy: 0.2791\n",
      "Epoch 297/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2494 - accuracy: 0.9578 - val_loss: 4.4907 - val_accuracy: 0.2616\n",
      "Epoch 298/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2505 - accuracy: 0.9593 - val_loss: 4.4898 - val_accuracy: 0.2616\n",
      "Epoch 299/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2479 - accuracy: 0.9593 - val_loss: 4.4761 - val_accuracy: 0.2791\n",
      "Epoch 300/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.2512 - accuracy: 0.9578 - val_loss: 4.5345 - val_accuracy: 0.2558\n",
      "Epoch 301/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2465 - accuracy: 0.9520 - val_loss: 4.5619 - val_accuracy: 0.2442\n",
      "Epoch 302/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2450 - accuracy: 0.9622 - val_loss: 4.5508 - val_accuracy: 0.2616\n",
      "Epoch 303/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2501 - accuracy: 0.9520 - val_loss: 4.5952 - val_accuracy: 0.2500\n",
      "Epoch 304/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2497 - accuracy: 0.9637 - val_loss: 4.5345 - val_accuracy: 0.2733\n",
      "Epoch 305/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2418 - accuracy: 0.9564 - val_loss: 4.6000 - val_accuracy: 0.2384\n",
      "Epoch 306/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2394 - accuracy: 0.9564 - val_loss: 4.4933 - val_accuracy: 0.2791\n",
      "Epoch 307/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2345 - accuracy: 0.9578 - val_loss: 4.6465 - val_accuracy: 0.2326\n",
      "Epoch 308/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2362 - accuracy: 0.9535 - val_loss: 4.5283 - val_accuracy: 0.2500\n",
      "Epoch 309/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2363 - accuracy: 0.9593 - val_loss: 4.5957 - val_accuracy: 0.2500\n",
      "Epoch 310/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.2346 - accuracy: 0.9564 - val_loss: 4.6474 - val_accuracy: 0.2442\n",
      "Epoch 311/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2435 - accuracy: 0.9549 - val_loss: 4.6119 - val_accuracy: 0.2674\n",
      "Epoch 312/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2338 - accuracy: 0.9651 - val_loss: 4.5671 - val_accuracy: 0.2616\n",
      "Epoch 313/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.2264 - accuracy: 0.9622 - val_loss: 4.6080 - val_accuracy: 0.2616\n",
      "Epoch 314/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.2288 - accuracy: 0.9637 - val_loss: 4.6343 - val_accuracy: 0.2558\n",
      "Epoch 315/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.2341 - accuracy: 0.9608 - val_loss: 4.6590 - val_accuracy: 0.2326\n",
      "Epoch 316/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2270 - accuracy: 0.9637 - val_loss: 4.6224 - val_accuracy: 0.2500\n",
      "Epoch 317/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2331 - accuracy: 0.9622 - val_loss: 4.6363 - val_accuracy: 0.2733\n",
      "Epoch 318/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2330 - accuracy: 0.9506 - val_loss: 4.7460 - val_accuracy: 0.2209\n",
      "Epoch 319/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2370 - accuracy: 0.9593 - val_loss: 4.7180 - val_accuracy: 0.2384\n",
      "Epoch 320/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2427 - accuracy: 0.9433 - val_loss: 4.6985 - val_accuracy: 0.2326\n",
      "Epoch 321/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2372 - accuracy: 0.9578 - val_loss: 4.6915 - val_accuracy: 0.2733\n",
      "Epoch 322/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2262 - accuracy: 0.9578 - val_loss: 4.6714 - val_accuracy: 0.2616\n",
      "Epoch 323/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2225 - accuracy: 0.9666 - val_loss: 4.7502 - val_accuracy: 0.2500\n",
      "Epoch 324/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2208 - accuracy: 0.9564 - val_loss: 4.7162 - val_accuracy: 0.2442\n",
      "Epoch 325/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2149 - accuracy: 0.9637 - val_loss: 4.6811 - val_accuracy: 0.2500\n",
      "Epoch 326/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2151 - accuracy: 0.9709 - val_loss: 4.7538 - val_accuracy: 0.2558\n",
      "Epoch 327/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2180 - accuracy: 0.9608 - val_loss: 4.6741 - val_accuracy: 0.2442\n",
      "Epoch 328/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2163 - accuracy: 0.9622 - val_loss: 4.8074 - val_accuracy: 0.2384\n",
      "Epoch 329/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2096 - accuracy: 0.9666 - val_loss: 4.7172 - val_accuracy: 0.2558\n",
      "Epoch 330/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2121 - accuracy: 0.9709 - val_loss: 4.7924 - val_accuracy: 0.2326\n",
      "Epoch 331/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2133 - accuracy: 0.9608 - val_loss: 4.7068 - val_accuracy: 0.2442\n",
      "Epoch 332/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.2092 - accuracy: 0.9666 - val_loss: 4.7386 - val_accuracy: 0.2733\n",
      "Epoch 333/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.2062 - accuracy: 0.9695 - val_loss: 4.8067 - val_accuracy: 0.2384\n",
      "Epoch 334/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2034 - accuracy: 0.9695 - val_loss: 4.8065 - val_accuracy: 0.2326\n",
      "Epoch 335/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.2066 - accuracy: 0.9666 - val_loss: 4.7650 - val_accuracy: 0.2326\n",
      "Epoch 336/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2040 - accuracy: 0.9797 - val_loss: 4.8401 - val_accuracy: 0.2500\n",
      "Epoch 337/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2101 - accuracy: 0.9622 - val_loss: 4.7693 - val_accuracy: 0.2442\n",
      "Epoch 338/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2099 - accuracy: 0.9637 - val_loss: 4.8399 - val_accuracy: 0.2500\n",
      "Epoch 339/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1991 - accuracy: 0.9753 - val_loss: 4.8214 - val_accuracy: 0.2733\n",
      "Epoch 340/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2119 - accuracy: 0.9651 - val_loss: 4.8307 - val_accuracy: 0.2384\n",
      "Epoch 341/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1997 - accuracy: 0.9738 - val_loss: 4.8354 - val_accuracy: 0.2500\n",
      "Epoch 342/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2074 - accuracy: 0.9637 - val_loss: 4.8256 - val_accuracy: 0.2384\n",
      "Epoch 343/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2055 - accuracy: 0.9666 - val_loss: 4.9464 - val_accuracy: 0.2384\n",
      "Epoch 344/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.2002 - accuracy: 0.9680 - val_loss: 4.8002 - val_accuracy: 0.2442\n",
      "Epoch 345/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1976 - accuracy: 0.9680 - val_loss: 4.9164 - val_accuracy: 0.2500\n",
      "Epoch 346/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1938 - accuracy: 0.9695 - val_loss: 4.8448 - val_accuracy: 0.2326\n",
      "Epoch 347/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.1922 - accuracy: 0.9767 - val_loss: 4.9365 - val_accuracy: 0.2558\n",
      "Epoch 348/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.1936 - accuracy: 0.9680 - val_loss: 4.8774 - val_accuracy: 0.2384\n",
      "Epoch 349/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.2033 - accuracy: 0.9622 - val_loss: 4.9569 - val_accuracy: 0.2384\n",
      "Epoch 350/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.2042 - accuracy: 0.9651 - val_loss: 4.9461 - val_accuracy: 0.2558\n",
      "Epoch 351/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1993 - accuracy: 0.9622 - val_loss: 4.9154 - val_accuracy: 0.2384\n",
      "Epoch 352/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1964 - accuracy: 0.9637 - val_loss: 5.0050 - val_accuracy: 0.2442\n",
      "Epoch 353/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.2007 - accuracy: 0.9637 - val_loss: 4.9508 - val_accuracy: 0.2326\n",
      "Epoch 354/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1868 - accuracy: 0.9767 - val_loss: 4.9083 - val_accuracy: 0.2558\n",
      "Epoch 355/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1886 - accuracy: 0.9738 - val_loss: 4.9599 - val_accuracy: 0.2616\n",
      "Epoch 356/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1907 - accuracy: 0.9724 - val_loss: 4.9967 - val_accuracy: 0.2442\n",
      "Epoch 357/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1874 - accuracy: 0.9680 - val_loss: 4.9843 - val_accuracy: 0.2500\n",
      "Epoch 358/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1914 - accuracy: 0.9709 - val_loss: 4.9463 - val_accuracy: 0.2500\n",
      "Epoch 359/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1821 - accuracy: 0.9695 - val_loss: 4.9913 - val_accuracy: 0.2326\n",
      "Epoch 360/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1900 - accuracy: 0.9709 - val_loss: 5.0011 - val_accuracy: 0.2558\n",
      "Epoch 361/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1844 - accuracy: 0.9695 - val_loss: 4.9936 - val_accuracy: 0.2442\n",
      "Epoch 362/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1810 - accuracy: 0.9709 - val_loss: 4.9617 - val_accuracy: 0.2500\n",
      "Epoch 363/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1772 - accuracy: 0.9782 - val_loss: 5.0370 - val_accuracy: 0.2384\n",
      "Epoch 364/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.1806 - accuracy: 0.9738 - val_loss: 5.1155 - val_accuracy: 0.2384\n",
      "Epoch 365/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.1813 - accuracy: 0.9724 - val_loss: 4.9974 - val_accuracy: 0.2326\n",
      "Epoch 366/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1796 - accuracy: 0.9695 - val_loss: 5.0632 - val_accuracy: 0.2442\n",
      "Epoch 367/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1756 - accuracy: 0.9782 - val_loss: 5.0809 - val_accuracy: 0.2384\n",
      "Epoch 368/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1761 - accuracy: 0.9767 - val_loss: 5.0042 - val_accuracy: 0.2500\n",
      "Epoch 369/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1704 - accuracy: 0.9782 - val_loss: 5.0676 - val_accuracy: 0.2558\n",
      "Epoch 370/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1688 - accuracy: 0.9797 - val_loss: 5.0838 - val_accuracy: 0.2442\n",
      "Epoch 371/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1693 - accuracy: 0.9840 - val_loss: 5.0718 - val_accuracy: 0.2500\n",
      "Epoch 372/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1686 - accuracy: 0.9782 - val_loss: 5.0710 - val_accuracy: 0.2558\n",
      "Epoch 373/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1741 - accuracy: 0.9797 - val_loss: 5.1616 - val_accuracy: 0.2267\n",
      "Epoch 374/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1741 - accuracy: 0.9738 - val_loss: 5.1922 - val_accuracy: 0.2442\n",
      "Epoch 375/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1719 - accuracy: 0.9782 - val_loss: 5.0629 - val_accuracy: 0.2674\n",
      "Epoch 376/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1671 - accuracy: 0.9767 - val_loss: 5.1016 - val_accuracy: 0.2442\n",
      "Epoch 377/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1671 - accuracy: 0.9797 - val_loss: 5.1799 - val_accuracy: 0.2442\n",
      "Epoch 378/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1684 - accuracy: 0.9782 - val_loss: 5.0763 - val_accuracy: 0.2384\n",
      "Epoch 379/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.1709 - accuracy: 0.9811 - val_loss: 5.2045 - val_accuracy: 0.2442\n",
      "Epoch 380/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.1666 - accuracy: 0.9767 - val_loss: 5.1588 - val_accuracy: 0.2384\n",
      "Epoch 381/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1650 - accuracy: 0.9680 - val_loss: 5.1185 - val_accuracy: 0.2558\n",
      "Epoch 382/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1682 - accuracy: 0.9782 - val_loss: 5.1955 - val_accuracy: 0.2442\n",
      "Epoch 383/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1610 - accuracy: 0.9811 - val_loss: 5.1213 - val_accuracy: 0.2442\n",
      "Epoch 384/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1609 - accuracy: 0.9797 - val_loss: 5.2548 - val_accuracy: 0.2442\n",
      "Epoch 385/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1613 - accuracy: 0.9811 - val_loss: 5.1441 - val_accuracy: 0.2442\n",
      "Epoch 386/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1617 - accuracy: 0.9753 - val_loss: 5.2025 - val_accuracy: 0.2442\n",
      "Epoch 387/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1630 - accuracy: 0.9826 - val_loss: 5.2730 - val_accuracy: 0.2384\n",
      "Epoch 388/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1576 - accuracy: 0.9782 - val_loss: 5.1716 - val_accuracy: 0.2442\n",
      "Epoch 389/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1596 - accuracy: 0.9840 - val_loss: 5.2388 - val_accuracy: 0.2326\n",
      "Epoch 390/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1602 - accuracy: 0.9811 - val_loss: 5.1800 - val_accuracy: 0.2326\n",
      "Epoch 391/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1599 - accuracy: 0.9767 - val_loss: 5.2048 - val_accuracy: 0.2442\n",
      "Epoch 392/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1616 - accuracy: 0.9797 - val_loss: 5.2778 - val_accuracy: 0.2500\n",
      "Epoch 393/1000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.1607 - accuracy: 0.9797 - val_loss: 5.2932 - val_accuracy: 0.2267\n",
      "Epoch 394/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.1564 - accuracy: 0.9797 - val_loss: 5.2522 - val_accuracy: 0.2326\n",
      "Epoch 395/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.1520 - accuracy: 0.9811 - val_loss: 5.2715 - val_accuracy: 0.2442\n",
      "Epoch 396/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1512 - accuracy: 0.9869 - val_loss: 5.2464 - val_accuracy: 0.2500\n",
      "Epoch 397/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1492 - accuracy: 0.9826 - val_loss: 5.3190 - val_accuracy: 0.2384\n",
      "Epoch 398/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1488 - accuracy: 0.9811 - val_loss: 5.3000 - val_accuracy: 0.2500\n",
      "Epoch 399/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1550 - accuracy: 0.9767 - val_loss: 5.3452 - val_accuracy: 0.2384\n",
      "Epoch 400/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1602 - accuracy: 0.9826 - val_loss: 5.3246 - val_accuracy: 0.2384\n",
      "Epoch 401/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1546 - accuracy: 0.9811 - val_loss: 5.3332 - val_accuracy: 0.2616\n",
      "Epoch 402/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1565 - accuracy: 0.9782 - val_loss: 5.4029 - val_accuracy: 0.2209\n",
      "Epoch 403/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1512 - accuracy: 0.9855 - val_loss: 5.2727 - val_accuracy: 0.2500\n",
      "Epoch 404/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1422 - accuracy: 0.9855 - val_loss: 5.3350 - val_accuracy: 0.2558\n",
      "Epoch 405/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1414 - accuracy: 0.9840 - val_loss: 5.2905 - val_accuracy: 0.2500\n",
      "Epoch 406/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1567 - accuracy: 0.9782 - val_loss: 5.4082 - val_accuracy: 0.2326\n",
      "Epoch 407/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.1517 - accuracy: 0.9797 - val_loss: 5.3113 - val_accuracy: 0.2558\n",
      "Epoch 408/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.1473 - accuracy: 0.9797 - val_loss: 5.3818 - val_accuracy: 0.2384\n",
      "Epoch 409/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1459 - accuracy: 0.9855 - val_loss: 5.3093 - val_accuracy: 0.2616\n",
      "Epoch 410/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1496 - accuracy: 0.9855 - val_loss: 5.3412 - val_accuracy: 0.2384\n",
      "Epoch 411/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1521 - accuracy: 0.9767 - val_loss: 5.3424 - val_accuracy: 0.2500\n",
      "Epoch 412/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1401 - accuracy: 0.9855 - val_loss: 5.4683 - val_accuracy: 0.2500\n",
      "Epoch 413/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1358 - accuracy: 0.9884 - val_loss: 5.3432 - val_accuracy: 0.2267\n",
      "Epoch 414/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1404 - accuracy: 0.9811 - val_loss: 5.4067 - val_accuracy: 0.2500\n",
      "Epoch 415/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1461 - accuracy: 0.9782 - val_loss: 5.4667 - val_accuracy: 0.2384\n",
      "Epoch 416/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1404 - accuracy: 0.9826 - val_loss: 5.4162 - val_accuracy: 0.2500\n",
      "Epoch 417/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1345 - accuracy: 0.9855 - val_loss: 5.4193 - val_accuracy: 0.2384\n",
      "Epoch 418/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1363 - accuracy: 0.9840 - val_loss: 5.4371 - val_accuracy: 0.2442\n",
      "Epoch 419/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.1336 - accuracy: 0.9826 - val_loss: 5.4415 - val_accuracy: 0.2558\n",
      "Epoch 420/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.1316 - accuracy: 0.9884 - val_loss: 5.4308 - val_accuracy: 0.2500\n",
      "Epoch 421/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1376 - accuracy: 0.9855 - val_loss: 5.4663 - val_accuracy: 0.2326\n",
      "Epoch 422/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1443 - accuracy: 0.9797 - val_loss: 5.4417 - val_accuracy: 0.2384\n",
      "Epoch 423/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1453 - accuracy: 0.9797 - val_loss: 5.5902 - val_accuracy: 0.2384\n",
      "Epoch 424/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1327 - accuracy: 0.9840 - val_loss: 5.4186 - val_accuracy: 0.2558\n",
      "Epoch 425/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1285 - accuracy: 0.9840 - val_loss: 5.5298 - val_accuracy: 0.2384\n",
      "Epoch 426/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1305 - accuracy: 0.9840 - val_loss: 5.4901 - val_accuracy: 0.2326\n",
      "Epoch 427/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1289 - accuracy: 0.9884 - val_loss: 5.5020 - val_accuracy: 0.2442\n",
      "Epoch 428/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1261 - accuracy: 0.9826 - val_loss: 5.4885 - val_accuracy: 0.2500\n",
      "Epoch 429/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1270 - accuracy: 0.9869 - val_loss: 5.4781 - val_accuracy: 0.2500\n",
      "Epoch 430/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1278 - accuracy: 0.9898 - val_loss: 5.4627 - val_accuracy: 0.2384\n",
      "Epoch 431/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1290 - accuracy: 0.9826 - val_loss: 5.5705 - val_accuracy: 0.2326\n",
      "Epoch 432/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.1455 - accuracy: 0.9753 - val_loss: 5.5645 - val_accuracy: 0.2267\n",
      "Epoch 433/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1373 - accuracy: 0.9826 - val_loss: 5.5240 - val_accuracy: 0.2442\n",
      "Epoch 434/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1310 - accuracy: 0.9855 - val_loss: 5.5207 - val_accuracy: 0.2558\n",
      "Epoch 435/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1319 - accuracy: 0.9840 - val_loss: 5.5348 - val_accuracy: 0.2500\n",
      "Epoch 436/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1247 - accuracy: 0.9855 - val_loss: 5.5757 - val_accuracy: 0.2326\n",
      "Epoch 437/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1269 - accuracy: 0.9826 - val_loss: 5.5413 - val_accuracy: 0.2558\n",
      "Epoch 438/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1243 - accuracy: 0.9869 - val_loss: 5.6559 - val_accuracy: 0.2500\n",
      "Epoch 439/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1219 - accuracy: 0.9884 - val_loss: 5.6046 - val_accuracy: 0.2384\n",
      "Epoch 440/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.1201 - accuracy: 0.9898 - val_loss: 5.5413 - val_accuracy: 0.2500\n",
      "Epoch 441/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1202 - accuracy: 0.9884 - val_loss: 5.6290 - val_accuracy: 0.2500\n",
      "Epoch 442/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1204 - accuracy: 0.9884 - val_loss: 5.6396 - val_accuracy: 0.2384\n",
      "Epoch 443/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1191 - accuracy: 0.9869 - val_loss: 5.6242 - val_accuracy: 0.2500\n",
      "Epoch 444/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1224 - accuracy: 0.9884 - val_loss: 5.6640 - val_accuracy: 0.2500\n",
      "Epoch 445/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1176 - accuracy: 0.9855 - val_loss: 5.6444 - val_accuracy: 0.2384\n",
      "Epoch 446/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.1186 - accuracy: 0.9869 - val_loss: 5.6557 - val_accuracy: 0.2500\n",
      "Epoch 447/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.1158 - accuracy: 0.9913 - val_loss: 5.5946 - val_accuracy: 0.2500\n",
      "Epoch 448/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.1170 - accuracy: 0.9913 - val_loss: 5.7065 - val_accuracy: 0.2326\n",
      "Epoch 449/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.1167 - accuracy: 0.9927 - val_loss: 5.6759 - val_accuracy: 0.2209\n",
      "Epoch 450/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.1169 - accuracy: 0.9869 - val_loss: 5.6644 - val_accuracy: 0.2674\n",
      "Epoch 451/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1163 - accuracy: 0.9869 - val_loss: 5.7643 - val_accuracy: 0.2209\n",
      "Epoch 452/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1158 - accuracy: 0.9869 - val_loss: 5.7120 - val_accuracy: 0.2558\n",
      "Epoch 453/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.1132 - accuracy: 0.9884 - val_loss: 5.7178 - val_accuracy: 0.2209\n",
      "Epoch 454/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1155 - accuracy: 0.9869 - val_loss: 5.7617 - val_accuracy: 0.2558\n",
      "Epoch 455/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1173 - accuracy: 0.9855 - val_loss: 5.6618 - val_accuracy: 0.2500\n",
      "Epoch 456/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1132 - accuracy: 0.9898 - val_loss: 5.7325 - val_accuracy: 0.2442\n",
      "Epoch 457/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1162 - accuracy: 0.9884 - val_loss: 5.6759 - val_accuracy: 0.2558\n",
      "Epoch 458/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1131 - accuracy: 0.9913 - val_loss: 5.7770 - val_accuracy: 0.2384\n",
      "Epoch 459/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1153 - accuracy: 0.9884 - val_loss: 5.7182 - val_accuracy: 0.2326\n",
      "Epoch 460/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1100 - accuracy: 0.9927 - val_loss: 5.7671 - val_accuracy: 0.2500\n",
      "Epoch 461/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.1130 - accuracy: 0.9855 - val_loss: 5.7440 - val_accuracy: 0.2384\n",
      "Epoch 462/1000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.1125 - accuracy: 0.9884 - val_loss: 5.7951 - val_accuracy: 0.2558\n",
      "Epoch 463/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1062 - accuracy: 0.9898 - val_loss: 5.7542 - val_accuracy: 0.2442\n",
      "Epoch 464/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1070 - accuracy: 0.9913 - val_loss: 5.7704 - val_accuracy: 0.2384\n",
      "Epoch 465/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1059 - accuracy: 0.9898 - val_loss: 5.8138 - val_accuracy: 0.2442\n",
      "Epoch 466/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1048 - accuracy: 0.9898 - val_loss: 5.7951 - val_accuracy: 0.2442\n",
      "Epoch 467/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1028 - accuracy: 0.9898 - val_loss: 5.7880 - val_accuracy: 0.2442\n",
      "Epoch 468/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1046 - accuracy: 0.9884 - val_loss: 5.8429 - val_accuracy: 0.2326\n",
      "Epoch 469/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1029 - accuracy: 0.9913 - val_loss: 5.8556 - val_accuracy: 0.2326\n",
      "Epoch 470/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1045 - accuracy: 0.9884 - val_loss: 5.8456 - val_accuracy: 0.2558\n",
      "Epoch 471/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1097 - accuracy: 0.9884 - val_loss: 5.8935 - val_accuracy: 0.2267\n",
      "Epoch 472/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1048 - accuracy: 0.9942 - val_loss: 5.9320 - val_accuracy: 0.2384\n",
      "Epoch 473/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1190 - accuracy: 0.9767 - val_loss: 5.8300 - val_accuracy: 0.2442\n",
      "Epoch 474/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1035 - accuracy: 0.9869 - val_loss: 5.8671 - val_accuracy: 0.2500\n",
      "Epoch 475/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.1132 - accuracy: 0.9840 - val_loss: 5.9012 - val_accuracy: 0.2442\n",
      "Epoch 476/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.1139 - accuracy: 0.9855 - val_loss: 5.8688 - val_accuracy: 0.2326\n",
      "Epoch 477/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.1067 - accuracy: 0.9884 - val_loss: 5.8828 - val_accuracy: 0.2442\n",
      "Epoch 478/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1032 - accuracy: 0.9884 - val_loss: 5.9564 - val_accuracy: 0.2326\n",
      "Epoch 479/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1031 - accuracy: 0.9927 - val_loss: 5.9167 - val_accuracy: 0.2384\n",
      "Epoch 480/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0999 - accuracy: 0.9927 - val_loss: 5.9208 - val_accuracy: 0.2384\n",
      "Epoch 481/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0974 - accuracy: 0.9927 - val_loss: 5.9294 - val_accuracy: 0.2384\n",
      "Epoch 482/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0969 - accuracy: 0.9913 - val_loss: 5.9481 - val_accuracy: 0.2500\n",
      "Epoch 483/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0972 - accuracy: 0.9942 - val_loss: 5.9593 - val_accuracy: 0.2267\n",
      "Epoch 484/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1037 - accuracy: 0.9898 - val_loss: 5.9471 - val_accuracy: 0.2558\n",
      "Epoch 485/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0985 - accuracy: 0.9884 - val_loss: 6.0709 - val_accuracy: 0.2326\n",
      "Epoch 486/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0945 - accuracy: 0.9884 - val_loss: 5.9028 - val_accuracy: 0.2558\n",
      "Epoch 487/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0945 - accuracy: 0.9898 - val_loss: 5.9929 - val_accuracy: 0.2500\n",
      "Epoch 488/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0948 - accuracy: 0.9913 - val_loss: 5.9776 - val_accuracy: 0.2442\n",
      "Epoch 489/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0981 - accuracy: 0.9869 - val_loss: 5.9609 - val_accuracy: 0.2558\n",
      "Epoch 490/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0951 - accuracy: 0.9913 - val_loss: 6.0603 - val_accuracy: 0.2151\n",
      "Epoch 491/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1001 - accuracy: 0.9942 - val_loss: 5.9486 - val_accuracy: 0.2500\n",
      "Epoch 492/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0956 - accuracy: 0.9927 - val_loss: 6.0121 - val_accuracy: 0.2384\n",
      "Epoch 493/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0922 - accuracy: 0.9913 - val_loss: 5.9737 - val_accuracy: 0.2384\n",
      "Epoch 494/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0931 - accuracy: 0.9884 - val_loss: 6.0424 - val_accuracy: 0.2442\n",
      "Epoch 495/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0917 - accuracy: 0.9927 - val_loss: 6.0622 - val_accuracy: 0.2442\n",
      "Epoch 496/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0906 - accuracy: 0.9913 - val_loss: 5.9718 - val_accuracy: 0.2384\n",
      "Epoch 497/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0895 - accuracy: 0.9942 - val_loss: 6.1122 - val_accuracy: 0.2326\n",
      "Epoch 498/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0945 - accuracy: 0.9913 - val_loss: 6.0007 - val_accuracy: 0.2384\n",
      "Epoch 499/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0935 - accuracy: 0.9898 - val_loss: 6.1659 - val_accuracy: 0.2326\n",
      "Epoch 500/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0946 - accuracy: 0.9913 - val_loss: 6.0595 - val_accuracy: 0.2558\n",
      "Epoch 501/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0977 - accuracy: 0.9898 - val_loss: 6.1092 - val_accuracy: 0.2558\n",
      "Epoch 502/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0983 - accuracy: 0.9913 - val_loss: 6.1060 - val_accuracy: 0.2442\n",
      "Epoch 503/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0991 - accuracy: 0.9884 - val_loss: 6.0491 - val_accuracy: 0.2326\n",
      "Epoch 504/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0971 - accuracy: 0.9898 - val_loss: 6.1679 - val_accuracy: 0.2442\n",
      "Epoch 505/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0904 - accuracy: 0.9884 - val_loss: 6.1188 - val_accuracy: 0.2209\n",
      "Epoch 506/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0965 - accuracy: 0.9855 - val_loss: 6.0614 - val_accuracy: 0.2500\n",
      "Epoch 507/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0915 - accuracy: 0.9913 - val_loss: 6.1600 - val_accuracy: 0.2326\n",
      "Epoch 508/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0909 - accuracy: 0.9927 - val_loss: 6.1185 - val_accuracy: 0.2500\n",
      "Epoch 509/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0897 - accuracy: 0.9898 - val_loss: 6.2302 - val_accuracy: 0.2326\n",
      "Epoch 510/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.0875 - accuracy: 0.9898 - val_loss: 6.1166 - val_accuracy: 0.2384\n",
      "Epoch 511/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.0897 - accuracy: 0.9913 - val_loss: 6.2261 - val_accuracy: 0.2384\n",
      "Epoch 512/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0895 - accuracy: 0.9898 - val_loss: 6.1534 - val_accuracy: 0.2442\n",
      "Epoch 513/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0891 - accuracy: 0.9956 - val_loss: 6.1582 - val_accuracy: 0.2442\n",
      "Epoch 514/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0855 - accuracy: 0.9927 - val_loss: 6.2088 - val_accuracy: 0.2442\n",
      "Epoch 515/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0841 - accuracy: 0.9942 - val_loss: 6.1247 - val_accuracy: 0.2326\n",
      "Epoch 516/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0920 - accuracy: 0.9898 - val_loss: 6.2534 - val_accuracy: 0.2384\n",
      "Epoch 517/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0867 - accuracy: 0.9898 - val_loss: 6.2907 - val_accuracy: 0.2093\n",
      "Epoch 518/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0892 - accuracy: 0.9898 - val_loss: 6.1377 - val_accuracy: 0.2384\n",
      "Epoch 519/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0872 - accuracy: 0.9913 - val_loss: 6.2179 - val_accuracy: 0.2209\n",
      "Epoch 520/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0905 - accuracy: 0.9913 - val_loss: 6.2943 - val_accuracy: 0.2209\n",
      "Epoch 521/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0867 - accuracy: 0.9913 - val_loss: 6.3095 - val_accuracy: 0.2209\n",
      "Epoch 522/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0856 - accuracy: 0.9942 - val_loss: 6.1714 - val_accuracy: 0.2558\n",
      "Epoch 523/1000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0836 - accuracy: 0.9927 - val_loss: 6.3117 - val_accuracy: 0.2384\n",
      "Epoch 524/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0798 - accuracy: 0.9927 - val_loss: 6.1714 - val_accuracy: 0.2616\n",
      "Epoch 525/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0814 - accuracy: 0.9956 - val_loss: 6.2361 - val_accuracy: 0.2267\n",
      "Epoch 526/1000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0851 - accuracy: 0.9913 - val_loss: 6.2518 - val_accuracy: 0.2442\n",
      "Epoch 527/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0797 - accuracy: 0.9898 - val_loss: 6.3370 - val_accuracy: 0.2326\n",
      "Epoch 528/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0808 - accuracy: 0.9956 - val_loss: 6.2212 - val_accuracy: 0.2558\n",
      "Epoch 529/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0778 - accuracy: 0.9956 - val_loss: 6.3305 - val_accuracy: 0.2442\n",
      "Epoch 530/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0810 - accuracy: 0.9913 - val_loss: 6.3148 - val_accuracy: 0.2384\n",
      "Epoch 531/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0786 - accuracy: 0.9956 - val_loss: 6.2578 - val_accuracy: 0.2500\n",
      "Epoch 532/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0800 - accuracy: 0.9927 - val_loss: 6.3825 - val_accuracy: 0.2326\n",
      "Epoch 533/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0793 - accuracy: 0.9942 - val_loss: 6.2528 - val_accuracy: 0.2326\n",
      "Epoch 534/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0781 - accuracy: 0.9942 - val_loss: 6.3952 - val_accuracy: 0.2442\n",
      "Epoch 535/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0748 - accuracy: 0.9971 - val_loss: 6.3094 - val_accuracy: 0.2384\n",
      "Epoch 536/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0757 - accuracy: 0.9942 - val_loss: 6.3292 - val_accuracy: 0.2500\n",
      "Epoch 537/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0751 - accuracy: 0.9942 - val_loss: 6.3313 - val_accuracy: 0.2384\n",
      "Epoch 538/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0753 - accuracy: 0.9956 - val_loss: 6.3459 - val_accuracy: 0.2442\n",
      "Epoch 539/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0778 - accuracy: 0.9942 - val_loss: 6.3332 - val_accuracy: 0.2442\n",
      "Epoch 540/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0744 - accuracy: 0.9956 - val_loss: 6.4276 - val_accuracy: 0.2384\n",
      "Epoch 541/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0765 - accuracy: 0.9927 - val_loss: 6.3240 - val_accuracy: 0.2442\n",
      "Epoch 542/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0730 - accuracy: 0.9927 - val_loss: 6.3764 - val_accuracy: 0.2326\n",
      "Epoch 543/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0769 - accuracy: 0.9942 - val_loss: 6.4289 - val_accuracy: 0.2267\n",
      "Epoch 544/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0865 - accuracy: 0.9840 - val_loss: 6.4103 - val_accuracy: 0.2442\n",
      "Epoch 545/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0803 - accuracy: 0.9884 - val_loss: 6.3946 - val_accuracy: 0.2326\n",
      "Epoch 546/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0774 - accuracy: 0.9956 - val_loss: 6.4805 - val_accuracy: 0.2326\n",
      "Epoch 547/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0804 - accuracy: 0.9942 - val_loss: 6.2700 - val_accuracy: 0.2558\n",
      "Epoch 548/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0718 - accuracy: 0.9942 - val_loss: 6.5575 - val_accuracy: 0.2151\n",
      "Epoch 549/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0738 - accuracy: 0.9927 - val_loss: 6.3441 - val_accuracy: 0.2442\n",
      "Epoch 550/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0691 - accuracy: 0.9942 - val_loss: 6.4445 - val_accuracy: 0.2384\n",
      "Epoch 551/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0721 - accuracy: 0.9927 - val_loss: 6.3821 - val_accuracy: 0.2558\n",
      "Epoch 552/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0692 - accuracy: 0.9942 - val_loss: 6.4701 - val_accuracy: 0.2384\n",
      "Epoch 553/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0708 - accuracy: 0.9927 - val_loss: 6.4160 - val_accuracy: 0.2442\n",
      "Epoch 554/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0684 - accuracy: 0.9971 - val_loss: 6.4545 - val_accuracy: 0.2442\n",
      "Epoch 555/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0679 - accuracy: 0.9956 - val_loss: 6.5442 - val_accuracy: 0.2209\n",
      "Epoch 556/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0730 - accuracy: 0.9927 - val_loss: 6.4700 - val_accuracy: 0.2500\n",
      "Epoch 557/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0689 - accuracy: 0.9942 - val_loss: 6.4698 - val_accuracy: 0.2384\n",
      "Epoch 558/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0664 - accuracy: 0.9942 - val_loss: 6.5350 - val_accuracy: 0.2384\n",
      "Epoch 559/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0717 - accuracy: 0.9927 - val_loss: 6.6003 - val_accuracy: 0.2151\n",
      "Epoch 560/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0696 - accuracy: 0.9942 - val_loss: 6.5427 - val_accuracy: 0.2267\n",
      "Epoch 561/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0705 - accuracy: 0.9942 - val_loss: 6.4631 - val_accuracy: 0.2558\n",
      "Epoch 562/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0718 - accuracy: 0.9927 - val_loss: 6.7035 - val_accuracy: 0.2151\n",
      "Epoch 563/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0713 - accuracy: 0.9913 - val_loss: 6.4774 - val_accuracy: 0.2558\n",
      "Epoch 564/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0688 - accuracy: 0.9927 - val_loss: 6.5255 - val_accuracy: 0.2267\n",
      "Epoch 565/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0664 - accuracy: 0.9971 - val_loss: 6.6258 - val_accuracy: 0.2267\n",
      "Epoch 566/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0682 - accuracy: 0.9913 - val_loss: 6.5551 - val_accuracy: 0.2442\n",
      "Epoch 567/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0680 - accuracy: 0.9956 - val_loss: 6.5778 - val_accuracy: 0.2326\n",
      "Epoch 568/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0678 - accuracy: 0.9956 - val_loss: 6.6252 - val_accuracy: 0.2326\n",
      "Epoch 569/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0800 - accuracy: 0.9869 - val_loss: 6.5133 - val_accuracy: 0.2267\n",
      "Epoch 570/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0885 - accuracy: 0.9782 - val_loss: 6.6602 - val_accuracy: 0.2267\n",
      "Epoch 571/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0701 - accuracy: 0.9942 - val_loss: 6.6167 - val_accuracy: 0.2558\n",
      "Epoch 572/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0687 - accuracy: 0.9913 - val_loss: 6.5886 - val_accuracy: 0.2442\n",
      "Epoch 573/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0665 - accuracy: 0.9927 - val_loss: 6.5691 - val_accuracy: 0.2326\n",
      "Epoch 574/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0671 - accuracy: 0.9956 - val_loss: 6.6571 - val_accuracy: 0.2384\n",
      "Epoch 575/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0672 - accuracy: 0.9913 - val_loss: 6.6538 - val_accuracy: 0.2151\n",
      "Epoch 576/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0648 - accuracy: 0.9942 - val_loss: 6.6312 - val_accuracy: 0.2442\n",
      "Epoch 577/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0707 - accuracy: 0.9898 - val_loss: 6.6651 - val_accuracy: 0.2384\n",
      "Epoch 578/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0678 - accuracy: 0.9913 - val_loss: 6.6717 - val_accuracy: 0.2267\n",
      "Epoch 579/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0657 - accuracy: 0.9913 - val_loss: 6.7722 - val_accuracy: 0.2442\n",
      "Epoch 580/1000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0673 - accuracy: 0.9913 - val_loss: 6.6897 - val_accuracy: 0.2384\n",
      "Epoch 581/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0650 - accuracy: 0.9956 - val_loss: 6.5345 - val_accuracy: 0.2384\n",
      "Epoch 582/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0632 - accuracy: 0.9942 - val_loss: 6.8216 - val_accuracy: 0.2093\n",
      "Epoch 583/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0638 - accuracy: 0.9971 - val_loss: 6.6435 - val_accuracy: 0.2500\n",
      "Epoch 584/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0652 - accuracy: 0.9927 - val_loss: 6.7804 - val_accuracy: 0.2326\n",
      "Epoch 585/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0620 - accuracy: 0.9913 - val_loss: 6.7360 - val_accuracy: 0.2326\n",
      "Epoch 586/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0626 - accuracy: 0.9956 - val_loss: 6.6713 - val_accuracy: 0.2267\n",
      "Epoch 587/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0638 - accuracy: 0.9913 - val_loss: 6.7320 - val_accuracy: 0.2500\n",
      "Epoch 588/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0648 - accuracy: 0.9942 - val_loss: 6.7827 - val_accuracy: 0.2326\n",
      "Epoch 589/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0799 - accuracy: 0.9855 - val_loss: 6.7166 - val_accuracy: 0.2267\n",
      "Epoch 590/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0731 - accuracy: 0.9898 - val_loss: 6.7203 - val_accuracy: 0.2209\n",
      "Epoch 591/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0671 - accuracy: 0.9927 - val_loss: 6.7810 - val_accuracy: 0.2267\n",
      "Epoch 592/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0668 - accuracy: 0.9898 - val_loss: 6.7607 - val_accuracy: 0.2326\n",
      "Epoch 593/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0615 - accuracy: 0.9956 - val_loss: 6.7951 - val_accuracy: 0.2442\n",
      "Epoch 594/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0579 - accuracy: 0.9971 - val_loss: 6.7032 - val_accuracy: 0.2384\n",
      "Epoch 595/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0574 - accuracy: 0.9942 - val_loss: 6.8110 - val_accuracy: 0.2151\n",
      "Epoch 596/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0583 - accuracy: 0.9942 - val_loss: 6.7038 - val_accuracy: 0.2442\n",
      "Epoch 597/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0582 - accuracy: 0.9942 - val_loss: 6.8395 - val_accuracy: 0.2209\n",
      "Epoch 598/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0629 - accuracy: 0.9942 - val_loss: 6.8349 - val_accuracy: 0.2326\n",
      "Epoch 599/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0603 - accuracy: 0.9971 - val_loss: 6.8308 - val_accuracy: 0.2326\n",
      "Epoch 600/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0650 - accuracy: 0.9898 - val_loss: 6.8640 - val_accuracy: 0.2267\n",
      "Epoch 601/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0644 - accuracy: 0.9956 - val_loss: 6.8504 - val_accuracy: 0.2326\n",
      "Epoch 602/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0650 - accuracy: 0.9913 - val_loss: 6.8031 - val_accuracy: 0.2500\n",
      "Epoch 603/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0611 - accuracy: 0.9942 - val_loss: 6.8336 - val_accuracy: 0.2442\n",
      "Epoch 604/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0614 - accuracy: 0.9927 - val_loss: 6.8763 - val_accuracy: 0.2384\n",
      "Epoch 605/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0631 - accuracy: 0.9956 - val_loss: 6.8825 - val_accuracy: 0.2209\n",
      "Epoch 606/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0590 - accuracy: 0.9956 - val_loss: 6.8103 - val_accuracy: 0.2326\n",
      "Epoch 607/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0589 - accuracy: 0.9942 - val_loss: 6.8826 - val_accuracy: 0.2326\n",
      "Epoch 608/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0534 - accuracy: 0.9971 - val_loss: 6.8856 - val_accuracy: 0.2442\n",
      "Epoch 609/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0552 - accuracy: 0.9971 - val_loss: 6.8455 - val_accuracy: 0.2384\n",
      "Epoch 610/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0617 - accuracy: 0.9927 - val_loss: 6.9175 - val_accuracy: 0.2209\n",
      "Epoch 611/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0572 - accuracy: 0.9927 - val_loss: 7.0035 - val_accuracy: 0.2326\n",
      "Epoch 612/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0639 - accuracy: 0.9927 - val_loss: 6.8010 - val_accuracy: 0.2442\n",
      "Epoch 613/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0537 - accuracy: 0.9971 - val_loss: 6.9431 - val_accuracy: 0.2267\n",
      "Epoch 614/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0548 - accuracy: 0.9971 - val_loss: 6.8630 - val_accuracy: 0.2442\n",
      "Epoch 615/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0517 - accuracy: 0.9942 - val_loss: 6.8897 - val_accuracy: 0.2442\n",
      "Epoch 616/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0532 - accuracy: 0.9927 - val_loss: 6.9087 - val_accuracy: 0.2267\n",
      "Epoch 617/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0540 - accuracy: 0.9942 - val_loss: 6.8914 - val_accuracy: 0.2326\n",
      "Epoch 618/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0541 - accuracy: 0.9942 - val_loss: 6.9559 - val_accuracy: 0.2384\n",
      "Epoch 619/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0542 - accuracy: 0.9956 - val_loss: 6.9794 - val_accuracy: 0.2267\n",
      "Epoch 620/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0563 - accuracy: 0.9971 - val_loss: 6.8766 - val_accuracy: 0.2500\n",
      "Epoch 621/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0562 - accuracy: 0.9942 - val_loss: 7.0663 - val_accuracy: 0.2442\n",
      "Epoch 622/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0583 - accuracy: 0.9927 - val_loss: 6.9093 - val_accuracy: 0.2500\n",
      "Epoch 623/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0511 - accuracy: 0.9985 - val_loss: 6.9068 - val_accuracy: 0.2442\n",
      "Epoch 624/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0518 - accuracy: 0.9956 - val_loss: 6.9967 - val_accuracy: 0.2442\n",
      "Epoch 625/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0490 - accuracy: 0.9971 - val_loss: 7.0001 - val_accuracy: 0.2384\n",
      "Epoch 626/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0512 - accuracy: 0.9956 - val_loss: 6.9872 - val_accuracy: 0.2326\n",
      "Epoch 627/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0506 - accuracy: 0.9927 - val_loss: 7.0631 - val_accuracy: 0.2267\n",
      "Epoch 628/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0503 - accuracy: 0.9971 - val_loss: 6.9523 - val_accuracy: 0.2384\n",
      "Epoch 629/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0565 - accuracy: 0.9942 - val_loss: 7.1522 - val_accuracy: 0.2209\n",
      "Epoch 630/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0517 - accuracy: 0.9956 - val_loss: 7.0867 - val_accuracy: 0.2209\n",
      "Epoch 631/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0580 - accuracy: 0.9942 - val_loss: 7.0247 - val_accuracy: 0.2326\n",
      "Epoch 632/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0559 - accuracy: 0.9942 - val_loss: 7.0734 - val_accuracy: 0.2093\n",
      "Epoch 633/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0538 - accuracy: 0.9942 - val_loss: 7.0667 - val_accuracy: 0.2384\n",
      "Epoch 634/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0485 - accuracy: 0.9956 - val_loss: 7.0026 - val_accuracy: 0.2384\n",
      "Epoch 635/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0530 - accuracy: 0.9956 - val_loss: 7.1500 - val_accuracy: 0.2267\n",
      "Epoch 636/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0541 - accuracy: 0.9927 - val_loss: 7.0142 - val_accuracy: 0.2500\n",
      "Epoch 637/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0548 - accuracy: 0.9913 - val_loss: 7.1248 - val_accuracy: 0.2384\n",
      "Epoch 638/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0496 - accuracy: 0.9971 - val_loss: 7.0741 - val_accuracy: 0.2326\n",
      "Epoch 639/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0526 - accuracy: 0.9927 - val_loss: 7.0862 - val_accuracy: 0.2442\n",
      "Epoch 640/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0506 - accuracy: 0.9971 - val_loss: 7.1307 - val_accuracy: 0.2326\n",
      "Epoch 641/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0541 - accuracy: 0.9913 - val_loss: 7.1898 - val_accuracy: 0.2326\n",
      "Epoch 642/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0517 - accuracy: 0.9927 - val_loss: 7.0807 - val_accuracy: 0.2326\n",
      "Epoch 643/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0519 - accuracy: 0.9956 - val_loss: 7.0041 - val_accuracy: 0.2384\n",
      "Epoch 644/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0479 - accuracy: 0.9956 - val_loss: 7.1900 - val_accuracy: 0.2326\n",
      "Epoch 645/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0484 - accuracy: 0.9956 - val_loss: 7.1142 - val_accuracy: 0.2500\n",
      "Epoch 646/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0504 - accuracy: 0.9927 - val_loss: 7.1851 - val_accuracy: 0.2267\n",
      "Epoch 647/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0483 - accuracy: 0.9956 - val_loss: 7.1181 - val_accuracy: 0.2500\n",
      "Epoch 648/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0506 - accuracy: 0.9942 - val_loss: 7.2437 - val_accuracy: 0.2209\n",
      "Epoch 649/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0462 - accuracy: 0.9956 - val_loss: 7.1101 - val_accuracy: 0.2442\n",
      "Epoch 650/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0444 - accuracy: 0.9956 - val_loss: 7.2353 - val_accuracy: 0.2326\n",
      "Epoch 651/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0449 - accuracy: 0.9956 - val_loss: 7.1003 - val_accuracy: 0.2384\n",
      "Epoch 652/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0448 - accuracy: 0.9971 - val_loss: 7.2161 - val_accuracy: 0.2384\n",
      "Epoch 653/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0435 - accuracy: 0.9956 - val_loss: 7.1284 - val_accuracy: 0.2442\n",
      "Epoch 654/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0457 - accuracy: 0.9942 - val_loss: 7.2269 - val_accuracy: 0.2326\n",
      "Epoch 655/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0455 - accuracy: 0.9971 - val_loss: 7.1980 - val_accuracy: 0.2384\n",
      "Epoch 656/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0457 - accuracy: 0.9956 - val_loss: 7.1898 - val_accuracy: 0.2500\n",
      "Epoch 657/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0479 - accuracy: 0.9942 - val_loss: 7.3029 - val_accuracy: 0.2384\n",
      "Epoch 658/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0427 - accuracy: 0.9956 - val_loss: 7.2006 - val_accuracy: 0.2326\n",
      "Epoch 659/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0509 - accuracy: 0.9927 - val_loss: 7.2130 - val_accuracy: 0.2442\n",
      "Epoch 660/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0435 - accuracy: 0.9971 - val_loss: 7.2818 - val_accuracy: 0.2384\n",
      "Epoch 661/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0460 - accuracy: 0.9971 - val_loss: 7.2489 - val_accuracy: 0.2384\n",
      "Epoch 662/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0434 - accuracy: 0.9956 - val_loss: 7.1765 - val_accuracy: 0.2558\n",
      "Epoch 663/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0467 - accuracy: 0.9956 - val_loss: 7.3069 - val_accuracy: 0.2151\n",
      "Epoch 664/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.0428 - accuracy: 0.9971 - val_loss: 7.2547 - val_accuracy: 0.2326\n",
      "Epoch 665/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0424 - accuracy: 0.9971 - val_loss: 7.3350 - val_accuracy: 0.2267\n",
      "Epoch 666/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0442 - accuracy: 0.9942 - val_loss: 7.3063 - val_accuracy: 0.2384\n",
      "Epoch 667/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0458 - accuracy: 0.9927 - val_loss: 7.3095 - val_accuracy: 0.2326\n",
      "Epoch 668/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0513 - accuracy: 0.9913 - val_loss: 7.3133 - val_accuracy: 0.2326\n",
      "Epoch 669/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0524 - accuracy: 0.9913 - val_loss: 7.2746 - val_accuracy: 0.2384\n",
      "Epoch 670/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0500 - accuracy: 0.9927 - val_loss: 7.3051 - val_accuracy: 0.2442\n",
      "Epoch 671/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0473 - accuracy: 0.9956 - val_loss: 7.3270 - val_accuracy: 0.2267\n",
      "Epoch 672/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0423 - accuracy: 0.9971 - val_loss: 7.3480 - val_accuracy: 0.2384\n",
      "Epoch 673/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0411 - accuracy: 0.9985 - val_loss: 7.2771 - val_accuracy: 0.2500\n",
      "Epoch 674/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0425 - accuracy: 0.9971 - val_loss: 7.5291 - val_accuracy: 0.2035\n",
      "Epoch 675/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0414 - accuracy: 0.9956 - val_loss: 7.2683 - val_accuracy: 0.2558\n",
      "Epoch 676/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0447 - accuracy: 0.9927 - val_loss: 7.4239 - val_accuracy: 0.2267\n",
      "Epoch 677/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0457 - accuracy: 0.9942 - val_loss: 7.3073 - val_accuracy: 0.2384\n",
      "Epoch 678/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0471 - accuracy: 0.9956 - val_loss: 7.3751 - val_accuracy: 0.2267\n",
      "Epoch 679/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0408 - accuracy: 0.9956 - val_loss: 7.3615 - val_accuracy: 0.2384\n",
      "Epoch 680/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0439 - accuracy: 0.9971 - val_loss: 7.3481 - val_accuracy: 0.2267\n",
      "Epoch 681/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0442 - accuracy: 0.9927 - val_loss: 7.4521 - val_accuracy: 0.2326\n",
      "Epoch 682/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0432 - accuracy: 0.9956 - val_loss: 7.3858 - val_accuracy: 0.2209\n",
      "Epoch 683/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0418 - accuracy: 0.9956 - val_loss: 7.3878 - val_accuracy: 0.2326\n",
      "Epoch 684/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0380 - accuracy: 0.9971 - val_loss: 7.5299 - val_accuracy: 0.2209\n",
      "Epoch 685/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0407 - accuracy: 0.9971 - val_loss: 7.3536 - val_accuracy: 0.2442\n",
      "Epoch 686/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0391 - accuracy: 0.9942 - val_loss: 7.3945 - val_accuracy: 0.2326\n",
      "Epoch 687/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0384 - accuracy: 0.9971 - val_loss: 7.4476 - val_accuracy: 0.2442\n",
      "Epoch 688/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0394 - accuracy: 0.9971 - val_loss: 7.4503 - val_accuracy: 0.2209\n",
      "Epoch 689/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0397 - accuracy: 0.9971 - val_loss: 7.3464 - val_accuracy: 0.2500\n",
      "Epoch 690/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0396 - accuracy: 0.9956 - val_loss: 7.4754 - val_accuracy: 0.2442\n",
      "Epoch 691/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0386 - accuracy: 0.9971 - val_loss: 7.4587 - val_accuracy: 0.2267\n",
      "Epoch 692/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0425 - accuracy: 0.9942 - val_loss: 7.3947 - val_accuracy: 0.2500\n",
      "Epoch 693/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0377 - accuracy: 0.9956 - val_loss: 7.5395 - val_accuracy: 0.2093\n",
      "Epoch 694/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0396 - accuracy: 0.9985 - val_loss: 7.4240 - val_accuracy: 0.2442\n",
      "Epoch 695/1000\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 0.0386 - accuracy: 0.9956 - val_loss: 7.5661 - val_accuracy: 0.2326\n",
      "Epoch 696/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0428 - accuracy: 0.9942 - val_loss: 7.4158 - val_accuracy: 0.2384\n",
      "Epoch 697/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0411 - accuracy: 0.9942 - val_loss: 7.4620 - val_accuracy: 0.2442\n",
      "Epoch 698/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0378 - accuracy: 0.9942 - val_loss: 7.5361 - val_accuracy: 0.2384\n",
      "Epoch 699/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0448 - accuracy: 0.9942 - val_loss: 7.4695 - val_accuracy: 0.2326\n",
      "Epoch 700/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0399 - accuracy: 0.9942 - val_loss: 7.4150 - val_accuracy: 0.2500\n",
      "Epoch 701/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0377 - accuracy: 0.9956 - val_loss: 7.5447 - val_accuracy: 0.2267\n",
      "Epoch 702/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0379 - accuracy: 0.9942 - val_loss: 7.4741 - val_accuracy: 0.2558\n",
      "Epoch 703/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0366 - accuracy: 0.9971 - val_loss: 7.6014 - val_accuracy: 0.2326\n",
      "Epoch 704/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0391 - accuracy: 0.9956 - val_loss: 7.5520 - val_accuracy: 0.2326\n",
      "Epoch 705/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0411 - accuracy: 0.9971 - val_loss: 7.4738 - val_accuracy: 0.2500\n",
      "Epoch 706/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0354 - accuracy: 0.9971 - val_loss: 7.5928 - val_accuracy: 0.2267\n",
      "Epoch 707/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0374 - accuracy: 0.9942 - val_loss: 7.5416 - val_accuracy: 0.2442\n",
      "Epoch 708/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0337 - accuracy: 0.9971 - val_loss: 7.5597 - val_accuracy: 0.2267\n",
      "Epoch 709/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0360 - accuracy: 0.9956 - val_loss: 7.5405 - val_accuracy: 0.2442\n",
      "Epoch 710/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0349 - accuracy: 0.9971 - val_loss: 7.5949 - val_accuracy: 0.2326\n",
      "Epoch 711/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0367 - accuracy: 0.9942 - val_loss: 7.5350 - val_accuracy: 0.2500\n",
      "Epoch 712/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0353 - accuracy: 0.9942 - val_loss: 7.5490 - val_accuracy: 0.2442\n",
      "Epoch 713/1000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0354 - accuracy: 0.9956 - val_loss: 7.6172 - val_accuracy: 0.2326\n",
      "Epoch 714/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0366 - accuracy: 0.9985 - val_loss: 7.6224 - val_accuracy: 0.2500\n",
      "Epoch 715/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0380 - accuracy: 0.9971 - val_loss: 7.6890 - val_accuracy: 0.2093\n",
      "Epoch 716/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0356 - accuracy: 0.9985 - val_loss: 7.5646 - val_accuracy: 0.2500\n",
      "Epoch 717/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0352 - accuracy: 0.9971 - val_loss: 7.5611 - val_accuracy: 0.2500\n",
      "Epoch 718/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0340 - accuracy: 0.9956 - val_loss: 7.6105 - val_accuracy: 0.2384\n",
      "Epoch 719/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0348 - accuracy: 0.9956 - val_loss: 7.6004 - val_accuracy: 0.2442\n",
      "Epoch 720/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0348 - accuracy: 0.9956 - val_loss: 7.6604 - val_accuracy: 0.2326\n",
      "Epoch 721/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0332 - accuracy: 0.9971 - val_loss: 7.5973 - val_accuracy: 0.2500\n",
      "Epoch 722/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0325 - accuracy: 0.9971 - val_loss: 7.7107 - val_accuracy: 0.2267\n",
      "Epoch 723/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0356 - accuracy: 0.9956 - val_loss: 7.6210 - val_accuracy: 0.2267\n",
      "Epoch 724/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0320 - accuracy: 0.9985 - val_loss: 7.6836 - val_accuracy: 0.2326\n",
      "Epoch 725/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.0326 - accuracy: 0.9956 - val_loss: 7.7403 - val_accuracy: 0.2209\n",
      "Epoch 726/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0324 - accuracy: 0.9985 - val_loss: 7.5785 - val_accuracy: 0.2558\n",
      "Epoch 727/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0320 - accuracy: 0.9985 - val_loss: 7.7660 - val_accuracy: 0.2326\n",
      "Epoch 728/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0379 - accuracy: 0.9927 - val_loss: 7.7013 - val_accuracy: 0.2384\n",
      "Epoch 729/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0326 - accuracy: 0.9971 - val_loss: 7.6830 - val_accuracy: 0.2384\n",
      "Epoch 730/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0315 - accuracy: 0.9971 - val_loss: 7.7346 - val_accuracy: 0.2442\n",
      "Epoch 731/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0315 - accuracy: 0.9956 - val_loss: 7.6991 - val_accuracy: 0.2442\n",
      "Epoch 732/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0322 - accuracy: 0.9942 - val_loss: 7.7747 - val_accuracy: 0.2267\n",
      "Epoch 733/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0310 - accuracy: 0.9985 - val_loss: 7.7240 - val_accuracy: 0.2442\n",
      "Epoch 734/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0310 - accuracy: 0.9971 - val_loss: 7.6936 - val_accuracy: 0.2500\n",
      "Epoch 735/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0316 - accuracy: 0.9971 - val_loss: 7.7196 - val_accuracy: 0.2326\n",
      "Epoch 736/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0317 - accuracy: 0.9985 - val_loss: 7.7854 - val_accuracy: 0.2384\n",
      "Epoch 737/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0332 - accuracy: 0.9956 - val_loss: 7.7823 - val_accuracy: 0.2209\n",
      "Epoch 738/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0318 - accuracy: 0.9956 - val_loss: 7.7253 - val_accuracy: 0.2326\n",
      "Epoch 739/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0329 - accuracy: 0.9971 - val_loss: 7.7799 - val_accuracy: 0.2326\n",
      "Epoch 740/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0320 - accuracy: 0.9971 - val_loss: 7.7397 - val_accuracy: 0.2500\n",
      "Epoch 741/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0329 - accuracy: 0.9942 - val_loss: 7.8568 - val_accuracy: 0.2209\n",
      "Epoch 742/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0324 - accuracy: 0.9956 - val_loss: 7.7616 - val_accuracy: 0.2267\n",
      "Epoch 743/1000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0352 - accuracy: 0.9956 - val_loss: 7.7325 - val_accuracy: 0.2500\n",
      "Epoch 744/1000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0345 - accuracy: 0.9942 - val_loss: 7.7600 - val_accuracy: 0.2384\n",
      "Epoch 745/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0307 - accuracy: 0.9956 - val_loss: 7.7868 - val_accuracy: 0.2384\n",
      "Epoch 746/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.0323 - accuracy: 0.9956 - val_loss: 7.9358 - val_accuracy: 0.2093\n",
      "Epoch 747/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0311 - accuracy: 0.9985 - val_loss: 7.6886 - val_accuracy: 0.2442\n",
      "Epoch 748/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0302 - accuracy: 0.9985 - val_loss: 7.8039 - val_accuracy: 0.2442\n",
      "Epoch 749/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0295 - accuracy: 0.9985 - val_loss: 7.7482 - val_accuracy: 0.2442\n",
      "Epoch 750/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0294 - accuracy: 0.9971 - val_loss: 7.8914 - val_accuracy: 0.2326\n",
      "Epoch 751/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0302 - accuracy: 0.9971 - val_loss: 7.8262 - val_accuracy: 0.2384\n",
      "Epoch 752/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0300 - accuracy: 0.9956 - val_loss: 7.8829 - val_accuracy: 0.2209\n",
      "Epoch 753/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0306 - accuracy: 0.9971 - val_loss: 7.8006 - val_accuracy: 0.2384\n",
      "Epoch 754/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.0317 - accuracy: 0.9956 - val_loss: 7.8456 - val_accuracy: 0.2384\n",
      "Epoch 755/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0321 - accuracy: 0.9942 - val_loss: 7.9289 - val_accuracy: 0.2267\n",
      "Epoch 756/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0308 - accuracy: 0.9971 - val_loss: 7.8597 - val_accuracy: 0.2442\n",
      "Epoch 757/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0288 - accuracy: 0.9971 - val_loss: 7.8046 - val_accuracy: 0.2442\n",
      "Epoch 758/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0294 - accuracy: 0.9985 - val_loss: 8.0261 - val_accuracy: 0.2151\n",
      "Epoch 759/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0318 - accuracy: 0.9971 - val_loss: 7.7523 - val_accuracy: 0.2558\n",
      "Epoch 760/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0329 - accuracy: 0.9971 - val_loss: 7.9357 - val_accuracy: 0.2326\n",
      "Epoch 761/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 7.9377 - val_accuracy: 0.2384\n",
      "Epoch 762/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0336 - accuracy: 0.9942 - val_loss: 7.7746 - val_accuracy: 0.2442\n",
      "Epoch 763/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0300 - accuracy: 0.9985 - val_loss: 8.1276 - val_accuracy: 0.2151\n",
      "Epoch 764/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0335 - accuracy: 0.9971 - val_loss: 7.8857 - val_accuracy: 0.2442\n",
      "Epoch 765/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0297 - accuracy: 0.9956 - val_loss: 7.9381 - val_accuracy: 0.2384\n",
      "Epoch 766/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0281 - accuracy: 0.9971 - val_loss: 7.9193 - val_accuracy: 0.2326\n",
      "Epoch 767/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0281 - accuracy: 0.9985 - val_loss: 7.9345 - val_accuracy: 0.2326\n",
      "Epoch 768/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0298 - accuracy: 0.9942 - val_loss: 7.9485 - val_accuracy: 0.2442\n",
      "Epoch 769/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0321 - accuracy: 0.9927 - val_loss: 8.0538 - val_accuracy: 0.2267\n",
      "Epoch 770/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0405 - accuracy: 0.9927 - val_loss: 8.0270 - val_accuracy: 0.2326\n",
      "Epoch 771/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0517 - accuracy: 0.9884 - val_loss: 7.9582 - val_accuracy: 0.2267\n",
      "Epoch 772/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0346 - accuracy: 0.9956 - val_loss: 8.0324 - val_accuracy: 0.2151\n",
      "Epoch 773/1000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0310 - accuracy: 0.9956 - val_loss: 7.9550 - val_accuracy: 0.2442\n",
      "Epoch 774/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0276 - accuracy: 0.9971 - val_loss: 8.0287 - val_accuracy: 0.2326\n",
      "Epoch 775/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0269 - accuracy: 0.9985 - val_loss: 7.9551 - val_accuracy: 0.2267\n",
      "Epoch 776/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0267 - accuracy: 0.9971 - val_loss: 7.9960 - val_accuracy: 0.2442\n",
      "Epoch 777/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0263 - accuracy: 0.9971 - val_loss: 8.0505 - val_accuracy: 0.2267\n",
      "Epoch 778/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0261 - accuracy: 0.9985 - val_loss: 7.9708 - val_accuracy: 0.2384\n",
      "Epoch 779/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0271 - accuracy: 0.9956 - val_loss: 7.9636 - val_accuracy: 0.2500\n",
      "Epoch 780/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0266 - accuracy: 0.9985 - val_loss: 8.1435 - val_accuracy: 0.2035\n",
      "Epoch 781/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0263 - accuracy: 0.9971 - val_loss: 7.9742 - val_accuracy: 0.2558\n",
      "Epoch 782/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0299 - accuracy: 0.9985 - val_loss: 8.1708 - val_accuracy: 0.2035\n",
      "Epoch 783/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0267 - accuracy: 0.9956 - val_loss: 7.9809 - val_accuracy: 0.2442\n",
      "Epoch 784/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0253 - accuracy: 0.9985 - val_loss: 8.1327 - val_accuracy: 0.2151\n",
      "Epoch 785/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0285 - accuracy: 0.9971 - val_loss: 7.9862 - val_accuracy: 0.2384\n",
      "Epoch 786/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0293 - accuracy: 0.9985 - val_loss: 8.0948 - val_accuracy: 0.2384\n",
      "Epoch 787/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0256 - accuracy: 0.9985 - val_loss: 8.2364 - val_accuracy: 0.2151\n",
      "Epoch 788/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0274 - accuracy: 0.9971 - val_loss: 7.9504 - val_accuracy: 0.2558\n",
      "Epoch 789/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0265 - accuracy: 0.9956 - val_loss: 8.2191 - val_accuracy: 0.2035\n",
      "Epoch 790/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0264 - accuracy: 0.9985 - val_loss: 8.0272 - val_accuracy: 0.2500\n",
      "Epoch 791/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0242 - accuracy: 0.9985 - val_loss: 8.1132 - val_accuracy: 0.2326\n",
      "Epoch 792/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0254 - accuracy: 0.9971 - val_loss: 8.1101 - val_accuracy: 0.2326\n",
      "Epoch 793/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0264 - accuracy: 0.9971 - val_loss: 8.1598 - val_accuracy: 0.2267\n",
      "Epoch 794/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.0311 - accuracy: 0.9942 - val_loss: 8.0811 - val_accuracy: 0.2326\n",
      "Epoch 795/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0293 - accuracy: 0.9971 - val_loss: 8.0685 - val_accuracy: 0.2326\n",
      "Epoch 796/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0299 - accuracy: 0.9956 - val_loss: 8.1424 - val_accuracy: 0.2500\n",
      "Epoch 797/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0239 - accuracy: 0.9985 - val_loss: 8.2362 - val_accuracy: 0.2267\n",
      "Epoch 798/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0293 - accuracy: 0.9956 - val_loss: 8.0020 - val_accuracy: 0.2500\n",
      "Epoch 799/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0303 - accuracy: 0.9956 - val_loss: 8.1830 - val_accuracy: 0.2209\n",
      "Epoch 800/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0318 - accuracy: 0.9942 - val_loss: 8.2460 - val_accuracy: 0.2326\n",
      "Epoch 801/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0325 - accuracy: 0.9985 - val_loss: 8.2438 - val_accuracy: 0.2267\n",
      "Epoch 802/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0245 - accuracy: 0.9971 - val_loss: 8.0630 - val_accuracy: 0.2326\n",
      "Epoch 803/1000\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.0244 - accuracy: 0.9971 - val_loss: 8.2119 - val_accuracy: 0.2267\n",
      "Epoch 804/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0243 - accuracy: 0.9956 - val_loss: 8.1980 - val_accuracy: 0.2500\n",
      "Epoch 805/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0311 - accuracy: 0.9913 - val_loss: 8.2653 - val_accuracy: 0.2267\n",
      "Epoch 806/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0310 - accuracy: 0.9956 - val_loss: 8.2447 - val_accuracy: 0.2267\n",
      "Epoch 807/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0245 - accuracy: 0.9971 - val_loss: 8.3141 - val_accuracy: 0.2326\n",
      "Epoch 808/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0276 - accuracy: 0.9985 - val_loss: 8.1832 - val_accuracy: 0.2209\n",
      "Epoch 809/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0250 - accuracy: 0.9985 - val_loss: 8.1649 - val_accuracy: 0.2384\n",
      "Epoch 810/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0235 - accuracy: 0.9985 - val_loss: 8.2708 - val_accuracy: 0.2209\n",
      "Epoch 811/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0229 - accuracy: 0.9971 - val_loss: 8.2202 - val_accuracy: 0.2326\n",
      "Epoch 812/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0261 - accuracy: 0.9956 - val_loss: 8.2517 - val_accuracy: 0.2209\n",
      "Epoch 813/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0266 - accuracy: 0.9956 - val_loss: 8.1599 - val_accuracy: 0.2384\n",
      "Epoch 814/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0300 - accuracy: 0.9942 - val_loss: 8.3677 - val_accuracy: 0.2326\n",
      "Epoch 815/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0249 - accuracy: 0.9956 - val_loss: 8.2104 - val_accuracy: 0.2500\n",
      "Epoch 816/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0233 - accuracy: 0.9985 - val_loss: 8.3193 - val_accuracy: 0.2442\n",
      "Epoch 817/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0228 - accuracy: 0.9971 - val_loss: 8.2669 - val_accuracy: 0.2326\n",
      "Epoch 818/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0241 - accuracy: 0.9971 - val_loss: 8.3442 - val_accuracy: 0.2209\n",
      "Epoch 819/1000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0272 - accuracy: 0.9927 - val_loss: 8.3167 - val_accuracy: 0.2267\n",
      "Epoch 820/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0248 - accuracy: 0.9971 - val_loss: 8.2732 - val_accuracy: 0.2326\n",
      "Epoch 821/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0287 - accuracy: 0.9942 - val_loss: 8.2865 - val_accuracy: 0.2384\n",
      "Epoch 822/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0220 - accuracy: 0.9985 - val_loss: 8.2873 - val_accuracy: 0.2442\n",
      "Epoch 823/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0253 - accuracy: 0.9971 - val_loss: 8.4115 - val_accuracy: 0.2035\n",
      "Epoch 824/1000\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 0.0230 - accuracy: 0.9971 - val_loss: 8.3121 - val_accuracy: 0.2558\n",
      "Epoch 825/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0239 - accuracy: 0.9942 - val_loss: 8.3890 - val_accuracy: 0.2093\n",
      "Epoch 826/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0240 - accuracy: 0.9956 - val_loss: 8.3139 - val_accuracy: 0.2326\n",
      "Epoch 827/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0235 - accuracy: 0.9985 - val_loss: 8.3911 - val_accuracy: 0.2267\n",
      "Epoch 828/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0229 - accuracy: 0.9971 - val_loss: 8.3322 - val_accuracy: 0.2267\n",
      "Epoch 829/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0254 - accuracy: 0.9956 - val_loss: 8.3954 - val_accuracy: 0.2267\n",
      "Epoch 830/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0271 - accuracy: 0.9942 - val_loss: 8.3650 - val_accuracy: 0.2326\n",
      "Epoch 831/1000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0224 - accuracy: 0.9985 - val_loss: 8.4072 - val_accuracy: 0.2209\n",
      "Epoch 832/1000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0218 - accuracy: 0.9971 - val_loss: 8.4228 - val_accuracy: 0.2384\n",
      "Epoch 833/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0232 - accuracy: 0.9971 - val_loss: 8.3909 - val_accuracy: 0.2326\n",
      "Epoch 834/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0230 - accuracy: 0.9956 - val_loss: 8.3327 - val_accuracy: 0.2500\n",
      "Epoch 835/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0229 - accuracy: 0.9971 - val_loss: 8.4148 - val_accuracy: 0.2267\n",
      "Epoch 836/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0238 - accuracy: 0.9971 - val_loss: 8.4372 - val_accuracy: 0.2326\n",
      "Epoch 837/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0261 - accuracy: 0.9942 - val_loss: 8.4161 - val_accuracy: 0.2326\n",
      "Epoch 838/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0217 - accuracy: 0.9985 - val_loss: 8.4821 - val_accuracy: 0.2209\n",
      "Epoch 839/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0214 - accuracy: 0.9971 - val_loss: 8.3628 - val_accuracy: 0.2384\n",
      "Epoch 840/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.0208 - accuracy: 0.9985 - val_loss: 8.4784 - val_accuracy: 0.2209\n",
      "Epoch 841/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0202 - accuracy: 0.9985 - val_loss: 8.5295 - val_accuracy: 0.2267\n",
      "Epoch 842/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0218 - accuracy: 0.9985 - val_loss: 8.4348 - val_accuracy: 0.2326\n",
      "Epoch 843/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0224 - accuracy: 0.9985 - val_loss: 8.4293 - val_accuracy: 0.2267\n",
      "Epoch 844/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0224 - accuracy: 0.9971 - val_loss: 8.4779 - val_accuracy: 0.2326\n",
      "Epoch 845/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0293 - accuracy: 0.9942 - val_loss: 8.3040 - val_accuracy: 0.2442\n",
      "Epoch 846/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0228 - accuracy: 0.9956 - val_loss: 8.5530 - val_accuracy: 0.2267\n",
      "Epoch 847/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0221 - accuracy: 0.9942 - val_loss: 8.4234 - val_accuracy: 0.2326\n",
      "Epoch 848/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0222 - accuracy: 0.9956 - val_loss: 8.4879 - val_accuracy: 0.2267\n",
      "Epoch 849/1000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0214 - accuracy: 0.9956 - val_loss: 8.4699 - val_accuracy: 0.2267\n",
      "Epoch 850/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 8.5615 - val_accuracy: 0.2267\n",
      "Epoch 851/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.0192 - accuracy: 0.9971 - val_loss: 8.4350 - val_accuracy: 0.2500\n",
      "Epoch 852/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0197 - accuracy: 0.9985 - val_loss: 8.5105 - val_accuracy: 0.2209\n",
      "Epoch 853/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0221 - accuracy: 0.9985 - val_loss: 8.5759 - val_accuracy: 0.2267\n",
      "Epoch 854/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0280 - accuracy: 0.9971 - val_loss: 8.7516 - val_accuracy: 0.2151\n",
      "Epoch 855/1000\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0237 - accuracy: 0.9956 - val_loss: 8.4523 - val_accuracy: 0.2500\n",
      "Epoch 856/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0221 - accuracy: 0.9956 - val_loss: 8.5839 - val_accuracy: 0.2267\n",
      "Epoch 857/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0193 - accuracy: 0.9985 - val_loss: 8.4713 - val_accuracy: 0.2558\n",
      "Epoch 858/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0190 - accuracy: 0.9985 - val_loss: 8.5900 - val_accuracy: 0.2326\n",
      "Epoch 859/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0201 - accuracy: 0.9985 - val_loss: 8.5798 - val_accuracy: 0.2209\n",
      "Epoch 860/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0223 - accuracy: 0.9985 - val_loss: 8.6523 - val_accuracy: 0.2384\n",
      "Epoch 861/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0247 - accuracy: 0.9956 - val_loss: 8.7005 - val_accuracy: 0.2151\n",
      "Epoch 862/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0288 - accuracy: 0.9942 - val_loss: 8.5815 - val_accuracy: 0.2326\n",
      "Epoch 863/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0241 - accuracy: 0.9985 - val_loss: 8.4209 - val_accuracy: 0.2384\n",
      "Epoch 864/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0267 - accuracy: 0.9971 - val_loss: 8.6913 - val_accuracy: 0.2442\n",
      "Epoch 865/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0261 - accuracy: 0.9971 - val_loss: 8.6094 - val_accuracy: 0.2267\n",
      "Epoch 866/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0355 - accuracy: 0.9942 - val_loss: 8.4930 - val_accuracy: 0.2442\n",
      "Epoch 867/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0317 - accuracy: 0.9942 - val_loss: 8.5578 - val_accuracy: 0.2267\n",
      "Epoch 868/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0248 - accuracy: 0.9971 - val_loss: 8.6502 - val_accuracy: 0.2616\n",
      "Epoch 869/1000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0216 - accuracy: 0.9985 - val_loss: 8.4920 - val_accuracy: 0.2326\n",
      "Epoch 870/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0194 - accuracy: 0.9985 - val_loss: 8.6557 - val_accuracy: 0.2267\n",
      "Epoch 871/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0273 - accuracy: 0.9942 - val_loss: 8.6128 - val_accuracy: 0.2442\n",
      "Epoch 872/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0215 - accuracy: 0.9985 - val_loss: 8.8088 - val_accuracy: 0.2267\n",
      "Epoch 873/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0211 - accuracy: 0.9956 - val_loss: 8.5313 - val_accuracy: 0.2500\n",
      "Epoch 874/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0212 - accuracy: 0.9971 - val_loss: 8.6246 - val_accuracy: 0.2209\n",
      "Epoch 875/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0245 - accuracy: 0.9956 - val_loss: 8.5877 - val_accuracy: 0.2384\n",
      "Epoch 876/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0228 - accuracy: 0.9927 - val_loss: 8.5675 - val_accuracy: 0.2384\n",
      "Epoch 877/1000\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.0192 - accuracy: 0.9985 - val_loss: 8.7425 - val_accuracy: 0.2267\n",
      "Epoch 878/1000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0186 - accuracy: 0.9985 - val_loss: 8.6935 - val_accuracy: 0.2209\n",
      "Epoch 879/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0197 - accuracy: 0.9956 - val_loss: 8.6803 - val_accuracy: 0.2267\n",
      "Epoch 880/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0202 - accuracy: 0.9942 - val_loss: 8.7156 - val_accuracy: 0.2326\n",
      "Epoch 881/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0193 - accuracy: 0.9942 - val_loss: 8.7875 - val_accuracy: 0.2093\n",
      "Epoch 882/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0192 - accuracy: 0.9956 - val_loss: 8.5948 - val_accuracy: 0.2384\n",
      "Epoch 883/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0177 - accuracy: 0.9985 - val_loss: 8.7394 - val_accuracy: 0.2267\n",
      "Epoch 884/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.0200 - accuracy: 0.9971 - val_loss: 8.8898 - val_accuracy: 0.2326\n",
      "Epoch 885/1000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 8.7391 - val_accuracy: 0.2209\n",
      "Epoch 886/1000\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0194 - accuracy: 0.9971 - val_loss: 8.6579 - val_accuracy: 0.2267\n",
      "Epoch 887/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0181 - accuracy: 0.9971 - val_loss: 8.7544 - val_accuracy: 0.2209\n",
      "Epoch 888/1000\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0175 - accuracy: 0.9971 - val_loss: 8.6823 - val_accuracy: 0.2442\n",
      "Epoch 889/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0197 - accuracy: 0.9971 - val_loss: 8.7411 - val_accuracy: 0.2326\n",
      "Epoch 890/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0194 - accuracy: 0.9956 - val_loss: 8.7801 - val_accuracy: 0.2267\n",
      "Epoch 891/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0231 - accuracy: 0.9971 - val_loss: 8.7556 - val_accuracy: 0.2326\n",
      "Epoch 892/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0193 - accuracy: 0.9971 - val_loss: 8.6698 - val_accuracy: 0.2384\n",
      "Epoch 893/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0191 - accuracy: 0.9956 - val_loss: 8.7988 - val_accuracy: 0.2384\n",
      "Epoch 894/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0164 - accuracy: 0.9985 - val_loss: 8.8182 - val_accuracy: 0.2209\n",
      "Epoch 895/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0175 - accuracy: 0.9985 - val_loss: 8.8009 - val_accuracy: 0.2267\n",
      "Epoch 896/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0165 - accuracy: 0.9985 - val_loss: 8.8371 - val_accuracy: 0.2209\n",
      "Epoch 897/1000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0167 - accuracy: 0.9971 - val_loss: 8.7769 - val_accuracy: 0.2384\n",
      "Epoch 898/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0162 - accuracy: 0.9985 - val_loss: 8.7764 - val_accuracy: 0.2442\n",
      "Epoch 899/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0175 - accuracy: 0.9985 - val_loss: 8.8950 - val_accuracy: 0.2326\n",
      "Epoch 900/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0241 - accuracy: 0.9956 - val_loss: 8.7648 - val_accuracy: 0.2267\n",
      "Epoch 901/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0266 - accuracy: 0.9942 - val_loss: 8.9622 - val_accuracy: 0.2151\n",
      "Epoch 902/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0284 - accuracy: 0.9942 - val_loss: 8.7511 - val_accuracy: 0.2209\n",
      "Epoch 903/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0222 - accuracy: 0.9956 - val_loss: 8.8037 - val_accuracy: 0.2326\n",
      "Epoch 904/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0185 - accuracy: 0.9971 - val_loss: 8.9240 - val_accuracy: 0.2442\n",
      "Epoch 905/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0214 - accuracy: 0.9956 - val_loss: 8.7959 - val_accuracy: 0.2267\n",
      "Epoch 906/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0222 - accuracy: 0.9971 - val_loss: 8.8312 - val_accuracy: 0.2384\n",
      "Epoch 907/1000\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 0.0189 - accuracy: 0.9985 - val_loss: 8.8255 - val_accuracy: 0.2267\n",
      "Epoch 908/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0166 - accuracy: 0.9971 - val_loss: 8.8718 - val_accuracy: 0.2267\n",
      "Epoch 909/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0177 - accuracy: 0.9971 - val_loss: 8.8516 - val_accuracy: 0.2209\n",
      "Epoch 910/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0201 - accuracy: 0.9985 - val_loss: 8.8368 - val_accuracy: 0.2384\n",
      "Epoch 911/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0181 - accuracy: 0.9985 - val_loss: 8.9828 - val_accuracy: 0.1977\n",
      "Epoch 912/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0168 - accuracy: 0.9985 - val_loss: 8.7744 - val_accuracy: 0.2500\n",
      "Epoch 913/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0179 - accuracy: 0.9985 - val_loss: 8.9644 - val_accuracy: 0.2326\n",
      "Epoch 914/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0170 - accuracy: 0.9971 - val_loss: 8.8234 - val_accuracy: 0.2326\n",
      "Epoch 915/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0185 - accuracy: 0.9971 - val_loss: 8.8854 - val_accuracy: 0.2209\n",
      "Epoch 916/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 8.8499 - val_accuracy: 0.2500\n",
      "Epoch 917/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0173 - accuracy: 0.9985 - val_loss: 9.0276 - val_accuracy: 0.2326\n",
      "Epoch 918/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0195 - accuracy: 0.9942 - val_loss: 8.9027 - val_accuracy: 0.2384\n",
      "Epoch 919/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0154 - accuracy: 0.9985 - val_loss: 8.8287 - val_accuracy: 0.2442\n",
      "Epoch 920/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0149 - accuracy: 0.9985 - val_loss: 8.9382 - val_accuracy: 0.2384\n",
      "Epoch 921/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0151 - accuracy: 0.9985 - val_loss: 8.8727 - val_accuracy: 0.2326\n",
      "Epoch 922/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 8.9249 - val_accuracy: 0.2384\n",
      "Epoch 923/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 9.0171 - val_accuracy: 0.2267\n",
      "Epoch 924/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0160 - accuracy: 0.9985 - val_loss: 8.9242 - val_accuracy: 0.2384\n",
      "Epoch 925/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0152 - accuracy: 0.9985 - val_loss: 8.9848 - val_accuracy: 0.2209\n",
      "Epoch 926/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0169 - accuracy: 0.9956 - val_loss: 8.9402 - val_accuracy: 0.2267\n",
      "Epoch 927/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0166 - accuracy: 0.9956 - val_loss: 9.0984 - val_accuracy: 0.2267\n",
      "Epoch 928/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0245 - accuracy: 0.9956 - val_loss: 8.9991 - val_accuracy: 0.2209\n",
      "Epoch 929/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0195 - accuracy: 0.9971 - val_loss: 8.8966 - val_accuracy: 0.2267\n",
      "Epoch 930/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0180 - accuracy: 0.9956 - val_loss: 9.0167 - val_accuracy: 0.2209\n",
      "Epoch 931/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0159 - accuracy: 0.9985 - val_loss: 8.9476 - val_accuracy: 0.2442\n",
      "Epoch 932/1000\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 0.0152 - accuracy: 0.9971 - val_loss: 8.9622 - val_accuracy: 0.2151\n",
      "Epoch 933/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 9.0180 - val_accuracy: 0.2442\n",
      "Epoch 934/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0136 - accuracy: 0.9985 - val_loss: 8.9458 - val_accuracy: 0.2326\n",
      "Epoch 935/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0149 - accuracy: 0.9985 - val_loss: 9.0094 - val_accuracy: 0.2326\n",
      "Epoch 936/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0161 - accuracy: 0.9971 - val_loss: 9.1056 - val_accuracy: 0.2326\n",
      "Epoch 937/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0148 - accuracy: 0.9985 - val_loss: 9.0186 - val_accuracy: 0.2267\n",
      "Epoch 938/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0160 - accuracy: 0.9971 - val_loss: 9.0748 - val_accuracy: 0.2035\n",
      "Epoch 939/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0151 - accuracy: 0.9985 - val_loss: 9.0586 - val_accuracy: 0.2384\n",
      "Epoch 940/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0166 - accuracy: 0.9956 - val_loss: 9.0486 - val_accuracy: 0.2267\n",
      "Epoch 941/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0142 - accuracy: 0.9985 - val_loss: 9.0680 - val_accuracy: 0.2326\n",
      "Epoch 942/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0143 - accuracy: 0.9971 - val_loss: 9.0738 - val_accuracy: 0.2267\n",
      "Epoch 943/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0135 - accuracy: 0.9985 - val_loss: 9.0504 - val_accuracy: 0.2326\n",
      "Epoch 944/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0142 - accuracy: 0.9985 - val_loss: 9.1223 - val_accuracy: 0.2093\n",
      "Epoch 945/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0143 - accuracy: 0.9985 - val_loss: 9.0572 - val_accuracy: 0.2442\n",
      "Epoch 946/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0150 - accuracy: 0.9985 - val_loss: 9.0823 - val_accuracy: 0.2209\n",
      "Epoch 947/1000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0149 - accuracy: 0.9956 - val_loss: 9.1552 - val_accuracy: 0.2326\n",
      "Epoch 948/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 9.0874 - val_accuracy: 0.2267\n",
      "Epoch 949/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0154 - accuracy: 0.9971 - val_loss: 9.0956 - val_accuracy: 0.2209\n",
      "Epoch 950/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0131 - accuracy: 0.9985 - val_loss: 9.0181 - val_accuracy: 0.2442\n",
      "Epoch 951/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0141 - accuracy: 0.9985 - val_loss: 9.2035 - val_accuracy: 0.2151\n",
      "Epoch 952/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 9.1103 - val_accuracy: 0.2384\n",
      "Epoch 953/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 9.1390 - val_accuracy: 0.2151\n",
      "Epoch 954/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0156 - accuracy: 0.9971 - val_loss: 9.1317 - val_accuracy: 0.2384\n",
      "Epoch 955/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0142 - accuracy: 0.9985 - val_loss: 9.1585 - val_accuracy: 0.2209\n",
      "Epoch 956/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.0182 - accuracy: 0.9956 - val_loss: 9.1836 - val_accuracy: 0.2209\n",
      "Epoch 957/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0396 - accuracy: 0.9869 - val_loss: 9.2913 - val_accuracy: 0.2151\n",
      "Epoch 958/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0209 - accuracy: 0.9985 - val_loss: 9.1118 - val_accuracy: 0.2326\n",
      "Epoch 959/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0191 - accuracy: 0.9956 - val_loss: 9.1340 - val_accuracy: 0.2384\n",
      "Epoch 960/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0213 - accuracy: 0.9971 - val_loss: 9.3985 - val_accuracy: 0.2093\n",
      "Epoch 961/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0171 - accuracy: 0.9971 - val_loss: 9.1664 - val_accuracy: 0.2326\n",
      "Epoch 962/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0155 - accuracy: 0.9971 - val_loss: 9.3242 - val_accuracy: 0.2093\n",
      "Epoch 963/1000\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.0263 - accuracy: 0.9942 - val_loss: 9.1462 - val_accuracy: 0.2500\n",
      "Epoch 964/1000\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.0286 - accuracy: 0.9927 - val_loss: 9.3327 - val_accuracy: 0.2093\n",
      "Epoch 965/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0188 - accuracy: 0.9971 - val_loss: 9.1347 - val_accuracy: 0.2384\n",
      "Epoch 966/1000\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0140 - accuracy: 0.9985 - val_loss: 9.1114 - val_accuracy: 0.2326\n",
      "Epoch 967/1000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0145 - accuracy: 0.9956 - val_loss: 9.1717 - val_accuracy: 0.2326\n",
      "Epoch 968/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0134 - accuracy: 0.9985 - val_loss: 9.2264 - val_accuracy: 0.2326\n",
      "Epoch 969/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0138 - accuracy: 0.9971 - val_loss: 9.2283 - val_accuracy: 0.2093\n",
      "Epoch 970/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 9.2427 - val_accuracy: 0.2326\n",
      "Epoch 971/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.0136 - accuracy: 0.9985 - val_loss: 9.2355 - val_accuracy: 0.2209\n",
      "Epoch 972/1000\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 0.0140 - accuracy: 0.9971 - val_loss: 9.1983 - val_accuracy: 0.2267\n",
      "Epoch 973/1000\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.0133 - accuracy: 0.9985 - val_loss: 9.1629 - val_accuracy: 0.2209\n",
      "Epoch 974/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0131 - accuracy: 0.9985 - val_loss: 9.2597 - val_accuracy: 0.2209\n",
      "Epoch 975/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0142 - accuracy: 0.9971 - val_loss: 9.2651 - val_accuracy: 0.2209\n",
      "Epoch 976/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0123 - accuracy: 0.9985 - val_loss: 9.2977 - val_accuracy: 0.2209\n",
      "Epoch 977/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 9.3273 - val_accuracy: 0.2209\n",
      "Epoch 978/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0126 - accuracy: 0.9985 - val_loss: 9.2651 - val_accuracy: 0.2267\n",
      "Epoch 979/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 9.3027 - val_accuracy: 0.2209\n",
      "Epoch 980/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0160 - accuracy: 0.9956 - val_loss: 9.3926 - val_accuracy: 0.2209\n",
      "Epoch 981/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0155 - accuracy: 0.9971 - val_loss: 9.3579 - val_accuracy: 0.2209\n",
      "Epoch 982/1000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0185 - accuracy: 0.9971 - val_loss: 9.3286 - val_accuracy: 0.2209\n",
      "Epoch 983/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0148 - accuracy: 0.9985 - val_loss: 9.3899 - val_accuracy: 0.2209\n",
      "Epoch 984/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0176 - accuracy: 0.9956 - val_loss: 9.3822 - val_accuracy: 0.2326\n",
      "Epoch 985/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0194 - accuracy: 0.9971 - val_loss: 9.4151 - val_accuracy: 0.2267\n",
      "Epoch 986/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0198 - accuracy: 0.9956 - val_loss: 9.2246 - val_accuracy: 0.2442\n",
      "Epoch 987/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0182 - accuracy: 0.9985 - val_loss: 9.4405 - val_accuracy: 0.2035\n",
      "Epoch 988/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0172 - accuracy: 0.9971 - val_loss: 9.4961 - val_accuracy: 0.2267\n",
      "Epoch 989/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0152 - accuracy: 0.9971 - val_loss: 9.4497 - val_accuracy: 0.2151\n",
      "Epoch 990/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0121 - accuracy: 0.9985 - val_loss: 9.4137 - val_accuracy: 0.2209\n",
      "Epoch 991/1000\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 9.4341 - val_accuracy: 0.2267\n",
      "Epoch 992/1000\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.0194 - accuracy: 0.9971 - val_loss: 9.3751 - val_accuracy: 0.2267\n",
      "Epoch 993/1000\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0194 - accuracy: 0.9956 - val_loss: 9.3846 - val_accuracy: 0.2384\n",
      "Epoch 994/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0141 - accuracy: 0.9971 - val_loss: 9.4487 - val_accuracy: 0.2267\n",
      "Epoch 995/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0142 - accuracy: 0.9971 - val_loss: 9.3670 - val_accuracy: 0.2442\n",
      "Epoch 996/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0131 - accuracy: 0.9985 - val_loss: 9.4846 - val_accuracy: 0.2093\n",
      "Epoch 997/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0119 - accuracy: 0.9985 - val_loss: 9.3746 - val_accuracy: 0.2384\n",
      "Epoch 998/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0108 - accuracy: 0.9985 - val_loss: 9.4237 - val_accuracy: 0.2209\n",
      "Epoch 999/1000\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 9.3883 - val_accuracy: 0.2151\n",
      "Epoch 1000/1000\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 9.4370 - val_accuracy: 0.2209\n"
     ]
    }
   ],
   "source": [
    "best_cnn_history = best_cnn.fit(\n",
    "    X_train_scaled_cnn,\n",
    "    y_train_m_ohe,\n",
    "    epochs=1000,\n",
    "    validation_data=(X_val_scaled_cnn, y_val_m_ohe),\n",
    "    batch_size=80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAHFCAYAAADcytJ5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABgPElEQVR4nO3dd1iT19sH8G/YhC2KgCBuELeiVnHVvffeu866fq5alTqqtnV0arVV+1ar1jpq66h7771XFbUqdYPskfP+cUowECBAyIDv57pymZznPE/uPFBy90yFEEKAiIiIyARZGDsAIiIiovQwUSEiIiKTxUSFiIiITBYTFSIiIjJZTFSIiIjIZDFRISIiIpPFRIWIiIhMFhMVIiIiMllMVIiIiMhkMVGhPEWhUOj0OHjwYI7eJyQkBAqFIlvnHjx4UC8xmLr+/fujWLFi6R5//vw5bGxs0L1793TrREREQKlUom3btjq/7+rVq6FQKBAaGqpzLO9SKBQICQnR+f2SPXnyBCEhIbh48WKaYzn5fcmpYsWKoXXr1kZ5byJ9sDJ2AET6dOLECY3Xs2fPxoEDB7B//36N8sDAwBy9z+DBg9G8efNsnVu1alWcOHEixzGYu0KFCqFt27bYunUrXr9+DTc3tzR11q9fj5iYGAwaNChH7zV9+nSMGTMmR9fIzJMnT/DJJ5+gWLFiqFy5ssaxnPy+EOV3TFQoT3nvvfc0XhcqVAgWFhZpylOLjo6GUqnU+X18fHzg4+OTrRidnZ0zjSe/GDRoEDZt2oS1a9di1KhRaY6vXLkShQsXRqtWrXL0PiVLlszR+TmVk98XovyOXT+U7zRo0ADly5fH4cOHUbt2bSiVSgwcOBAAsGHDBjRt2hReXl6wt7dH2bJlMWXKFERFRWlcQ1tTfnIT+65du1C1alXY29sjICAAK1eu1Kinreunf//+cHR0xN27d9GyZUs4OjrC19cXEyZMQFxcnMb5//zzDzp37gwnJye4urqiV69eOHPmDBQKBVavXp3hZ3/+/DlGjBiBwMBAODo6wsPDAw0bNsSRI0c06oWGhkKhUOCLL77AokWLULx4cTg6OqJWrVo4efJkmuuuXr0a/v7+sLW1RdmyZfF///d/GcaRrFmzZvDx8cGqVavSHLtx4wZOnTqFvn37wsrKCnv27EG7du3g4+MDOzs7lCpVCh988AFevHiR6fto6/qJiIjAkCFD4O7uDkdHRzRv3hy3b99Oc+7du3cxYMAAlC5dGkqlEkWKFEGbNm1w5coVdZ2DBw+ievXqAIABAwaouxiTu5C0/b6oVCp89tlnCAgIgK2tLTw8PNC3b1/8888/GvWSf1/PnDmDunXrQqlUokSJEpg/fz5UKlWmn10XsbGxmDp1KooXLw4bGxsUKVIEI0eOxJs3bzTq7d+/Hw0aNIC7uzvs7e1RtGhRdOrUCdHR0eo6S5cuRaVKleDo6AgnJycEBATgo48+0kuclD+xRYXypadPn6J3796YNGkSPv30U1hYyJz9zp07aNmyJcaOHQsHBwfcvHkTCxYswOnTp9N0H2lz6dIlTJgwAVOmTEHhwoXxww8/YNCgQShVqhTq1auX4bkJCQlo27YtBg0ahAkTJuDw4cOYPXs2XFxcMGPGDABAVFQU3n//fbx69QoLFixAqVKlsGvXLnTr1k2nz/3q1SsAwMyZM+Hp6YnIyEhs2bIFDRo0wL59+9CgQQON+t9++y0CAgKwZMkSALILpWXLlrh//z5cXFwAyCRlwIABaNeuHRYuXIjw8HCEhIQgLi5OfV/TY2Fhgf79+2POnDm4dOkSKlWqpD6WnLwkJ5F///03atWqhcGDB8PFxQWhoaFYtGgR6tSpgytXrsDa2lqnewAAQgi0b98ex48fx4wZM1C9enUcO3YMLVq0SFP3yZMncHd3x/z581GoUCG8evUKP/30E2rWrIkLFy7A398fVatWxapVqzBgwAB8/PHH6hagjFpRhg8fjuXLl2PUqFFo3bo1QkNDMX36dBw8eBDnz59HwYIF1XXDwsLQq1cvTJgwATNnzsSWLVswdepUeHt7o2/fvjp/7ozuxb59+zB16lTUrVsXly9fxsyZM3HixAmcOHECtra2CA0NRatWrVC3bl2sXLkSrq6uePz4MXbt2oX4+HgolUqsX78eI0aMwOjRo/HFF1/AwsICd+/exfXr13MUI+VzgigP69evn3BwcNAoq1+/vgAg9u3bl+G5KpVKJCQkiEOHDgkA4tKlS+pjM2fOFKn/8/Hz8xN2dnbiwYMH6rKYmBhRoEAB8cEHH6jLDhw4IACIAwcOaMQJQPz6668a12zZsqXw9/dXv/72228FALFz506Neh988IEAIFatWpXhZ0otMTFRJCQkiEaNGokOHTqoy+/fvy8AiAoVKojExER1+enTpwUAsW7dOiGEEElJScLb21tUrVpVqFQqdb3Q0FBhbW0t/Pz8Mo3h3r17QqFQiA8//FBdlpCQIDw9PUVwcLDWc5J/Ng8ePBAAxO+//64+tmrVKgFA3L9/X13Wr18/jVh27twpAIgvv/xS47pz584VAMTMmTPTjTcxMVHEx8eL0qVLi3HjxqnLz5w5k+7PIPXvy40bNwQAMWLECI16p06dEgDERx99pC5L/n09deqURt3AwEDRrFmzdONM5ufnJ1q1apXu8V27dgkA4rPPPtMo37BhgwAgli9fLoQQ4rfffhMAxMWLF9O91qhRo4Srq2umMRFlBbt+KF9yc3NDw4YN05Tfu3cPPXv2hKenJywtLWFtbY369esDkF0RmalcuTKKFi2qfm1nZ4cyZcrgwYMHmZ6rUCjQpk0bjbKKFStqnHvo0CE4OTmlGZjZo0ePTK+fbNmyZahatSrs7OxgZWUFa2tr7Nu3T+vna9WqFSwtLTXiAaCO6datW3jy5Al69uyp0bXh5+eH2rVr6xRP8eLF8f7772Pt2rWIj48HAOzcuRNhYWHq1hQAePbsGYYNGwZfX1913H5+fgB0+9m868CBAwCAXr16aZT37NkzTd3ExER8+umnCAwMhI2NDaysrGBjY4M7d+5k+X1Tv3///v01ymvUqIGyZcti3759GuWenp6oUaOGRlnq343sSm4pTB1Lly5d4ODgoI6lcuXKsLGxwdChQ/HTTz/h3r17aa5Vo0YNvHnzBj169MDvv/+uU7ccUWaYqFC+5OXllaYsMjISdevWxalTpzBnzhwcPHgQZ86cwebNmwEAMTExmV7X3d09TZmtra1O5yqVStjZ2aU5NzY2Vv365cuXKFy4cJpztZVps2jRIgwfPhw1a9bEpk2bcPLkSZw5cwbNmzfXGmPqz2Nrawsg5V68fPkSgPwiTU1bWXoGDRqEly9fYtu2bQBkt4+joyO6du0KQI7naNq0KTZv3oxJkyZh3759OH36tHq8jC73910vX76ElZVVms+nLebx48dj+vTpaN++Pf744w+cOnUKZ86cQaVKlbL8vu++P6D999Db21t9PFlOfq90icXKygqFChXSKFcoFPD09FTHUrJkSezduxceHh4YOXIkSpYsiZIlS+LLL79Un9OnTx+sXLkSDx48QKdOneDh4YGaNWtiz549OY6T8i+OUaF8SduaFvv378eTJ09w8OBBdSsKgDQDCo3J3d0dp0+fTlMeFham0/lr1qxBgwYNsHTpUo3yt2/fZjue9N5f15gAoGPHjnBzc8PKlStRv359/Pnnn+jbty8cHR0BAFevXsWlS5ewevVq9OvXT33e3bt3sx13YmIiXr58qZEEaIt5zZo16Nu3Lz799FON8hcvXsDV1TXb7w/IsVKpx7E8efJEY3xKbku+F8+fP9dIVoQQCAsLUw8SBoC6deuibt26SEpKwtmzZ/H1119j7NixKFy4sHo9nAEDBmDAgAGIiorC4cOHMXPmTLRu3Rq3b99Wt4ARZQVbVIj+k5y8JLcaJPv++++NEY5W9evXx9u3b7Fz506N8vXr1+t0vkKhSPP5Ll++nGb9GV35+/vDy8sL69atgxBCXf7gwQMcP35c5+vY2dmhZ8+e2L17NxYsWICEhASNbh99/2zef/99AMDatWs1yn/55Zc0dbXds+3bt+Px48caZalbmzKS3O24Zs0ajfIzZ87gxo0baNSoUabX0Jfk90ody6ZNmxAVFaU1FktLS9SsWRPffvstAOD8+fNp6jg4OKBFixaYNm0a4uPjce3atVyInvIDtqgQ/ad27dpwc3PDsGHDMHPmTFhbW2Pt2rW4dOmSsUNT69evHxYvXozevXtjzpw5KFWqFHbu3Im//voLADKdZdO6dWvMnj0bM2fORP369XHr1i3MmjULxYsXR2JiYpbjsbCwwOzZszF48GB06NABQ4YMwZs3bxASEpKlrh9Adv98++23WLRoEQICAjTGuAQEBKBkyZKYMmUKhBAoUKAA/vjjj2x3KTRt2hT16tXDpEmTEBUVhaCgIBw7dgw///xzmrqtW7fG6tWrERAQgIoVK+LcuXP4/PPP07SElCxZEvb29li7di3Kli0LR0dHeHt7w9vbO801/f39MXToUHz99dewsLBAixYt1LN+fH19MW7cuGx9rvSEhYXht99+S1NerFgxNGnSBM2aNcPkyZMRERGB4OBg9ayfKlWqoE+fPgDk2Kb9+/ejVatWKFq0KGJjY9VT7xs3bgwAGDJkCOzt7REcHAwvLy+EhYVh3rx5cHFx0WiZIcoSIw/mJcpV6c36KVeunNb6x48fF7Vq1RJKpVIUKlRIDB48WJw/fz7NbI70Zv1om11Rv359Ub9+ffXr9Gb9pI4zvfd5+PCh6Nixo3B0dBROTk6iU6dOYseOHWlmv2gTFxcn/ve//4kiRYoIOzs7UbVqVbF169Y0s2KSZ/18/vnnaa4BLbNifvjhB1G6dGlhY2MjypQpI1auXJnmmrqoUqWK1hkoQghx/fp10aRJE+Hk5CTc3NxEly5dxMOHD9PEo8usHyGEePPmjRg4cKBwdXUVSqVSNGnSRNy8eTPN9V6/fi0GDRokPDw8hFKpFHXq1BFHjhxJ83MVQoh169aJgIAAYW1trXEdbT/HpKQksWDBAlGmTBlhbW0tChYsKHr37i0ePXqkUS+931dd76+fn58AoPXRr18/IYScnTZ58mTh5+cnrK2thZeXlxg+fLh4/fq1+jonTpwQHTp0EH5+fsLW1la4u7uL+vXri23btqnr/PTTT+L9998XhQsXFjY2NsLb21t07dpVXL58OdM4idKjEOKd9loiMkuffvopPv74Yzx8+JAroBJRnsKuHyIz88033wCQ3SEJCQnYv38/vvrqK/Tu3ZtJChHlOUxUiMyMUqnE4sWLERoairi4OBQtWhSTJ0/Gxx9/bOzQiIj0jl0/REREZLI4PZmIiIhMFhMVIiIiMllMVIiIiMhkmfVgWpVKhSdPnsDJyUnrkuhERERkeoQQePv2Lby9vTNdqNKsE5UnT57A19fX2GEQERFRNjx69CjTZRXMOlFxcnICID+os7OzkaMhIiIiXURERMDX11f9PZ4Rs05Ukrt7nJ2dmagQERGZGV2GbXAwLREREZksJipERERkspioEBERkcky6zEqukpKSkJCQoKxwyAzZ21tDUtLS2OHQUSUr+TpREUIgbCwMLx588bYoVAe4erqCk9PT67bQ0RkIHk6UUlOUjw8PKBUKvnlQtkmhEB0dDSePXsGAPDy8jJyRERE+UOeTVSSkpLUSYq7u7uxw6E8wN7eHgDw7NkzeHh4sBuIiMgA8uxg2uQxKUql0siRUF6S/PvEMU9ERIaRZxOVZOzuIX3i7xMRkWHl+USFiIiIzBcTlXyiQYMGGDt2rM71Q0NDoVAocPHixVyLCQAOHjwIhULBmVlERKRVnh1Ma64y61ro168fVq9eneXrbt68GdbW1jrX9/X1xdOnT1GwYMEsvxcREZG+MFExMU+fPlU/37BhA2bMmIFbt26py5JnniRLSEjQKQEpUKBAluKwtLSEp6dnls4hIiLzJASQmAhk4f9nDYZdPybG09NT/XBxcYFCoVC/jo2NhaurK3799Vc0aNAAdnZ2WLNmDV6+fIkePXrAx8cHSqUSFSpUwLp16zSum7rrp1ixYvj0008xcOBAODk5oWjRoli+fLn6eOqun+Qumn379iEoKAhKpRK1a9fWSKIAYM6cOfDw8ICTkxMGDx6MKVOmoHLlylm6B5s2bUK5cuVga2uLYsWKYeHChRrHv/vuO5QuXRp2dnYoXLgwOnfurD7222+/oUKFCrC3t4e7uzsaN26MqKioLL0/EVF+07Mn4OkJPHwoE5bRo4HNm40dlZS/EhUhgKgowz+E0OvHmDx5Mj788EPcuHEDzZo1Q2xsLKpVq4Y///wTV69exdChQ9GnTx+cOnUqw+ssXLgQQUFBuHDhAkaMGIHhw4fj5s2bGZ4zbdo0LFy4EGfPnoWVlRUGDhyoPrZ27VrMnTsXCxYswLlz51C0aFEsXbo0S5/t3Llz6Nq1K7p3744rV64gJCQE06dPV3d3nT17Fh9++CFmzZqFW7duYdeuXahXrx4A2RrVo0cPDBw4EDdu3MDBgwfRsWNHCD3ffyKivCQ6Gli/Hnj1CliyBBg5EvjmG6BTJyAszNjRARBmLDw8XAAQ4eHhaY7FxMSI69evi5iYmJTCyEghZNpg2EdkZLY+36pVq4SLi4v69f379wUAsWTJkkzPbdmypZgwYYL6df369cWYMWPUr/38/ETv3r3Vr1UqlfDw8BBLly7VeK8LFy4IIYQ4cOCAACD27t2rPmf79u0CgPoe16xZU4wcOVIjjuDgYFGpUqV040y+7uvXr4UQQvTs2VM0adJEo87EiRNFYGCgEEKITZs2CWdnZxEREZHmWufOnRMARGhoaLrvl1Naf6+IiMzY+vUZf4V16qT/98zo+zu1/NWikkcEBQVpvE5KSsLcuXNRsWJFuLu7w9HREbt378bDhw8zvE7FihXVz5O7mJKXiNflnORl5JPPuXXrFmrUqKFRP/XrzNy4cQPBwcEaZcHBwbhz5w6SkpLQpEkT+Pn5oUSJEujTpw/Wrl2L6OhoAEClSpXQqFEjVKhQAV26dMGKFSvw+vXrLL0/EZG5e/IEGDsWSNUzryE+HvjlF9ly0r17xtfz9dVreFmWvxIVpRKIjDT8Q8+r4zo4OGi8XrhwIRYvXoxJkyZh//79uHjxIpo1a4b4+PgMr5N6EK5CoYBKpdL5nOQZSu+ek3rWkshit4sQIsNrODk54fz581i3bh28vLwwY8YMVKpUCW/evIGlpSX27NmDnTt3IjAwEF9//TX8/f1x//79LMVARGTOevUCvvwSqFs3/TpLl8p6o0dnfr06dfQXW3bkr0RFoQAcHAz/yOXVTI8cOYJ27dqhd+/eqFSpEkqUKIE7d+7k6ntq4+/vj9OnT2uUnT17NkvXCAwMxNGjRzXKjh8/jjJlyqj31rGyskLjxo3x2Wef4fLlywgNDcX+/fsByEQpODgYn3zyCS5cuAAbGxts2bIlB5+KiMi8JP8Jff4cOHYMSP3/n4mJwMqVac/r0gUoVEizrEIFoG3b3IlTV5yenAeUKlUKmzZtwvHjx+Hm5oZFixYhLCwMZcuWNWgco0ePxpAhQxAUFITatWtjw4YNuHz5MkqUKKHzNSZMmIDq1atj9uzZ6NatG06cOIFvvvkG3333HQDgzz//xL1791CvXj24ublhx44dUKlU8Pf3x6lTp7Bv3z40bdoUHh4eOHXqFJ4/f27w+0BEZEyWljIZAWRryIoVwODBKcd79wYuX057nr09cOUK8PffwIEDQMeOgCn8+WSikgdMnz4d9+/fR7NmzaBUKjF06FC0b98e4eHhBo2jV69euHfvHv73v/8hNjYWXbt2Rf/+/dO0smSkatWq+PXXXzFjxgzMnj0bXl5emDVrFvr37w8AcHV1xebNmxESEoLY2FiULl0a69atQ7ly5XDjxg0cPnwYS5YsQUREBPz8/LBw4UK0aNEilz4xEZFpWbIEiIvTLFu6NCVRuXUL2LBB+7kqFVC4sHzUrp2rYWaJQmR1EIEJiYiIgIuLC8LDw+Hs7KxxLDY2Fvfv30fx4sVhZ2dnpAipSZMm8PT0xM8//2zsUPSCv1dEZMq0jTSoVk12ByUmAo0aAe/+v2N0dMowyp49gbVrDRNnRt/fqbFFhfQmOjoay5YtQ7NmzWBpaYl169Zh79692LNnj7FDIyLK8xIStJefOweUKAG8s/A5AGDCBNndkyyTuRRGw0SF9EahUGDHjh2YM2cO4uLi4O/vj02bNqFx48bGDo2IKM+6ehWYPRvo1y/9Ou8mKRUrAidOAKkbhVOtfGEymKiQ3tjb22Pv3r3GDoOIKF/p0AG4exf49Vfd6hcqpLlqxsWLwF9/6TZV2Rjy1/RkIiKiPOLOHcDLSyYpmalZM+W5t7fmsUqVgEmTABsb/canL0xUiIiITFDqqS5Xrsg1RAHgt9+AMmV034snKAg4exbo2hWYN0+/ceY2JipEREQm5vJloGBBYNEi+frAATm2JCgISEqSi7Np89+6mACAQ4dSnterJ2f/bNgAFCmSe3HnBiYqREREJmbqVLmb8YQJ8vX338t/b90CrNIZXTpypGxxmT9fdgvVqwecOQN89pncCdlccTAtERGRkZ08Kcea9O4NvH2rOdU4JkaWZeaDD+RMnsmTU8qCgkx3No+umKgQEREZWa1a8l9n55RkJZm2fW2rV5fls2YB9eunnJsXsesnj2rQoAHGjh2rfl2sWDEsWbIkw3MUCgW2bt2a4/fW13UyEhISgsqVK+fqexAR6dOrV0BUVMrrP/6QC7G9u0FgSEjGrSfOznI12dOngYMHZffOkCFA585A0aK5FblxsUXFxLRp0wYxMTFa1yM5ceIEateujXPnzqFq1apZuu6ZM2fg4OCgrzAByGRh69atuHjxokb506dP4ebmptf3IiIyZ+HhgLu7XMPk2TNZlrwr8aBBKfUuXMj4OqGhmqvJAsDy5XoL0ySxRcXEDBo0CPv378eDBw/SHFu5ciUqV66c5SQFAAoVKgSltvbDXODp6QlbW1uDvBcRkTlITkCePwfefx94+TJ718mP/w/IRMXEtG7dGh4eHli9erVGeXR0NDZs2IBBgwbh5cuX6NGjB3x8fKBUKlGhQgWsW7cuw+um7vq5c+cO6tWrBzs7OwQGBmrdj2fy5MkoU6YMlEolSpQogenTpyPhvxFeq1evxieffIJLly5BoVBAoVCoY07d9XPlyhU0bNgQ9vb2cHd3x9ChQxGZvBgAgP79+6N9+/b44osv4OXlBXd3d4wcOVL9XrpQqVSYNWsWfHx8YGtri8qVK2PXrl3q4/Hx8Rg1ahS8vLxgZ2eHYsWKYd47iwmEhISgaNGisLW1hbe3Nz788EOd35uICAAOHwamTdO+58748SnPDx7M+lom27YBt2/nKDyzla+6foSQfXuGplRq39FSGysrK/Tt2xerV6/GjBkzoPjvxI0bNyI+Ph69evVCdHQ0qlWrhsmTJ8PZ2Rnbt29Hnz59UKJECdR8d/nBdKhUKnTs2BEFCxbEyZMnERERoTGeJZmTkxNWr14Nb29vXLlyBUOGDIGTkxMmTZqEbt264erVq9i1a5e6m8rFxSXNNaKjo9G8eXO89957OHPmDJ49e4bBgwdj1KhRGsnYgQMH4OXlhQMHDuDu3bvo1q0bKleujCFDhuh037788kssXLgQ33//PapUqYKVK1eibdu2uHbtGkqXLo2vvvoK27Ztw6+//oqiRYvi0aNHePToEQDgt99+w+LFi7F+/XqUK1cOYWFhuHTpkk7vS0SULHlQq5cXMGpUSvmbN2m7dBYuzPhaR44A9+8DffsCc+YAbdroNVTzIsxYeHi4ACDCw8PTHIuJiRHXr18XMTEx6rLISCFkumLYR2Rk1j7XjRs3BACxf/9+dVm9evVEjx490j2nZcuWYsKECerX9evXF2PGjFG/9vPzE4sXLxZCCPHXX38JS0tL8ejRI/XxnTt3CgBiy5Yt6b7HZ599JqpVq6Z+PXPmTFGpUqU09d69zvLly4Wbm5uIfOcmbN++XVhYWIiwsDAhhBD9+vUTfn5+IjExUV2nS5cuolu3bunGkvq9vb29xdy5czXqVK9eXYwYMUIIIcTo0aNFw4YNhUqlSnOthQsXijJlyoj4+Ph03y+Ztt8rIiIhUv7mDx+eUvbwoRDly2ftO8PFRYjkP1X//JPyPC/J6Ps7NXb9mKCAgADUrl0bK/8bCv7333/jyJEjGDhwIAAgKSkJc+fORcWKFeHu7g5HR0fs3r0bDx8+1On6N27cQNGiReHj46Muq5U8N+4dv/32G+rUqQNPT084Ojpi+vTpOr/Hu+9VqVIljYG8wcHBUKlUuHXrlrqsXLlysHxnSUUvLy88Sx5xlomIiAg8efIEwcHBGuXBwcG4ceMGANm9dPHiRfj7++PDDz/E7t271fW6dOmCmJgYlChRAkOGDMGWLVuQmJiYpc9JRPlHYiIwYADw448pZSpV2npnzsiZOFevZu36166ltMIXKaJ7i3xela8SFaVSrtpn6Ed2xrAOGjQImzZtQkREBFatWgU/Pz80atQIALBw4UIsXrwYkyZNwv79+3Hx4kU0a9YM8fHxOl1bpN5AAlB3MSU7efIkunfvjhYtWuDPP//EhQsXMG3aNJ3f4933Sn1tbe9pbW2d5phK23/5GUj9Pu++d9WqVXH//n3Mnj0bMTEx6Nq1Kzp37gwA8PX1xa1bt/Dtt9/C3t4eI0aMQL169bI0RoaI8o8NG4DVq4HBg+Xr6Gjgnf/vAyDHqtSokfF1unVLef7jj3L34xUrzG+J+9yWr8aoKBSAnmfo5pquXbtizJgx+OWXX/DTTz9hyJAh6i/dI0eOoF27dujduzcAOebkzp07KFu2rE7XDgwMxMOHD/HkyRN4/7eN5okTJzTqHDt2DH5+fpg2bZq6LPVMJBsbGyQlJWX6Xj/99BOioqLUrSrHjh2DhYUFypQpo1O8mXF2doa3tzeOHj2KevXqqcuPHz+OGu/8pXB2dka3bt3QrVs3dO7cGc2bN8erV69QoEAB2Nvbo23btmjbti1GjhyJgIAAXLlyJVszrIjIvEVEyDEmPXoALVqkPf7uRoCXLwPffgs8fZpStnRp5u+xbRvg5yeTHgD4r8GctMhXiYo5cXR0RLdu3fDRRx8hPDwc/fv3Vx8rVaoUNm3ahOPHj8PNzQ2LFi1CWFiYzolK48aN4e/vj759+2LhwoWIiIjQSEiS3+Phw4dYv349qlevju3bt2PLli0adYoVK4b79+/j4sWL8PHxgZOTU5ppyb169cLMmTPRr18/hISE4Pnz5xg9ejT69OmDwoULZ+/maDFx4kTMnDkTJUuWROXKlbFq1SpcvHgRa9euBQAsXrwYXl5eqFy5MiwsLLBx40Z4enrC1dUVq1evRlJSEmrWrAmlUomff/4Z9vb28PPz01t8RGQ+QkKAn3+WDy0N0BqzeipVyvx6a9fKxd3Wr5evP/88ZXDs2rV5d6E2fclXXT/mZtCgQXj9+jUaN26Mou/8Jk+fPh1Vq1ZFs2bN0KBBA3h6eqJ9+/Y6X9fCwgJbtmxBXFwcatSogcGDB2Pu3Lkaddq1a4dx48Zh1KhRqFy5Mo4fP47p06dr1OnUqROaN2+O999/H4UKFdI6RVqpVOKvv/7Cq1evUL16dXTu3BmNGjXCN998k7WbkYkPP/wQEyZMwIQJE1ChQgXs2rUL27ZtQ+nSpQHIxG/BggUICgpC9erVERoaih07dsDCwgKurq5YsWIFgoODUbFiRezbtw9//PEH3N3d9RojEZmHmzdTnkdHA82aAXXrytk3SUlyjEpWNGsGrFsnV5zdt09zqnLPnkCdOvqJO69SCG0DFsxEREQEXFxcEB4eDudUmxzExsbi/v37KF68OOzs7IwUIeU1/L0iyvsaNQL275fPv/lGc6rxV18Be/fKrhtdqVQcEJtaRt/fqbHrh4iI6B2xsSnPU69/outakB4ecqn88uWZpOQUu36IiIje8W6i8m43UHr+62FWmzlTDrj95x8g1TwFyga2qBAREf1HCM1E5dixzM+JiUl53rq1HIwLcJqxvrBFhYiI8r0ffwQmTAAqVgSuX8/aufb2ckZPjRrA11/nTnz5WZ5vUTHjscJkgvj7RJR3xMQAvXoBtWoBkyZl/zoLF8rpxu8u4Eb6k2cTleSVTqOjo2Fvb2/kaCiviP5vV8vUK+kSkXkRApg1C9iyRT4yolSmv6HthQtA5cp6D4/ekWcTFUtLS7i6uqr3i1Eqleku5U6UGSEEoqOj8ezZM7i6umrsS0RE5mfxYmD+fN3q+vgAt29rlrVpIxdrc3LSf2ykKc8mKgDg6ekJADpvbkeUGVdXV/XvFRGZn7g4uQfbhAm6n1OlSkqi8u23QHw80KcPkxRDydOJikKhgJeXFzw8PLjBHOWYtbU1W1KIzEhUFDBxItCpU8oibh06yL18dFGqFLBjh+bePSNG5E6slL48nagks7S05BcMEVE+EB4OODvLRdYWLZJJhi6bBCb77DN57siRcjYPIJOdn38G/tsHlgwsXyQqRESU9x09KvfkGT1ads3cuqXbeYULA//+K59PnJj2uJeXPG7BBT2MIs/u9UNERHmfSiXHj/j7A8HBWVsJ1s9PJiHh4cCNG7LMfL8RzUtWvr+ZHxIRkdmaOxcoW1ZuFqjLqgEnTwJWVsD//icTnKNHARub3I+Tso9dP0REZJZiYoAZM+TzsWMzrtu0KbBpE+DoCLx8CTg4AMlDF+fNA1q2BIYPz9VwKZuYqBARkUl68UImF927Ay4usmzPHuCPP4AzZwBdJnPWrAns3i0H2CZL3dPQogXw9Kkcq0Kmh2NUiIjIJNWpk7Ip4N27MlkpVEi3c/v3l60sFSvKWTxkWjhGhYiIzFZSEvD6tebOxYMHy2RFVxMmAJUqMUnJC5ioEBGRyUhKkivBFiigWX7woOyi0UVwMFC+vN5DIyNhokJERAbz5g0weTJw+bL24716AVeupH9uaoMGyZ2P313Tc9mynEZJpoSJChERGcykSXL110qVtB/fsCFr17OyAhYsAN6+ld1F166xNSWvMWqikpiYiI8//hjFixeHvb09SpQogVmzZkGlUhkzLCIiyiVnz+r3elb/zV21twdcXYHAQP1en4zPqInKggULsGzZMnzzzTe4ceMGPvvsM3z++ef4+uuvjRkWERHlApUqJbEA5CJtXl7A//0f8OoV0LNn1q9ZooT+4iPTZNTpya1bt0bhwoXx448/qss6deoEpVKJn3/+OdPzOT2ZiMg8fPstMG0aEB2t2/on2uzaBTRvnvJ6yBCZ7NjZ6SdGMhyzmZ5cp04d7Nu3D7dv3wYAXLp0CUePHkXLli211o+Li0NERITGg4iITN+oUXJPnewmKX36AM2apbwuVQpYvpxJSn5g1ERl8uTJ6NGjBwICAmBtbY0qVapg7Nix6NGjh9b68+bNg4uLi/rh6+tr4IiJiMgYkhve//oLqF5drlhL+YNRE5UNGzZgzZo1+OWXX3D+/Hn89NNP+OKLL/DTTz9prT916lSEh4erH48ePTJwxERElBEh5JThCRNSyrZuzfp1bGyAJk1SXidvONi0KXD6tFxxlvIHo45R8fX1xZQpUzBy5Eh12Zw5c7BmzRrcvHkz0/M5RoWIyLTcvCl3MwbkWikFCwLe3unXb9dOTie+exfo0UMmIAUKpOzt8/HHQNGiwNChuR87GU5Wvr+NuilhdHQ0LCw0G3UsLS05PZmIyAypVHIcSrLMWj3c3YGNG1NaS7SZM0c/sZH5Mmqi0qZNG8ydOxdFixZFuXLlcOHCBSxatAgDBw40ZlhERJRFt24B770H+Pvrfs6jRxknKUSAkROVr7/+GtOnT8eIESPw7NkzeHt744MPPsCMGTOMGRYREWXR7NlyiftTp3Srv3atXKSNKDNGTVScnJywZMkSLFmyxJhhEBFRNsTGyq6eX36RiUdWREfnTkyU93CvHyIi0tmuXUDDhsC6dYCDA+DpCYwfn/E5FSqkLTPeNA4yN0xUiIhIJ3v3Ai1aAAcOyOXudZn3MH68nP2zcWNKWbVqcoYPkS6YqBARkVYJCUBionw+c6bmuia6Sm5t6dABGDFCdhGdPQs4OuovTsrbmKgQEVEa8fFyJ+Lq1WU3zaxZGdcfPhx4+FB2CSXz9gaKFJHPLS3lfj/Z2XiQ8jejDqYlIiLTdPasXIQNkKvBalO5slzQLSQECA6WZd27yy6hiRO5zD3pBxMVIiJKY+XKlOd796Y9fvRoSnKSWs+ebDkh/WGiQkSUz508CXTuDPj5Af36ycGy69dnfE7yMvlEuY2JChFRPjRlClCokNw8cNgw4PFj+Th+XLfzk/fiIcptTFSIiPKQ8HBg506gTRu5zgkgW0isrYE6deTrzp1Txo/s2gVcupTxNW1tgWvX5OaA3bsDPj5ycCyRIRh19+Sc4u7JRESamjcH/voLGDgQ+PFH4PVruRsxIGfyJCXpvnT93Lny3GrV5OwfIn0xm92TiYhIv/76S/67cqVMVJ4+TTn27BmweHHm19i1S05JbtYMUChyJ04iXTFRISLKo/79V3YBJfPxyfyczp1lgkJkKpioEBHlUUOGAPfu6VbX3V2uHDtqVO7GRJRVTFSIiPKoP/7Qve7y5UDHjrkXC1F2MVEhIsqnxo0DgoKA8+eBdu2MHQ2RdkxUiIjM2IMHcmfiESOA/ft1O8ffHzh1KmUtFK4iS6aMiQoRkZnZvl2uDKtUAsWKybKJE7XXdXQE1qwBvvpKTk8+f17O6uGCbWQumKgQEZmR/fuB1q11rx8eDlhYyK4dIYDYWN3XUSEyBRbGDoCIiNInhOymef1avj5wQPdzz5+XSUoyhYJJCpkftqgQEZmwXbuAli3l85Ilgb//1l5v506Z0AQEyGXuJ04EqlQxXJxEuYWJChGRiYmNBbZtA2xsgAEDUsrTS1JWrpRL5zdvLl9365b7MRIZChMVIiIjEgJITJSbBgohu2c++ki3pe6T1aiRe/ERGRvHqBARGVGXLkCRInKacUCAbEHJLEmpXBk4eRKoWFHuaFyqlEFCJTIK7p5MRGREyZv+lSsHXLum2zkvXsgl7xMS5G7Idna5Fx9RbuDuyUREJkylkrNxVKqUMl2TlBo1ZJICyO4ia2v9x0dkStj1Q0RkQLt3A66uwLp1wMyZup9TtizQty9w4kSuhkdkctj1Q0RkQPb2claPrvz8gNDQXAuHyCiy8v3NFhUiolw0caLchydZfHzm5/Tpk/L86VP9x0RkTpioEBHlkhcvgC++AJYuBcLCgHPnNMelvMvTM+X5/PnAtGny+Zdf5n6cRKaMiQoRUS65cSPl+ZIlQFBQ+nVDQ1MSGW9vYNYs4PZt4IMPcjtKItPGRIWIKJe8m6gsWJD2eMWKcnfjjz8GbG2BqlVTpitbWAClS6e8JsqvOD2ZiEjPjh+Xy9r/+GPG9dq0AUJCACv+JSZKF//zICLSkxEjgEOHgOvXM69bvbqszySFKGP8T4SISEfnzwMODoC/v2b5o0dyKfvMzJ8vu3QeP5bL5LNbhyhzTFSIiHTw/DlQrZp8rlKlJBnbtgHt2ul2jcmTcyc2oryMiQoRUSZevAB69Up53bo1cOWKbEl5l7W13H8HkInMsGHy3I0bgX79DBcvUV7CRIWIKBNjxgB79qS83rFDe71Ro4ChQ+W4k+QdjSMigM6dgVatcj9OoryIiQoREYDXr+UuxPb2aY/t3avbNRISgIAAzTJnZ6Br15zHR5RfcR0VIsr33rwBChQASpTQftzWVrfruLrqKyIiSsYWFSLK986elf+GhQFCAP/+C3z+uRyLsmVL2rEoyTZvBqKi5N48RYoA48YZLmai/IKJChHRO1atAkaOlDscL1qUfr3p04EOHeTz3r0NExtRfsREhYjytehouaZJskGDMj/n9Gm5YBsR5T6OUSGifEUIzddjx6Y/iyfZ8uUpz21smKQQGRITFSLKN06cAAoVkt07SUlyR+MVKzI/b/DglOfx8bkWHhFpwa4fIso3evUCXr4EBg4ELC0zH/zavj3Qv7/mUveFCuVmhESUGltUiChfEAK4fz/l9alTmZ/z+ecpy+MfPQrUqgX8+WfuxEdE2rFFhYjyhS1bNF9/9532euXKATNnAs+epawuCwDBwcDx47kXHxFpx0SFiPK0kyfl+iidOulWf/NmoEyZ3I2JiHTHRIWI8pzoaCA0VC7CVqtWxnX37ZOzePbvB5o1k8voE5Hp4BgVIspTdu0CHBxkF84PP2Rct3ZtoGFDwMlJjkVhkkJkehRCpF5VwHxERETAxcUF4eHhcHZ2NnY4RGREUVHyUbiw7ueoVJozeojIMLLy/c0WFSIyWyoVcPGiXCm2SpWsJSlnzjBJITIHTFSIyOzExABffAFUqyYTlJo1gTt3Mj5n3ryU540aAUFBuRsjEekHExUiMjs9ewITJ8rWlIx89ZVMak6dAiZPTikfMCBXwyMiPeIYFSIyCydPygXbGjcGPDx0Oyf1XzdPT+Dff4FHjwAfH/3HSES6ycr3N6cnE5HJe/s282nGqS1alLbs7l0gPFxOWyYi88BEhYhM0ps3gKurbAF58kS3c1xc5JTkzp21H3d0lA8iMh8co0JEJmfFCsDNTY4r8fYGqlbV7bw3b9JPUojIPDFRISKTIgQwdKh8/tlncgpyRipWBCws0u7lQ0R5g9ETlcePH6N3795wd3eHUqlE5cqVce7cOWOHRURG8uOPutdt0QI4fx5ISgLat8+1kIjIiIyaqLx+/RrBwcGwtrbGzp07cf36dSxcuBCurq7GDIuIDGTZMqBfP+DFC9kiEh0NDBmSfv327YF//gF27pS7G+/YAVhaGixcIjICo05PnjJlCo4dO4YjR45k63xOTyYyb1lZGdbLS/dBtURk2sxmCf1t27YhKCgIXbp0gYeHB6pUqYIVK1YYMyQiMpC4ON3qWVkBPXrIXY6JKP8x6vTke/fuYenSpRg/fjw++ugjnD59Gh9++CFsbW3Rt2/fNPXj4uIQ985ft4iICEOGS0R69PRp5nWePAEKFZLJChHlT0b9z1+lUiEoKAiffvopAKBKlSq4du0ali5dqjVRmTdvHj755BNDh0lEerRokZzJc/hwxvWCgmR3DxHlb0bt+vHy8kJgYKBGWdmyZfHw4UOt9adOnYrw8HD149GjR4YIk4hyaM0aoEkT4OZNYMIEuU/PH39or1u3LtCnj1wyn4jIqC0qwcHBuHXrlkbZ7du34efnp7W+ra0tbG1tDREaEemJEDLxAICyZdOvN3Om3A25TRvDxEVE5sGoicq4ceNQu3ZtfPrpp+jatStOnz6N5cuXY/ny5cYMi4j06Ngx7eXffw84O8uBsoMGASEhBg2LiMyE0XdP/vPPPzF16lTcuXMHxYsXx/jx4zEko4UU3sHpyUSm7dIloHLltOXr1gHdu8vWltu3gZIlOWCWKD/Jyve30ROVnGCiQmS6NmyQyUhqW7cC7doZPBwiMiFZ+f7m/8MQkd7cuCFXme3eXXNxto8+Aq5fBzp1YpJCRFnDRIWIsi0qSg5+vX4dqFkT2LYtbZ0CBYBJkwAXF8PHR0Tmz+ibEhKR+dq4EThwAPj3X+1JSu/ectNAJilElF1sUSGibDl8GBgwIP3js2cDU6dy00AiyhkmKkSkk4QEmXQoFMDu3UD//unXffAAKFrUYKERUR7GRIWIMhUXB5QrJ9c9cXYGDh1Kv+7ChUxSiEh/mKgQUaZu3gT+/lv7scBAYO1aubJs48bA8OGGjY2I8jYmKkSk9tNPwJEjwLJlcgG2pCTZ3XPtWvrnbN8OFCsG/P67wcIkonyEiQoRqSWPO2nSBAgOBipWBHx9gcuXNetdvQq0bi1XlvXxMXiYRJSPMFEhIgAy6Uj22Wdyxs7r1/KRWtmyspVFCC59T0S5i39iiAgA8OZNyvPz54EuXbTXGzAAsLAAlEqDhEVE+RwTFaJ86u5d4MoVmXDUqye7c9LTsyewdKmc8UNEZEhMVIjyqdKlU55XqCCTFm2+/hoYNcowMRERpcYl9InyiagouWjbyZPy33dpS1JWrAAaNZIbCRIRGQtbVIjygQsXgBo1gMRE+bp8+Yzr16wJDB4sH0RExsQWFaJ8YOTIlCQFSDse5ZtvgPh4+di2Ddixw7DxERGlhy0qRHnchAnAiRMZ1xk5MuV5mza5Gw8RUVYwUSHKo+7eBbZsARYtyrje7NmGiYeIKDuYqBDlIb/9Bri7yzEpEyZkXNfXVyYzNjaGiY2IKDuYqBDlAZcvA/Xray7a9q6ZM4FPPkl5ffUq4O3NJIWITB8TFSIzp1IBffqkn6QAwMSJclG3W7e4uzERmRcmKkRmaM0aIDwcOHsWWL06/XrLlwNlygAODkDDhvJBRGROmKgQmZm//5YtKJlp0gQYMiT34yEiyk1cR4XIzPz+e+Z1rK2Bn37K/ViIiHIbExUiE3f0qNyXZ/t2+fr27Yzr//uvXLjNyyv3YyMiym3s+iEyca1by/EorVsD770n9+rRpkQJ4NIlwNHRsPEREeWmbCUqjx49gkKhgI+PDwDg9OnT+OWXXxAYGIihQ4fqNUCi/C48POV5ekmKEIaJhYjI0LLV9dOzZ08cOHAAABAWFoYmTZrg9OnT+OijjzBr1iy9BkhEaVWsKLt2evYEjh0zdjRERLknWy0qV69eRY0aNQAAv/76K8qXL49jx45h9+7dGDZsGGbMmKHXIInyiwEDgHPngDp1gCdPgIsX09Z5773M9+4hIsorspWoJCQkwNbWFgCwd+9etG3bFgAQEBCAp0+f6i86onzk5MmUNVGuXNFex8oKWLbMYCERERldtrp+ypUrh2XLluHIkSPYs2cPmjdvDgB48uQJ3N3d9RogUV7Wty/QqJFcLbZWrfTrubrKfXkSEoBKlQwWHhGR0WWrRWXBggXo0KEDPv/8c/Tr1w+V/vvLuW3bNnWXEBFlLCYG+Pln+Xz/fu11mjaViUyHDkDJkoaLjYjIVGQrUWnQoAFevHiBiIgIuLm5qcuHDh0KpVKpt+CI8rInTzI+/uABUKQIYGlpmHiIiExRthKVmJgYCCHUScqDBw+wZcsWlC1bFs2aNdNrgER5TWQk8PYtEBqa9ti4cbL1BACKFjVoWEREJilbiUq7du3QsWNHDBs2DG/evEHNmjVhbW2NFy9eYNGiRRjO7VmJNAgBKBRyh+N3GiHT+PxztqAQEb0rW4Npz58/j7p16wIAfvvtNxQuXBgPHjzA//3f/+Grr77Sa4BE5m72bMDCAqhbN+Mk5eFDJilERKllK1GJjo6Gk5MTAGD37t3o2LEjLCws8N577+HBgwd6DZDI3CUvK3T0aNpjyd0748YBvr6Gi4mIyFxkK1EpVaoUtm7dikePHuGvv/5C06ZNAQDPnj2Ds7OzXgMkMifR0UCbNsA33wDt28vunvQsXw78/Tdw6hSwYIHBQiQiMivZGqMyY8YM9OzZE+PGjUPDhg1R678FIHbv3o0qVaroNUAic/LHH8Cff8pHerp3B7p0kYNmFQqAM/qJiNKXrUSlc+fOqFOnDp4+fapeQwUAGjVqhA7JUxaI8qHnzzOvs25d7sdBRJRXZCtRAQBPT094enrin3/+gUKhQJEiRbjYG+U7Fy7IVhEhgNhYYPTo9OtOnAgMHGi42IiI8oJsJSoqlQpz5szBwoULERkZCQBwcnLChAkTMG3aNFhYZGvoC5FZuXQJqFpV+7E+fYD794EKFYDdu4HJk4EhQwwbHxFRXpCtRGXatGn48ccfMX/+fAQHB0MIgWPHjiEkJASxsbGYO3euvuMkMikHDgANG6Z/fORIoGZNw8VDRJRXKYQQIqsneXt7Y9myZepdk5P9/vvvGDFiBB4/fqy3ADMSEREBFxcXhIeHc7YRGcyVK0DFitqPuboCK1emrC5LRERpZeX7O1stKq9evUJAQECa8oCAALx69So7lyQyCzEx6ScpAHD4sOzuISIi/cjWYJJKlSrhm2++SVP+zTffoGJGf8WJzNT168ChQ8Bvv2k/7uMDbNjAJIWISN+y1aLy2WefoVWrVti7dy9q1aoFhUKB48eP49GjR9ixY4e+YyQyuJs3gd9/B8aPl/vvTJumebxBA+DgwZTXp04B3t6GjJCIKH/IVotK/fr1cfv2bXTo0AFv3rzBq1ev0LFjR1y7dg2rVq3Sd4xEBle1KjBlCmBjkzZJAYBRo+Sy9wCwdy+TFCKi3JKtwbTpuXTpEqpWrYqkpCR9XTJDHExLuSWjpe8bNgS2bwdsbYEXL4BChQwXFxFRXpDrg2mJ8qKwMGDEiIx3MK5SBdi3L+U1kxQiotzFRIXoP+PGAVu2pC2fOFGOVUlIAP7bNJyIiAyEiQrla0IAO3cCy5bJDQXfNXgwYG8PzJkjx6oQEZHhZSlR6dixY4bH37x5k5NYiAzq/HngvfdkS0lqX34JfPih4WMiIiJNWUpUXFxcMj3et2/fHAVElJtevwY++ki2oGSkd2/DxENERBnLUqLCqcdk7nr2BHbtSv/4s2dyF+QCBQwXExERpY9jVCjfePFCe5LSqBFw7RrQpAln8RARmRomKpRnxcbKJMTJCfjhB7ma7LuaNAF8fYElSwBHx4zXTiEiIuNgokJ50sqVwJo1wPHj8rWvr+bxvn2Bn34yfFxERJQ1TFQoz0hMlIu2ubsDgwZprzNhAtCypVxdloiITB8TFTJ7168DT58C+/cDn36asgdPal9/LffoISIi85GtTQlzw7x586BQKDB27Fhjh0JmRAigXDmgcWOZpADA4sWadZYskfWYpBARmR+TaFE5c+YMli9fjooVKxo7FDIjf/8NvH2bcZ3evYHhww0TDxER6Z/RW1QiIyPRq1cvrFixAm5ubsYOh0xcZCQwZAhQvTpQqpTcJDC1WrXkIFohgJ9/5vL3RETmzOiJysiRI9GqVSs0btzY2KGQGZgyRU41PntW+/GvvpJJSq1aho2LiIhyh1G7ftavX4/z58/jzJkzOtWPi4tDXFyc+nVERERuhUYmRKUCLCyA334Dvv02/XoFCwKjRxsuLiIiyn1Ga1F59OgRxowZgzVr1sDOzk6nc+bNmwcXFxf1wzf14hiUp+zdKxdhs7SU/3bpor3ezp3AmDHA4cOGjY+IiHKfQgghjPHGW7duRYcOHWBpaakuS0pKgkKhgIWFBeLi4jSOAdpbVHx9fREeHg5nZ2eDxU65Z8cOoEcPoGxZ4NSp9Ot99BEwZ45cO8Xa2nDxERFRzkVERMDFxUWn72+jdf00atQIV65c0SgbMGAAAgICMHny5DRJCgDY2trC1tbWUCGSAUVGAiEhwKpVQESE9iSlTh25oFuzZsDs2bKVhUkKEVHeZrRExcnJCeXLl9coc3BwgLu7e5pyyvvmzwcWLsy4zuefA++9Z5h4iIjINBh91g/lT7dvA3fvAmPHAitWyGnEqXl6piziBsiF3YiIKH8xiQXfkh08eNDYIZABHDkC1KuXcZ3du4Hy5QEvL6B+fTnrx8nJMPEREZHpMKlEhfKHd1tJtBkyBGjSJOV17dq5Gw8REZkudv2QQS1aBOzalXGdZcsMEwsREZk+JiqU6+LiUtY6mTBB89jcuXKGT7Vq8vW1a7Kbh4iICGDXD+nZnTvAhQtycbbYWKBtW7lwW2rffSfXS3F1la8PHpRTj0uVMmS0RERk6piokF6VKwckJACvXmnftbhXL9m14+ioWe7oyCSFiIjSYqJCevPLLzJJAdImKWXLAhs3cooxERFlDRMVyrKkJDmWpHx54NYt4P/+Ty7Ylp6rV5mgEBFR9nDYImXJq1fAzJlApUpypdjAwPSTlOrVgcePmaQQEVH2sUWFMjR7NvDyJbB4MXDsGFC3bsqxKVPSP+/SJaBixdyPj4iI8jYmKpSu2Fhgxgz5fMMGOSsnMwqFXNCNSQoREekDu34oXU+epDzPLEmZPx9QqYC3bzNuaSEiIsoKtqiQVvfuASVLZlynb19g5UrA0jKlzMEhd+MiIqL8hYkKaZgwARBCzurJzKxZmkkKERGRvjFRIQAyOdm0Se7Fk55z54AKFYACBQBra8Db23DxERFR/sREJR+Li5MzeRITZTfOv/+mX7dJE6BqVfn86VOZ2FhbGyZOIiLKv5io5GOffALMm5f+8fv3gefP5aaBPXumlKde/p6IiCi3MFHJh4QAOnUCtmxJv86+fUCxYvJRvbqhIiMiItLERCWfePECWLECKF0a6NZNTiVOz7p1QMOGhouNiIgoPUxU8oHdu4FmzdI/fuYMEBQknwshF20jIiIyBUxU8qCvvwYuXwYGDQJu3gQGDEhb59dfgbZt5QaDSmVKOZMUIiIyJUxU8pANG4AvvwROnJCvf/hB83hQENChA9CmjZxmTEREZOqYqJg5lQqoXVvOzMlIo0ayC8iCmyYQEZEZ4deWmYqOBjZulGubZJSkrFkjV5ndu5dJChERmR+2qJiBxETg4UO55snatUCDBsDRo3IDQG0WLpTHQ0K4izEREZk3hRBCGDuI7IqIiICLiwvCw8Ph7Oxs7HByxenTQM2autcvXRq4fTv34iEiIsqprHx/s0XFxGW0nomXF/DPP0BMDPDXX3K3Yy8vw8VGRESU25iomKjYWGDaNCAqSvvxTz4B2reX404cHICOHQ0aHhERkUEwUTEhN24A8fGAvT3g76+9Tt26ckn76dO55gkREeV9TFQM6O+/5YZ+hQunPfb6NVCrFhAerv1cCwtg8mTg009zN0YiIiJTwgmrBhISApQqBRQvLl/fuydn5CgU8lGggPYkZfduIDRUriDLJIWIiPIbtqgYwMuXckwJIAe+PnsGjBsHXLmivf6QIUC7dnKNFA6OJSKi/IyJigF8/73ma21dPx98ACxeLMenEBERkcRERc+io+WibG3ayITku+/k7J3UbGyAPn2Af/8FfHyAL75gkkJERJQaExU9evECKFQo83oHD8r9eaytcz0kIiIis8bBtHoSEZFxktKtm9xvJy4OqF+fSQoREZEu2KKSTVFRwOrVwNKlcuqwtoGxU6fKXYudnYGyZeXUZCIiItIdE5Us2L8f2LIFOHECOHcu47qFCgGzZgFWvMNERETZxq/RTAgBfPmlXAOlTZv061WqJKcUOzgAKpVcnI0rxxIREeUME5VMfPQRMH9++sf795fTj21sDBYSERFRvsFEJR0xMUDjxsDx42mPtWgB/PEHYGlp+LiIiIjyE8760eLQIUCp1J6k9O8P7NjBJIWIiMgQ2KKixcOHKc+trOTU4oULgfv35ewdIiIiMgwmKlr06QPExwOurrL7x8VFlmtb+p6IiIhyDxOVdAwaZOwIiIiIiGNUiIiIyGQxUSEiIiKTxUSFiIiITBYTFSIiIjJZTFSIiIjIZDFRISIiIpPFRIWIiIhMFhMVIiIiMllMVIiIiMhkMVEhIiIik8VEhYiIiEwWExUiIiIyWUxUiIiIyGQxUSEiIiKTxUSFiIiITBYTFSIiIjJZRk1U5s2bh+rVq8PJyQkeHh5o3749bt26ZcyQiIiIyIQYNVE5dOgQRo4ciZMnT2LPnj1ITExE06ZNERUVZcywiIiIyEQohBDC2EEke/78OTw8PHDo0CHUq1cv0/oRERFwcXFBeHg4nJ2dDRAhERER5VRWvr+tDBSTTsLDwwEABQoU0Ho8Li4OcXFx6tcREREGiYuIiIiMw2QG0wohMH78eNSpUwfly5fXWmfevHlwcXFRP3x9fQ0cJRERERmSyXT9jBw5Etu3b8fRo0fh4+OjtY62FhVfX192/RAREZkRs+v6GT16NLZt24bDhw+nm6QAgK2tLWxtbQ0YGRERERmTURMVIQRGjx6NLVu24ODBgyhevLgxwyEiIiITY9REZeTIkfjll1/w+++/w8nJCWFhYQAAFxcX2NvbGzM0IiIiMgFGHaOiUCi0lq9atQr9+/fP9HxOTyYiIjI/ZjNGxUTG8RIREZGJMpnpyURERESpMVEhIiIik8VEhYiIiEwWExUiIiIyWUxUiIiIyGQxUSEiIiKTxUSFiIiITBYTFSIiIjJZTFSIiIjIZDFRISIiIpPFRIWIiIhMFhMVIiIiMllMVIiIiMhkMVEhIiIik8VEhYiIiEwWExUiIiIyWUxUiIiIyGQxUSEiIiKTxUSFiIiITBYTFSIiIjJZTFSIiIjIZDFRISIiIpPFRIWIiIhMFhMVIiIiMllMVIiIiMhkMVEhIiIik8VEhYiIiEwWExUiIiIyWUxUiIiIyGQxUdHm5Uvg1Cng3j1ACGNHQ0RElG8xUdFm927gvfeAkiWBYsWAQYOAM2cAlcrYkREREeUrTFS0cXAA/PwAa2vg4UNg5UqgRg3AzQ1wdQUCA4H//Q84csTYkRIREeVpCiHMt28jIiICLi4uCA8Ph7Ozs/7f4O1bYNcu4Ndf5b+RkZrHrayAAQOA4sWBtm2BcuX0HwMREVEek5XvbyYquoqLA86dA16/BvbsAb78Mm0dpRIIDgZGjABq1QIKF87dmIiIiMwQExVDiI8Htm6VY1cuXQL27k078NbfH+jbF2jcWLa62NoCho6TiIjIxDBRMYawMDmeZe1aYM0a4NWrtHWcnICgIDlAt107oEULwMbG4KESEREZExMVU/DLL3KK8/HjsstI2222sZFjW9q0AXr0kIN3iYiI8jgmKqYmPh4IDwdWrABmzZLjXVJzc5MtLW3aANWqATVrcowLERHlSUxUzEFCArBhg2x52bkz7XFLS7luixBAly5A165Ax46yhaZqVTlwl4iIyAwxUTE3MTEyWVm/Hnj6FLh8GYiISFvPzg6IjQXs7YGzZ4GyZeXYmMKFAQsuiUNEROaBiYq5EwK4c0eukHvjhiz77ru09QoUkIN2a9UCSpQAKleWa7k0a8bEhYiITBYTlbzo0iXg6lXg1i3Z/bNvX/p1W7aUY1xatpTjXRQKw8VJRESUCSYq+cHdu8Dz53Jg7vr1wNGjgIcHcOiQ5p5ExYrJpMXLCyhYEGjQAKhYUU6VJiIiMgImKvnZ7t3A//0fcO0acP26nHGkTY0agIsLUL68nCJdt64cwEtERJTLmKiQFB0N/P67XIju0iXZCnPmjPa6np6ypaVGDSAgQCYur1/LsS4VKhg2biIiytOYqFD6VCogNBQ4fBj4/nsgKSn95CVZoUJyanRgoExmihUDihThAnVERJQtTFQoa169kgN1r18HTpyQiUvybKOMdO0KXLwok5emTYF69QB3dzkWhoiIKB1MVCjnnj2Tq+k+fCgH6O7bJ9d3iYzM/NyiReUKu8lrvtSqBbRuzQ0ZiYgIABMVyi1CyMerV3Jdl3PngAMH5J5Fb97IbqT0ODrKriN/f7kh4+PHchxMlSoGC5+IiEwDExUyvLdv5XTp27eBX3+Va73cuiVX0/XyAu7f136enZ38NzZW/uvlJce+lCsH/Pgj8OiRfO7gYJjPQUREuY6JCpkWIeRO0seOAbt2yXEtL19q31FaGz8/OR7m1CnZAqNSycRo1izAx4cL2hERmRkmKmQeHjwAbt6UXUYXLshF6548Ae7d020sDAD4+gLdugFlysjZSYGBcjsBKyvZwlOwIBMZIiITw0SFzN/r1zJ5KVgQ+OILOS7m2jXZmvLwYcbn2trKRCUqSk6jfv99uZVAhQrAe+/JbiQhgMRETrEmIjICJiqUtwkhx7QkJQErVsjp1P/+K2cp3bghF7rLiJOT7DoCZLeSm5scG9OsGVCqlCyLi5MtMhUrAt7e8pqHDwMNG8rBw0RElG1MVCj/UqnkwN3Ll+XUaAsLOS4meXDvmzdZv2aJErI7CpCJTIMGgFIpW2NatQJKlpTr0Lz3ntzROjYWOH1adkd5eurz0xER5QlMVIi0UalkK0lEhOz2sbeXs4r+/lsmN9euyW0G/v5b1s2sZSYzSiUwapQcN2NlBRQuDJQtKxfFCw2VZZ6esp6FhV4+IhGROWCiQqQPQsiE4sEDmbwolTKRiYiQY2iePZN7KD16pPsMJm3c3WUrzeXLsvvK01OOp/H3l+/TqpXcf8nCQh7j4GAiMnNMVIgMSaWSY1ouXJCzlXx9ZXfQ5s3An38CL17IFpuEBJn0qFTZfy9nZ6BSJTk7SqmUqwBHR8sxNoGBgIeHXJsm+eHrKx8nT8rza9YE7tyRY2/i4+X1uGs2ERkYExUiUxURIQfyKpWyq8nNTXZHHT8uy/z9gS+/lM9DQ4GwMJn8REbKlpScJDnvsrKS3V/e3nJcTfny8vmRI4CLC9Cjh5wxtXmzXPema1egalWZJDk6ysTM0ZGtO0SULUxUiPKSpCT5UCjkonf//CPH1yQlySTHwgK4ckV2RcXGAjEx8t/oaLlOTWSknLLt5CRbd/TFxUWuXZOQIK+flCRnRDk4yGMVK8oWHhcX4OxZYOVKeV7PnnL2VPnywNOnctG+AgXkcy8vOXg5IUFOSX/2TCZnRYrIpI6tP0R5glklKt999x0+//xzPH36FOXKlcOSJUtQt25dnc5lokKUCSFkwmJvL58fOCCTC0dHOR5m7145e+nJE5n0FCok16l58iRl0HF8vEw2Hj2SiUNus7CQSVnqvaPs7WV8BQrIVql//5XJl4ODnGFlZSXH+zg4yLp2doCrq7yOhQVw8KD83GXLyiSqQAGZ+MTGyveztJTX8PGRM8V+/hno109uqPnihTy3QAF5L5KSZLdeaCjQqZM8J/l+JyXJ6xBRuswmUdmwYQP69OmD7777DsHBwfj+++/xww8/4Pr16yhatGim5zNRITKwyEjZ2qFSyfE20dEy0YmPly05UVFyAPDbt3I6eEyMTIgUCjmGp2ZNmUzcvCkfcXGybmKibJWJidEcmFyggKwTFWW8z5yahUXaLjh/f/kZ792T98LJSbYAeXjIhOr5c7kJp6+vnCL/+rU8XrasbDm6elUmQN7eMkFUKuV7eHvLa0VFAcWLywQsOlpe4/FjORaqQAE51ujcOXmvS5eWiVt4uLyfbm4pLWrOzjL+xMSUx4sXsiVLqZQ/22LF5M/UUDPRjhyR3YsffMA1ivQhIUEm305Oxo4kQ2aTqNSsWRNVq1bF0qVL1WVly5ZF+/btMW/evEzPZ6JClMdERckv2Kgo2QVkaSlbKGJi5Bf+q1dyn6jXr+WXdsGC8tiDB7LrKCpKJgoKhfxj/eZNSjeYnZ1sFbl3T479UankuQkJ8gvSxUXWTR6oHB4ur//qlayrLUHJ6xQKzcTRwkK2Hv37r7xH7u4pP6OkJHl/XryQ5yiV8mdkb5+SgFhYyK7L5MTJ1VWuOQTIZKl6dXm9xESZwNrZydYpW1t5zcePZbIVESF/D0qXlq18Fhby98XBQc7QK1lSXvPpU3muh4d8nZgo40xO0goVkj/3+HiZLJYqJcvs7OSMPldXmbjdvy/j/ucfGUO1ajL+kiVlLMldnLa2KYlmQoKM0dtbJn5JSbIL9MULOdZLqZSJa3y8/IxCyKTc0zOli/PmTfn7FxgolzcA5O+4jY08H5DnJ2/u+vYt8PHHcvzbgAEylqQkoHt3mdAC8to2NjKm5CQ6Lk7ej4IF5b9v3shWR2dnWc/ZOeUe6olZJCrx8fFQKpXYuHEjOnTooC4fM2YMLl68iEOHDqU5Jy4uDnFxcerXERER8PX1ZaJCRLlHpZIPS0v5RRAVJf94Jyc+z5/LLyBAfskVKSJbnl6+lF9+gPwyCQ+XX0YuLvKLNbm7zc4u5Qvh+XP5RfXqlSx/8UJ+idjaytlaQEpry+nT8ks0uYvMz0++vnNHTqNXKmWCZmMj6ycP5BYipZsLSOnis7GR75W6y42oe3dg3Tq9XjIriYrROlJfvHiBpKQkFE7OEv9TuHBhhIWFaT1n3rx5+OSTTwwRHhGRZGGR0g3i7Cwf7ypUCGjRIu15pUvL1YrNgRCy9SQhQf5ftrY9sBQKmYA9f56S5MTGyiTH0jLl4ewsk57oaNliFRMjEzVAXh+Q51hZyTr+/vL/1k+eTGnpEkJeIyFB1k0et2RvL5MtZ2eZJN66JVs2LC3l+0RGpnS3WVqmjMVK/hkmJ2hWVvJ6Dx6kjGF680a2oMTEyLiSNzR9+lS2fDg6pnTpXb0qW+YKF5aJ54sX8r1jYmS9mBh5bvJ2HQkJ8lG8uLzu3bspLS62tvJ5TIyMKzJSfn4hZLJqayuvFRYm40xurYmKkuU2NvLnIETKjDxLS6BcOfne8fGyJSi5TSIpSZbFx6eMu3J2lq1Rz57JxNXBQd7DyEh5/5Nbb4zE6CO+FKmmNwoh0pQlmzp1KsaPH69+ndyiQkREOZD8N9faOmVgsDYeHrKLJTe0bZs71yWzZ7REpWDBgrC0tEzTevLs2bM0rSzJbG1tYWtra4jwiIiIyAQYbYMRGxsbVKtWDXv27NEo37NnD2rXrm2kqIiIiMiUGLXrZ/z48ejTpw+CgoJQq1YtLF++HA8fPsSwYcOMGRYRERGZCKMmKt26dcPLly8xa9YsPH36FOXLl8eOHTvg5+dnzLCIiIjIRBh9Zdqc4DoqRERE5icr399GG6NCRERElBkmKkRERGSymKgQERGRyWKiQkRERCaLiQoRERGZLCYqREREZLKYqBAREZHJYqJCREREJouJChEREZksoy6hn1PJi+pGREQYORIiIiLSVfL3ti6L45t1ovL27VsAgK+vr5EjISIioqx6+/YtXFxcMqxj1nv9qFQqPHnyBE5OTlAoFHq7bkREBHx9ffHo0SPuIZTLeK8Ng/fZMHifDYP32XBy614LIfD27Vt4e3vDwiLjUShm3aJiYWEBHx+fXLu+s7Mz/yMwEN5rw+B9NgzeZ8PgfTac3LjXmbWkJONgWiIiIjJZTFSIiIjIZDFR0cLW1hYzZ86Era2tsUPJ83ivDYP32TB4nw2D99lwTOFem/VgWiIiIsrb2KJCREREJouJChEREZksJipERERkspioEBERkclioqLFd999h+LFi8POzg7VqlXDkSNHjB2S2Zg3bx6qV68OJycneHh4oH379rh165ZGHSEEQkJC4O3tDXt7ezRo0ADXrl3TqBMXF4fRo0ejYMGCcHBwQNu2bfHPP/8Y8qOYlXnz5kGhUGDs2LHqMt5n/Xn8+DF69+4Nd3d3KJVKVK5cGefOnVMf573OucTERHz88ccoXrw47O3tUaJECcyaNQsqlUpdh/c56w4fPow2bdrA29sbCoUCW7du1Tiur3v6+vVr9OnTBy4uLnBxcUGfPn3w5s0b/XwIQRrWr18vrK2txYoVK8T169fFmDFjhIODg3jw4IGxQzMLzZo1E6tWrRJXr14VFy9eFK1atRJFixYVkZGR6jrz588XTk5OYtOmTeLKlSuiW7duwsvLS0RERKjrDBs2TBQpUkTs2bNHnD9/Xrz//vuiUqVKIjEx0Rgfy6SdPn1aFCtWTFSsWFGMGTNGXc77rB+vXr0Sfn5+on///uLUqVPi/v37Yu/eveLu3bvqOrzXOTdnzhzh7u4u/vzzT3H//n2xceNG4ejoKJYsWaKuw/ucdTt27BDTpk0TmzZtEgDEli1bNI7r6542b95clC9fXhw/flwcP35clC9fXrRu3Vovn4GJSio1atQQw4YN0ygLCAgQU6ZMMVJE5u3Zs2cCgDh06JAQQgiVSiU8PT3F/Pnz1XViY2OFi4uLWLZsmRBCiDdv3ghra2uxfv16dZ3Hjx8LCwsLsWvXLsN+ABP39u1bUbp0abFnzx5Rv359daLC+6w/kydPFnXq1En3OO+1frRq1UoMHDhQo6xjx46id+/eQgjeZ31Inajo655ev35dABAnT55U1zlx4oQAIG7evJnjuNn18474+HicO3cOTZs21Shv2rQpjh8/bqSozFt4eDgAoECBAgCA+/fvIywsTOMe29raon79+up7fO7cOSQkJGjU8fb2Rvny5flzSGXkyJFo1aoVGjdurFHO+6w/27ZtQ1BQELp06QIPDw9UqVIFK1asUB/nvdaPOnXqYN++fbh9+zYA4NKlSzh69ChatmwJgPc5N+jrnp44cQIuLi6oWbOmus57770HFxcXvdx3s96UUN9evHiBpKQkFC5cWKO8cOHCCAsLM1JU5ksIgfHjx6NOnTooX748AKjvo7Z7/ODBA3UdGxsbuLm5panDn0OK9evX4/z58zhz5kyaY7zP+nPv3j0sXboU48ePx0cffYTTp0/jww8/hK2tLfr27ct7rSeTJ09GeHg4AgICYGlpiaSkJMydOxc9evQAwN/p3KCvexoWFgYPD4801/fw8NDLfWeiooVCodB4LYRIU0aZGzVqFC5fvoyjR4+mOZade8yfQ4pHjx5hzJgx2L17N+zs7NKtx/uccyqVCkFBQfj0008BAFWqVMG1a9ewdOlS9O3bV12P9zpnNmzYgDVr1uCXX35BuXLlcPHiRYwdOxbe3t7o16+fuh7vs/7p455qq6+v+86un3cULFgQlpaWaTLAZ8+epck4KWOjR4/Gtm3bcODAAfj4+KjLPT09ASDDe+zp6Yn4+Hi8fv063Tr53blz5/Ds2TNUq1YNVlZWsLKywqFDh/DVV1/ByspKfZ94n3POy8sLgYGBGmVly5bFw4cPAfB3Wl8mTpyIKVOmoHv37qhQoQL69OmDcePGYd68eQB4n3ODvu6pp6cn/v333zTXf/78uV7uOxOVd9jY2KBatWrYs2ePRvmePXtQu3ZtI0VlXoQQGDVqFDZv3oz9+/ejePHiGseLFy8OT09PjXscHx+PQ4cOqe9xtWrVYG1trVHn6dOnuHr1Kn8O/2nUqBGuXLmCixcvqh9BQUHo1asXLl68iBIlSvA+60lwcHCaKfa3b9+Gn58fAP5O60t0dDQsLDS/kiwtLdXTk3mf9U9f97RWrVoIDw/H6dOn1XVOnTqF8PBw/dz3HA/HzWOSpyf/+OOP4vr162Ls2LHCwcFBhIaGGjs0szB8+HDh4uIiDh48KJ4+fap+REdHq+vMnz9fuLi4iM2bN4srV66IHj16aJ0O5+PjI/bu3SvOnz8vGjZsmK+nGOri3Vk/QvA+68vp06eFlZWVmDt3rrhz545Yu3atUCqVYs2aNeo6vNc5169fP1GkSBH19OTNmzeLggULikmTJqnr8D5n3du3b8WFCxfEhQsXBACxaNEiceHCBfWSG/q6p82bNxcVK1YUJ06cECdOnBAVKlTg9OTc9O233wo/Pz9hY2Mjqlatqp5aS5kDoPWxatUqdR2VSiVmzpwpPD09ha2trahXr564cuWKxnViYmLEqFGjRIECBYS9vb1o3bq1ePjwoYE/jXlJnajwPuvPH3/8IcqXLy9sbW1FQECAWL58ucZx3uuci4iIEGPGjBFFixYVdnZ2okSJEmLatGkiLi5OXYf3OesOHDig9W9yv379hBD6u6cvX74UvXr1Ek5OTsLJyUn06tVLvH79Wi+fQSGEEDlvlyEiIiLSP45RISIiIpPFRIWIiIhMFhMVIiIiMllMVIiIiMhkMVEhIiIik8VEhYiIiEwWExUiIiIyWUxUiMjsKRQKbN261dhhEFEuYKJCRDnSv39/KBSKNI/mzZsbOzQiygOsjB0AEZm/5s2bY9WqVRpltra2RoqGiPIStqgQUY7Z2trC09NT4+Hm5gZAdsssXboULVq0gL29PYoXL46NGzdqnH/lyhU0bNgQ9vb2cHd3x9ChQxEZGalRZ+XKlShXrhxsbW3h5eWFUaNGaRx/8eIFOnToAKVSidKlS2Pbtm3qY69fv0avXr1QqFAh2Nvbo3Tp0mkSKyIyTUxUiCjXTZ8+HZ06dcKlS5fQu3dv9OjRAzdu3AAAREdHo3nz5nBzc8OZM2ewceNG7N27VyMRWbp0KUaOHImhQ4fiypUr2LZtG0qVKqXxHp988gm6du2Ky5cvo2XLlujVqxdevXqlfv/r169j586duHHjBpYuXYqCBQsa7gYQUfbpZWtDIsq3+vXrJywtLYWDg4PGY9asWUIIuaP2sGHDNM6pWbOmGD58uBBCiOXLlws3NzcRGRmpPr59+3ZhYWEhwsLChBBCeHt7i2nTpqUbAwDx8ccfq19HRkYKhUIhdu7cKYQQok2bNmLAgAH6+cBEZFAco0JEOfb+++9j6dKlGmUFChRQP69Vq5bGsVq1auHixYsAgBs3bqBSpUpwcHBQHw8ODoZKpcKtW7egUCjw5MkTNGrUKMMYKlasqH7u4OAAJycnPHv2DAAwfPhwdOrUCefPn0fTpk3Rvn171K5dO1uflYgMi4kKEeWYg4NDmq6YzCgUCgCAEEL9XFsde3t7na5nbW2d5lyVSgUAaNGiBR48eIDt27dj7969aNSoEUaOHIkvvvgiSzETkeFxjAoR5bqTJ0+meR0QEAAACAwMxMWLFxEVFaU+fuzYMVhYWKBMmTJwcnJCsWLFsG/fvhzFUKhQIfTv3x9r1qzBkiVLsHz58hxdj4gMgy0qRJRjcXFxCAsL0yizsrJSD1jduHEjgoKCUKdOHaxduxanT5/Gjz/+CADo1asXZs6ciX79+iEkJATPnz/H6NGj0adPHxQuXBgAEBISgmHDhsHDwwMtWrTA27dvcezYMYwePVqn+GbMmIFq1aqhXLlyiIuLw59//omyZcvq8Q4QUW5hokJEObZr1y54eXlplPn7++PmzZsA5Iyc9evXY8SIEfD09MTatWsRGBgIAFAqlfjrr78wZswYVK9eHUqlEp06dcKiRYvU1+rXrx9iY2OxePFi/O9//0PBggXRuXNnneOzsbHB1KlTERoaCnt7e9StWxfr16/XwycnotymEEIIYwdBRHmXQqHAli1b0L59e2OHQkRmiGNUiIiIyGQxUSEiIiKTxTEqRJSr2LtMRDnBFhUiIiIyWUxUiIiIyGQxUSEiIiKTxUSFiIiITBYTFSIiIjJZTFSIiIjIZDFRISIiIpPFRIWIiIhMFhMVIiIiMln/D6hOwse2QQAaAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_loss = best_cnn_history.history[\"loss\"]\n",
    "validation_loss = best_cnn_history.history[\"val_loss\"]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.plot(epochs, training_loss, \"r\", label=\"Training loss\")\n",
    "plt.plot(epochs, validation_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 2.2829 - accuracy: 0.1439 - val_loss: 2.2073 - val_accuracy: 0.3081\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.0813 - accuracy: 0.1948 - val_loss: 2.1403 - val_accuracy: 0.3081\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 189.5430 - accuracy: 0.1933 - val_loss: 2.1348 - val_accuracy: 0.3081\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 2.2179 - accuracy: 0.2006 - val_loss: 2.1500 - val_accuracy: 0.3081\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2.2159 - accuracy: 0.2006 - val_loss: 2.1407 - val_accuracy: 0.3081\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2.2108 - accuracy: 0.1991 - val_loss: 2.1317 - val_accuracy: 0.3198\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2.2062 - accuracy: 0.2108 - val_loss: 2.1224 - val_accuracy: 0.3198\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2.2022 - accuracy: 0.2049 - val_loss: 2.1133 - val_accuracy: 0.3198\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2.1985 - accuracy: 0.2020 - val_loss: 2.1056 - val_accuracy: 0.3198\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2.1955 - accuracy: 0.2049 - val_loss: 2.0997 - val_accuracy: 0.3198\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Accuracy (LSTM): 0.31976744186046513\n",
      "F1 Score (LSTM): 0.1549533859235734\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential(\n",
    "    [\n",
    "        LSTM(50, activation=\"relu\", input_shape=(X_train_scaled.shape[1], 1)),\n",
    "        Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_lstm.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_m_ohe,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_scaled, y_val_m_ohe),\n",
    ")\n",
    "\n",
    "predictions_lstm = model_lstm.predict(X_val_scaled)\n",
    "accuracy_lstm = accuracy_score(\n",
    "    y_val_m_ohe.argmax(axis=1), predictions_lstm.argmax(axis=1)\n",
    ")\n",
    "f1_lstm = f1_score(\n",
    "    y_val_m_ohe.argmax(axis=1), predictions_lstm.argmax(axis=1), average=\"weighted\"\n",
    ")\n",
    "\n",
    "print(f\"Accuracy (LSTM): {accuracy_lstm}\")\n",
    "print(f\"F1 Score (LSTM): {f1_lstm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
